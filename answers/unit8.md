# Week 8

## Summary Questions:

- Show some text that you finetuned model generated!

  > Clown:
  > I am clown!
  >
  > Clowness:
  > Why, no, no; I am not a tapster.
  >
  > Shepherd:
  > Clubs and such; but I am a tapstool.
  > What, is it good to be a tapsters' bird?
  >
  > AUTOLYCUS:
  > Good to be so, to be good to me, I know not.
  > Clog and such, I am too fond:
  > But I am no tapster, I have a score of them.
  > I have a dozen that are as many as you have:
  > And you have to

  > I'll be your father, and not your mother;
  > For I am a widow, and you a wife.
  >
  > KING RICHARD III:
  > What, doth your husband live?
  >
  > DUKE OF AUMERLE:
  > Ay, if he die to-morrow,
  > And you are not heir to his daughter.
  > I'll have you wed again, and that's enough.
  > What say you, then, to the prince your son?
  > Tell me, my son, and tell me, your husband:
  > Tell him, my daughter, and his

  > Washington:
  > I will not be so long in saying.
  >
  > First Senator:
  > You are a senator, and you must be so.
  > You have been forsworn to the people's house:
  > And you have not been so long to demand it.
  > Your honours both,--
  >
  > SICINIUS:
  > We have been so forswaken, and so long.
  > I would be so brief to say the truth.
  > The people are not so much in their love
  > As they are in fearing to hear me tell it. They
  > are not as much in fear

- What would you like to explore further?
  - Other applications of transformers like computer vision.
  - Generating a text generator from my own text history.
  - Persistent memory and outside-of-input context for generating a response.
  - How exactly the attention mechanism works, like how the array is formed etc.

- What is a resource you read and was there anything interesting in it?
  - Carter's HF tutorial jupyter notebook.
  - [The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/), still reading but looks fantastic.
  - Hugging face course