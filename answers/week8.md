# Week 8

## Summary Questions:

- Show some text that you finetuned model generated!

  > I have been a prisoner of the king;
  >
  > And, being so, I am now a free man.

  > First Gentleman:
  >
  > I have a letter from your lady-maid to your grace.
  >
  > Second Gentleman:
  >
  > I am sorry, my lord

  > I'll be brief, and not be long.
  >
  > DUKE VINCENTIO:
  >
  > I am not so brief, nor so long

- What would you like to explore further?
  - Other applications of transformers like computer vision.
  - Generating a text generator from my own text history.
  - Persistent memory and outside-of-input context for generating a response.
  - How exactly the attention mechanism works, like how the array is formed etc.

- What is a resource you read and was there anything interesting in it?
  - Carter's HF tutorial jupyter notebook.
  - [The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/), still reading but looks fantastic.
  - Hugging face course