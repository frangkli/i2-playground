{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35Q_4O9HM-Lg"
      },
      "source": [
        "# What is this notebook?\n",
        "Hugging Face is a popular library and resouce for training and using AI models. While it has many valuable resources, it can be extremely difficult to use. This notebook aims to serve as an introduction to Hugging Face and all the tools it provides."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pysjik7mNCQz"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8sudHdocbh6"
      },
      "source": [
        "You might want a Hugging Face account. If you don't have one already, sign up [here](https://huggingface.co/join)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H-Vt2eEnVbq7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\frank\\anaconda3\\lib\\site-packages (4.26.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: requests in c:\\users\\frank\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\frank\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: colorama in c:\\users\\frank\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: datasets in c:\\users\\frank\\anaconda3\\lib\\site-packages (2.10.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (11.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (2.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (0.12.1)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (1.21.5)\n",
            "Requirement already satisfied: pandas in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (1.4.4)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (2022.7.1)\n",
            "Requirement already satisfied: packaging in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: responses<0.19 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: xxhash in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.3.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\frank\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: colorama in c:\\users\\frank\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# install the necessary libraries!\n",
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3bE1FYBNFot"
      },
      "source": [
        "# Basics of HuggingFace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1Hl12swQm35"
      },
      "source": [
        "(IMO) Hugging Face serves two tasks: storage of AI resouces (models, tokenizers, datasets) and a library of tools for training/using AI models. These resources take the form of a Github-like repository service (the HF Hub) in addition to libraries.\n",
        "\n",
        "Hugging Face's prominent library is [transformers](https://github.com/huggingface/transformers/), a library containing powerful foundation (pretrained) models and tools to use them. Hugging Face also has a [tokenizers](https://github.com/huggingface/tokenizers) library for tokenizers, a [datasets](https://github.com/huggingface/datasets) library for datasets, and a [diffusers](https://github.com/huggingface/diffusers) library for, you guessed it, diffusion models. (They have a lot of stuff. Too much stuff IMO. It is a bit overwhelming.)\n",
        "\n",
        "Hugging Face has the [Hub](https://huggingface.co/docs/hub/index), a Github-like service for storing models, datasets, and more. You can use this to store trained models or datasets or access others pre-trained models!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aphmE_rl2yZS"
      },
      "source": [
        "The good of Hugging Face:\n",
        "\n",
        "- Easy to store and manage models and datasets.\n",
        "- Has many important pre-trained models and popular datasets.\n",
        "- Has some really powerful tools.\n",
        "\n",
        "The bad of Hugging Face:\n",
        "- The documentation can be terrible.\n",
        "- Incredibly confusing - there's too much going on.\n",
        "- Poor abstractions - too often I have to *give in to the Great Hugging Face* and just trust that it works.\n",
        "- Little consistency in classes, data structures, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DAnDXXcOCGX"
      },
      "source": [
        "# Let's use a pretrained model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMv9z22oT92R"
      },
      "source": [
        "Alright, enough talking. Let's get to something fun!\n",
        "\n",
        "The first way to use a model is with a [`pipeline`](https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/pipelines). A `pipeline` is a crazy abstraction that reduces a bunch of \"scary AI stuff\" into a simple object for inference. We simply need to give the `pipeline` a task or model at instantiation and it is ready for inference. Take a look:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YxKdZTYkVWmh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9998693466186523},\n",
              " {'label': 'NEGATIVE', 'score': 0.9945459365844727}]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-classification\") # text-classification is the task\n",
        "pipe(['Wow, this notebook is amazing!', 'I hate self-referential jokes!']) # inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sMDCdtYWYo2"
      },
      "source": [
        "There's a couple things to take note of:\n",
        "\n",
        "First, we gave it a task, \"text-classification\". There are many different [tasks](https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/pipelines#transformers.pipeline.task) such as text generation, text classification, and visual tasks. When `pipeline` is instantiated with a task it actually creates a specific pipeline for the task - in this case a [`TextClassificationPipeline`](https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/pipelines#transformers.TextClassificationPipeline).\n",
        "\n",
        "Pipelines for different tasks require different arguments when being called. Text-classification pipelines require either a single string or a list of strings when being called. Make sure to check the docs for the specific type of `pipeline`.\n",
        "\n",
        "When we give it a task without specifying a model it defaults to one. For \"text-classification\" it defaults to \"distilbert\", a type of BERT model. If we want something other than the default, we can pass a model name at instantiation: `pipe = pipeline(model=model_name)`\n",
        "\n",
        "Let's look at another example: If I'm speaking with my German friends, I might like to use sentiment classification what they're saying in German. Fortunately, there's a pretrained [model](https://huggingface.co/oliverguhr/german-sentiment-bert) for that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GET3yuVMZ2CR"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'negative', 'score': 0.9905546307563782}]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipe = pipeline(model='oliverguhr/german-sentiment-bert') # instantiate with model name\n",
        "pipe('Carter, ich hasse deinen Humor!') # we can run inference with just a string!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmQgKLxKaRk-"
      },
      "source": [
        "Yikes, looks bad...\n",
        "\n",
        "Let's turn to a different task, generating text! If we want to generate from the following prompt:\n",
        "\n",
        "`AP News: The University of Washington recently announced`\n",
        "\n",
        "We can create a new type of pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eUAe_-zJbriA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "c:\\Users\\Frank\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "c:\\Users\\Frank\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 50 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'AP News: The University of Washington recently announced they are considering building a $250 million college soccer complex in Seattle, with $15 billion in stadium capital available for lease after an initial $50 million investment.\\n\\nRead Part 1 here:'}]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create a text generation pipeline!\n",
        "pipe = pipeline('text-generation')\n",
        "pipe('AP News: The University of Washington recently announced')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jeex8XRUdIIo"
      },
      "source": [
        "# Our first step under the hood: transformers and tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax_PAftd6kdC"
      },
      "source": [
        "Good work! Let's dive a bit deeper into what actually happens inside the pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lzRC0nI87IZf"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# example pipeline using AutoModel and AutoTokenizer\n",
        "class TextPipe:\n",
        "    def __init__(self, model):\n",
        "        # download models and tokenizers\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "    def __call__(self, prompt):\n",
        "        # make sure it is a list\n",
        "        if type(prompt) is str:\n",
        "            prompt = [prompt]\n",
        "\n",
        "        # generate \n",
        "        outputs = []\n",
        "        for p in prompt:\n",
        "            # tokenize the prompt\n",
        "            tokenized_prompt = self.tokenizer(p, return_tensors='pt')\n",
        "            # forward pass through the model\n",
        "            gen_tensor = self.model.generate(tokenized_prompt['input_ids'])\n",
        "            # decode the model outputs\n",
        "            gen_text = self.tokenizer.decode(gen_tensor[0])\n",
        "            outputs.append(gen_text)\n",
        "            # note that we could pass everything in a batch, but i want to be explicit\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "j2aU0Ng2-3sz"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "c:\\Users\\Frank\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['amazon.com is the only place you can buy the book.\\n\\nThe book is a great',\n",
              " \"AI will eventually be able to provide a more complete and accurate picture of the world's population.\\n\"]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# let's try it out!\n",
        "\n",
        "pipe = TextPipe(model='gpt2')\n",
        "pipe(['amazon.com is the', 'AI will eventually'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5cZao6XDvTm"
      },
      "source": [
        "There are two stored fields: `model` and `tokenizer`. `model` comes from: `AutoModelForCausalLM`, a HF class for loading AI models. In this case it loads a pretrained GPT2. `AutoTokenizer` does something similar, loading a tokenizer for GPT2. These AutoThings basically instantiate a class, loading weights or configurations from for them. There are multiple types of AutoThings, but I'll mainly focus on lanugage generation for the rest of this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCBgOR0wNzlS"
      },
      "source": [
        "Let's first look at `AutoTokenizer`. This is an object that can encode and decode plaintext into tensors which can be used by models. You need to instanitate it with `AutoTokenizer.from_pretrained(name)`, loading `name`'s associated tokenizer from the HF Hub. Often these will be some form of a GPT2 tokenizer (it is exactly that in this case).\n",
        "\n",
        "There's two important methods you should know: First, simply calling `tokenizer(input)` encodes a string or list of strings. One must specify the flag `return_tensors='pt'` to return PyTorch tensors **THIS IS IMPORTANT**. The output will be a dict containing keys `input_ids` and `attention_mask`. These keys point to PyTorch tensors which can be passed into a model later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OrRpQRgvQg2X"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[1212,  318,  281, 1672, 2420]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('gpt2') # load gpt2 tokenizer\n",
        "out = tokenizer('This is an example text', return_tensors='pt') # one example (string)\n",
        "out # returns a dict with input_ids and attention_mask pointing to tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ccSq_P-RYgE"
      },
      "source": [
        "If we pass in multiple strings in a list, we need to make sure they're the same length. If they aren't, we'll get an error:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "7hft13ReRMUD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error!\n"
          ]
        }
      ],
      "source": [
        "# they must be the same length\n",
        "try:\n",
        "  tokenizer(['This is example one', 'I am example two!'], return_tensors='pt')\n",
        "except:\n",
        "  print('Error!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k74MrXNvRtgH"
      },
      "source": [
        "To tokenize texts which are different lengths, we need to tell the model how to deal with that. There's two main options - truncate to a certain length, or pad (with special tokens) to a certain length ([docs](https://huggingface.co/docs/transformers/pad_truncation)). For now, I'll pad to the longest sequence in the batch. To do so, I'll need to pass the argument `padding=True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "_kY4E5AYSkmm"
      },
      "outputs": [],
      "source": [
        "# again, but with padding!\n",
        "try:\n",
        "  tokenizer(['This is example one', 'I am example two!'], return_tensors='pt', padding=True)\n",
        "except:\n",
        "  print('Error!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP-aVx2FSoMJ"
      },
      "source": [
        "Shoot, we need to tell the model what token to pad with. A typical choice is the tokenizer's end of sentence or `eos` token. We can set it like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "vAkFRlA6S7n1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 1212,   318,  1672,   530, 50256],\n",
              "        [   40,   716,  1672,   734,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 0],\n",
              "        [1, 1, 1, 1, 1]])}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token # set pad token to eos token\n",
        "# we try again!\n",
        "out = tokenizer(['This is example one', 'I am example two!'], return_tensors='pt', padding=True)\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TJqa77rTJFh"
      },
      "source": [
        "It works! Note that the returned tensors now have first dimension of two, because we passed in two inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bqXMKaWKTQd9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 5])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out['input_ids'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8N_RaYfTYaa"
      },
      "source": [
        "Now for the second important method: `tokenizer.decode(input)` (and `tokenizer.batch_decode`). We want to be able to decode outputs from the model - this is the method for that!\n",
        "\n",
        "The `input` for `tokenizer.decode(input)` should be a PyTorch tensor of encoded text with **one** dimension. We can first encode text to get a tensor, then decode it and it should be the same!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "QPorq0MkUJi7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error!\n"
          ]
        }
      ],
      "source": [
        "# encode text\n",
        "tokenized_text = tokenizer('This is example text', return_tensors='pt')\n",
        "\n",
        "# decode text\n",
        "try:\n",
        "  tokenizer.decode(tokenized_text['input_ids'])\n",
        "except:\n",
        "  print('Error!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPHr5RtUU2yd"
      },
      "source": [
        "Shoot, I forgot that `tokenizer(input)` outputs a batch dimension always, even if it is one. Let's index into the first dimension and try again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "kzCF2OgfVLgZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'This is example text'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# encode text\n",
        "tokenized_text = tokenizer('This is example text', return_tensors='pt')\n",
        "\n",
        "print(tokenized_text['input_ids'].shape)\n",
        "\n",
        "# decode text\n",
        "tokenizer.decode(tokenized_text['input_ids'][0]) # index into first dim this time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQJM1-DKVPer"
      },
      "source": [
        "Sweet! What if we want to decode *using* a batch? Well, you guessed it, use: `tokenizer.batch_decode(input)`. This expects a batch dimension - let's give it one!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "4c9pN9SJVvdy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 7])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['I am example one!<|endoftext|><|endoftext|>',\n",
              " 'Do not forget about example two!']"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# encode text\n",
        "tokenized_text = tokenizer(\n",
        "    ['I am example one!', 'Do not forget about example two!'],\n",
        "    return_tensors='pt',\n",
        "    padding=True\n",
        ")\n",
        "\n",
        "print(tokenized_text['input_ids'].shape)\n",
        "\n",
        "# decode text\n",
        "tokenizer.batch_decode(tokenized_text['input_ids']) # no need to index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6VN6b9HWAck"
      },
      "source": [
        "Perfect! We can even see the `eos` tokens that were used to pad!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOa9GnGlWM26"
      },
      "source": [
        "`AutoModelForCausalLM` is pretty similar to `AutoTokenizer`. Again, it loads a type of model from HF Hub that you pass in when you instantiate with `AutoModelForCausalLM.from_pretrained(model_name)`. There's also two methods that I'll highlight here as well!\n",
        "\n",
        "The first is simply calling `model()`, running a forward pass of the model. It often requires several parameters:\n",
        "- `input_ids` is a tokenized PyTorch tensor (we saw this from the tokenizer)!\n",
        "- `attention_mask` is another PyTorch tensor, again created with the tokenizer.\n",
        "- `labels` is not always required, but allows the `model` to output a loss as well. For language generation, `labels` typically is the same as `input_ids`.\n",
        "\n",
        "For GPT-2, this will output a data structure containing logits and sometimes a loss.\n",
        "\n",
        "Let's take a look at this in action!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "REPaKIM4X1rD"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CausalLMOutputWithCrossAttentions(loss=None, logits=tensor([[[ -37.2172,  -36.8864,  -40.3563,  ...,  -43.4351,  -43.0232,\n",
              "           -37.0993],\n",
              "         [-100.4229, -101.9893, -103.9724,  ..., -110.4855, -107.2627,\n",
              "          -102.8470],\n",
              "         [ -93.9326,  -94.9589,  -99.5998,  ..., -102.0784, -103.2679,\n",
              "           -96.6539],\n",
              "         [ -80.3484,  -84.3211,  -88.6568,  ...,  -94.4965,  -96.8762,\n",
              "           -86.4970],\n",
              "         [ -78.7350,  -82.5628,  -87.0541,  ...,  -93.1001,  -95.4846,\n",
              "           -84.7598],\n",
              "         [-112.2791, -114.0136, -113.4279,  ..., -123.7545, -123.7561,\n",
              "          -108.3679]]], grad_fn=<UnsafeViewBackward0>), past_key_values=((tensor([[[[-1.3190e+00,  1.8644e+00,  8.9757e-01,  ..., -1.4033e+00,\n",
              "           -2.3651e-01,  1.2896e+00],\n",
              "          [-2.1646e+00,  2.7695e+00,  1.3394e+00,  ..., -5.7841e-01,\n",
              "           -1.6374e+00,  2.1936e+00],\n",
              "          [-2.7988e+00,  2.0681e+00,  2.6545e+00,  ..., -4.6508e-01,\n",
              "           -2.2150e+00,  1.7963e+00],\n",
              "          [-2.4105e+00,  2.6055e+00,  1.2694e+00,  ...,  1.3742e-01,\n",
              "           -8.1007e-01,  1.7689e+00],\n",
              "          [-1.2077e+00,  2.6973e+00,  9.7582e-01,  ..., -1.6049e+00,\n",
              "           -1.6714e+00,  2.2172e+00],\n",
              "          [-2.0559e+00,  2.5747e+00,  2.0065e+00,  ..., -1.4898e+00,\n",
              "           -1.9779e+00,  2.2355e+00]],\n",
              "\n",
              "         [[-3.7719e-01,  4.4023e-01, -6.4755e-01,  ..., -3.9102e-01,\n",
              "            2.5417e+00,  1.0485e+00],\n",
              "          [-5.6477e-01, -1.5940e+00, -2.6486e+00,  ..., -1.4112e+00,\n",
              "            4.2986e+00,  2.9322e-01],\n",
              "          [ 1.4412e-01, -1.0276e+00, -2.7664e+00,  ..., -1.7278e+00,\n",
              "            4.5384e+00,  7.6921e-01],\n",
              "          [ 7.3604e-01, -4.5334e-01, -1.6045e+00,  ..., -9.7720e-01,\n",
              "            4.3377e+00,  1.3353e+00],\n",
              "          [-4.7193e-01, -1.2994e+00,  3.3836e-03,  ..., -8.8472e-01,\n",
              "            5.5692e+00,  8.2580e-01],\n",
              "          [-8.1175e-03, -7.9827e-01, -1.6568e+00,  ..., -1.8595e+00,\n",
              "            1.8977e+00,  1.0514e+00]],\n",
              "\n",
              "         [[ 2.2048e-02, -7.5673e-02,  8.3024e-01,  ..., -1.4517e+00,\n",
              "           -1.6848e+00,  8.1629e-01],\n",
              "          [ 2.2634e-01,  1.6200e-02,  4.4857e-01,  ..., -2.7974e+00,\n",
              "           -1.6248e-01,  1.3393e+00],\n",
              "          [ 1.5120e+00, -1.8420e-01,  1.5525e+00,  ..., -2.9215e+00,\n",
              "            6.4245e-01,  1.2902e+00],\n",
              "          [-1.0722e+00,  5.5266e-01, -1.1205e+00,  ..., -3.4635e+00,\n",
              "           -7.9314e-02,  1.9480e+00],\n",
              "          [-4.3125e-01, -5.7769e-01,  3.5792e-01,  ..., -2.6742e+00,\n",
              "            3.3341e-01,  1.9875e+00],\n",
              "          [ 1.0891e+00,  1.2473e-03,  9.2419e-01,  ..., -2.9487e+00,\n",
              "            1.2022e+00,  1.6356e+00]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[ 4.3110e-01, -4.6927e-02, -1.3321e-01,  ...,  3.7510e-01,\n",
              "            6.8562e-01,  5.7607e-01],\n",
              "          [-2.3236e-02, -5.8801e-02, -5.1998e-02,  ...,  1.0591e+00,\n",
              "            6.6432e-01,  4.4426e-01],\n",
              "          [ 4.0327e-01,  1.8651e-01, -2.6977e-01,  ...,  1.6992e+00,\n",
              "            6.1950e-01,  3.2791e-01],\n",
              "          [-6.9375e-01, -1.1468e-01, -7.3626e-01,  ...,  1.6070e+00,\n",
              "            9.0676e-03,  4.4526e-01],\n",
              "          [ 1.8411e-01, -9.4539e-01,  8.5426e-02,  ...,  1.2627e+00,\n",
              "           -2.6845e-01,  1.1701e-01],\n",
              "          [ 4.0446e-02,  3.1212e-01,  2.9708e-01,  ...,  9.4665e-01,\n",
              "            1.2455e-01,  3.1234e-01]],\n",
              "\n",
              "         [[ 1.4246e+00,  1.3487e+00, -2.4757e-01,  ..., -2.8181e-01,\n",
              "            9.5264e-01, -1.1325e+00],\n",
              "          [ 1.0014e+00,  1.0030e+00, -2.7165e-01,  ..., -1.0854e+00,\n",
              "            1.1519e+00, -1.2769e+00],\n",
              "          [ 1.1471e+00,  1.0686e-01,  1.4069e-01,  ..., -1.2444e+00,\n",
              "            1.4395e+00, -9.5344e-01],\n",
              "          [ 1.2253e+00,  1.1933e+00, -6.0561e-01,  ..., -1.5932e+00,\n",
              "            2.9360e-01,  5.9844e-03],\n",
              "          [ 2.4779e-01,  4.2760e-01,  2.3432e-01,  ..., -5.6475e-01,\n",
              "            1.2139e+00,  1.5342e-02],\n",
              "          [ 9.1988e-01,  9.0977e-02,  6.3444e-02,  ..., -6.8504e-01,\n",
              "            7.6588e-01, -3.3454e-01]],\n",
              "\n",
              "         [[ 4.9501e-01,  2.0561e-01, -7.4766e-02,  ..., -3.4694e-01,\n",
              "            1.5860e-01,  1.7998e+00],\n",
              "          [ 8.0477e-02,  1.3493e+00,  1.0601e-01,  ...,  5.1209e-01,\n",
              "            6.5740e-01,  1.3528e+00],\n",
              "          [-9.6740e-02,  1.4428e-01,  3.1114e-01,  ...,  2.5092e-01,\n",
              "           -4.5149e-01,  1.0717e+00],\n",
              "          [-5.8550e-01, -6.5092e-01,  2.2550e-01,  ..., -5.9241e-01,\n",
              "           -1.4166e-01,  1.0745e+00],\n",
              "          [-2.1431e-02, -5.7721e-01, -1.4335e-01,  ..., -2.1817e-01,\n",
              "           -2.0797e-01,  1.4723e+00],\n",
              "          [-2.1610e-01,  1.0908e-01,  1.2146e+00,  ...,  4.0661e-02,\n",
              "            9.9250e-01,  2.2597e-01]]]], grad_fn=<PermuteBackward0>), tensor([[[[ 2.3089e-02,  8.2066e-02,  2.7945e-02,  ...,  2.6097e-02,\n",
              "           -2.3956e-02,  9.6702e-02],\n",
              "          [-4.7365e-02,  1.3187e-02, -6.6870e-03,  ...,  1.0275e-01,\n",
              "            8.5023e-03, -8.3876e-02],\n",
              "          [ 1.6769e-01,  3.4293e-03, -7.5566e-02,  ...,  6.3153e-03,\n",
              "           -6.9631e-02, -1.8760e-01],\n",
              "          [-9.6784e-02, -1.6394e-01,  8.0787e-02,  ..., -1.3499e-01,\n",
              "           -9.3497e-02,  1.4542e-02],\n",
              "          [-6.1622e-02, -5.4964e-02,  2.3530e-01,  ...,  1.5329e-01,\n",
              "            2.4020e-02,  2.5910e-01],\n",
              "          [-1.4186e-01, -1.3872e-02, -1.7564e-01,  ...,  2.4234e-01,\n",
              "           -2.4299e-01, -2.0696e-01]],\n",
              "\n",
              "         [[ 4.7284e-01,  1.1574e-01, -2.9770e-01,  ..., -6.3272e-01,\n",
              "           -2.1164e-01,  1.9056e-01],\n",
              "          [ 7.0031e-01, -1.1542e-01,  3.7084e-01,  ...,  1.4072e-02,\n",
              "            3.6301e-01,  5.1110e-02],\n",
              "          [ 5.2535e-01,  3.2990e-02, -1.0581e-01,  ...,  9.5916e-03,\n",
              "            2.5140e-01, -2.7154e-02],\n",
              "          [ 2.7942e-01, -1.2388e-01,  2.9195e-01,  ...,  2.7991e-01,\n",
              "           -2.6876e-01,  1.4587e-01],\n",
              "          [ 2.6249e-01,  1.3881e-01, -1.8018e-02,  ...,  1.8056e-01,\n",
              "            3.6964e-02,  7.4957e-02],\n",
              "          [ 2.7351e-01, -1.2729e-01, -4.1158e-02,  ...,  1.1455e-01,\n",
              "            5.5423e-01, -2.2308e-01]],\n",
              "\n",
              "         [[ 7.3907e-02, -1.1804e-01,  8.6928e-02,  ..., -7.0653e-03,\n",
              "           -1.7061e-04, -6.2186e-02],\n",
              "          [-4.3652e-01, -4.7473e-02, -5.1191e-01,  ..., -1.0580e-01,\n",
              "           -7.3917e-02,  2.2885e-01],\n",
              "          [-2.0141e-01, -1.3453e-01,  4.1347e-01,  ..., -1.0020e-01,\n",
              "            4.0095e-02,  1.7554e-01],\n",
              "          [ 1.8880e-01, -4.6214e-02, -7.9963e-02,  ...,  1.0918e-01,\n",
              "           -7.0184e-02, -1.8141e-01],\n",
              "          [-5.8355e-02,  2.8031e-02, -5.6481e-02,  ...,  1.4733e-01,\n",
              "           -1.6500e-01,  4.3662e-02],\n",
              "          [-2.6629e-02, -1.9141e-01,  2.3519e-01,  ..., -2.3143e-01,\n",
              "           -8.8398e-03, -7.5688e-02]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[-6.8233e-02,  7.8248e-02,  8.7423e-02,  ..., -1.5830e-01,\n",
              "            8.4335e-02, -6.9996e-02],\n",
              "          [-2.9272e-01,  1.6173e-02, -1.6687e-01,  ...,  2.3447e-01,\n",
              "           -1.6137e-01,  4.4118e-01],\n",
              "          [ 2.1311e-01,  2.4379e-01,  1.3394e-01,  ...,  4.1461e-02,\n",
              "           -9.7978e-02,  3.4482e-01],\n",
              "          [-5.2773e-02,  1.7007e-01,  5.3379e-01,  ...,  2.6673e-01,\n",
              "           -8.4042e-02, -1.8719e-01],\n",
              "          [-4.1873e-01,  3.3305e-01,  2.3117e-01,  ..., -6.9919e-02,\n",
              "            3.6901e-01, -1.1701e-01],\n",
              "          [ 1.2543e-01,  1.6468e-01,  1.3133e-01,  ..., -7.7218e-02,\n",
              "           -1.9788e-01,  6.4266e-03]],\n",
              "\n",
              "         [[ 1.7370e-02, -8.3901e-02, -2.1068e-01,  ...,  1.3147e-01,\n",
              "            2.3579e-01, -4.4249e-02],\n",
              "          [-3.7733e-01,  9.9479e-02,  7.9697e-02,  ...,  1.3490e-01,\n",
              "            1.1732e-01, -2.8392e-02],\n",
              "          [-4.8987e-03,  1.8035e-01,  2.3269e-01,  ..., -4.2952e-01,\n",
              "           -1.6099e-01, -1.4157e-01],\n",
              "          [-5.5331e-02, -3.5366e-01, -1.1028e-01,  ..., -1.9362e-01,\n",
              "           -6.9196e-02,  3.3124e-01],\n",
              "          [ 2.2259e-01, -9.2030e-02,  2.3298e-01,  ..., -2.0721e-01,\n",
              "            1.4496e-02,  1.1641e-01],\n",
              "          [-2.1205e-01,  1.2080e-01, -1.9901e-01,  ...,  9.1864e-03,\n",
              "            2.6859e-01, -2.4173e-01]],\n",
              "\n",
              "         [[ 7.5052e-02, -4.4469e-01,  1.6571e-01,  ...,  7.0882e-03,\n",
              "           -2.7171e-01, -8.7654e-02],\n",
              "          [-6.3136e-02,  7.1419e-02, -3.2544e-02,  ...,  2.1798e-01,\n",
              "            1.0220e-01,  1.0285e-01],\n",
              "          [ 1.7534e-01,  3.3825e-01,  7.0202e-02,  ...,  2.3653e-01,\n",
              "            3.3903e-01, -4.5763e-02],\n",
              "          [ 6.1333e-02, -5.3625e-02, -7.5724e-02,  ..., -2.3723e-01,\n",
              "           -9.6016e-03,  1.4857e-01],\n",
              "          [ 2.4040e-01,  3.6432e-01, -3.4780e-01,  ...,  4.8223e-02,\n",
              "           -2.3326e-01,  1.9315e-01],\n",
              "          [ 9.9054e-02, -8.9763e-02,  1.4463e-01,  ...,  1.0130e-02,\n",
              "            4.2593e-01, -3.9923e-02]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-1.5472e-01,  1.6268e+00, -1.8836e+00,  ...,  1.4842e+00,\n",
              "           -1.3666e+00,  2.1387e-01],\n",
              "          [ 1.1588e+00,  1.8842e+00, -1.5314e+00,  ..., -1.8914e-01,\n",
              "           -1.5323e+00, -2.1637e-01],\n",
              "          [ 1.0928e+00,  1.6343e+00, -9.1489e-01,  ...,  1.2632e-01,\n",
              "           -1.2097e+00, -5.3712e-01],\n",
              "          [ 8.1802e-02,  9.0877e-01, -2.0325e-01,  ..., -8.3124e-01,\n",
              "           -8.1246e-01,  2.1698e-01],\n",
              "          [ 1.2874e-02,  1.6050e+00,  3.9513e-01,  ..., -1.1210e-01,\n",
              "           -1.5026e+00,  6.7626e-01],\n",
              "          [-1.9685e-03,  9.2298e-01, -7.3925e-01,  ...,  2.4539e-01,\n",
              "           -4.0840e-01, -7.9471e-02]],\n",
              "\n",
              "         [[-9.3697e-01, -2.7260e-01, -5.7192e-01,  ..., -9.0301e-02,\n",
              "            5.2307e-01, -8.0643e-01],\n",
              "          [-7.5287e-01,  2.0603e-01, -1.1111e+00,  ..., -1.0188e+00,\n",
              "            1.3180e-01, -2.3271e-01],\n",
              "          [-3.4986e-01, -4.6716e-01, -2.0729e+00,  ...,  5.5718e-02,\n",
              "            1.0078e-01,  8.9975e-02],\n",
              "          [-2.9190e-01, -6.2827e-01, -9.9475e-01,  ..., -9.2521e-01,\n",
              "           -2.3275e-01, -4.7053e-01],\n",
              "          [ 3.8301e-01,  6.7448e-01, -1.9222e+00,  ..., -9.2253e-01,\n",
              "           -6.2312e-01, -1.9338e-01],\n",
              "          [-2.6690e-01,  1.1134e+00, -1.5757e+00,  ..., -6.4422e-01,\n",
              "           -1.2205e-01, -3.5686e-01]],\n",
              "\n",
              "         [[ 5.1715e-01,  1.6527e-02, -6.9063e-02,  ..., -1.3421e+00,\n",
              "            9.8667e-02, -3.5192e-01],\n",
              "          [ 1.6005e-02,  2.3115e-01, -6.0910e-02,  ..., -9.2763e-01,\n",
              "            3.3301e-02,  2.5650e-01],\n",
              "          [ 9.0586e-02,  3.7690e-01,  1.4217e-01,  ..., -9.6758e-01,\n",
              "           -4.0037e-02, -5.6685e-02],\n",
              "          [ 2.6878e-01,  1.2106e-01, -2.4601e-01,  ..., -1.0099e+00,\n",
              "            2.0142e-01,  2.2103e-01],\n",
              "          [-1.5185e-01, -3.0028e-01, -2.1091e-01,  ..., -9.7266e-01,\n",
              "            2.8223e-02,  4.8801e-01],\n",
              "          [-1.3169e-01,  1.6888e-01, -2.5809e-01,  ..., -1.2851e+00,\n",
              "            7.7406e-02,  2.0439e-01]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[-2.2233e-02, -8.5425e-01, -7.2925e-01,  ..., -1.0152e+00,\n",
              "            7.2173e-01, -7.8648e-01],\n",
              "          [-6.6945e-01,  6.4075e-01,  1.3081e+00,  ..., -6.8462e-01,\n",
              "            9.0172e-01, -8.3398e-01],\n",
              "          [ 2.7999e-01,  5.3014e-01,  1.9839e+00,  ...,  3.1999e-01,\n",
              "           -5.5604e-01, -3.0273e-02],\n",
              "          [-2.6122e+00,  1.9853e+00,  2.9428e+00,  ...,  1.0254e-01,\n",
              "           -2.2267e+00,  4.6722e-01],\n",
              "          [-2.3054e-01, -6.9698e-01,  1.9842e+00,  ..., -7.4211e-02,\n",
              "           -1.0443e+00, -3.7865e-01],\n",
              "          [ 2.0715e-03,  2.1298e+00,  2.0376e+00,  ...,  8.0616e-01,\n",
              "           -1.5865e+00,  8.7017e-02]],\n",
              "\n",
              "         [[-1.1444e+00, -2.9872e+00,  1.7379e-01,  ...,  1.7589e+00,\n",
              "            1.6085e+00, -1.3629e+00],\n",
              "          [ 4.1033e-01,  7.7277e-01, -4.4855e-01,  ..., -6.8379e-01,\n",
              "            4.5690e-01, -4.2859e-01],\n",
              "          [-5.7692e-02,  7.8774e-01, -4.9560e-01,  ..., -6.9985e-01,\n",
              "            6.9230e-01,  7.6598e-02],\n",
              "          [-8.7450e-02,  7.3090e-01, -6.4250e-01,  ..., -4.5064e-01,\n",
              "            5.7891e-01,  1.6083e-01],\n",
              "          [-2.4106e-01,  7.5520e-01, -6.5854e-01,  ..., -9.1696e-01,\n",
              "            6.2079e-01,  2.1136e-01],\n",
              "          [-7.7809e-02,  6.7974e-01, -6.8641e-01,  ..., -5.9418e-01,\n",
              "            5.2034e-01, -3.9192e-01]],\n",
              "\n",
              "         [[ 9.3066e-01,  2.5125e+00,  5.3014e-01,  ..., -6.1372e-01,\n",
              "           -5.9689e-01,  8.7409e-01],\n",
              "          [ 6.1928e-01,  2.5806e+00,  8.3879e-01,  ...,  1.1833e+00,\n",
              "           -1.3337e+00,  1.9545e-01],\n",
              "          [-7.2838e-01,  2.8759e+00,  1.3746e+00,  ...,  8.2575e-01,\n",
              "           -7.3376e-01,  2.8124e-01],\n",
              "          [-6.5956e-02,  7.7675e-01,  1.5630e+00,  ...,  2.5112e-01,\n",
              "            1.5645e-01,  1.2631e-01],\n",
              "          [-1.0001e+00,  1.8715e+00,  1.6813e+00,  ...,  1.4134e+00,\n",
              "           -4.7926e-01,  3.5433e-01],\n",
              "          [-3.6487e-01,  1.9100e+00,  1.1296e+00,  ...,  1.5438e-01,\n",
              "           -3.4834e-01,  3.0241e-01]]]], grad_fn=<PermuteBackward0>), tensor([[[[ 6.8883e-01, -1.2200e-01,  5.8287e-02,  ..., -1.6138e-01,\n",
              "           -5.1933e-02, -6.8165e-02],\n",
              "          [-1.6427e-01,  7.4968e-01, -2.5108e-01,  ...,  1.5922e-01,\n",
              "           -3.6005e-01,  2.8105e-01],\n",
              "          [-1.2120e-01,  1.2247e-01, -2.3031e-01,  ..., -1.8114e-01,\n",
              "           -3.9707e-01, -3.9224e-01],\n",
              "          [-1.5188e-01, -4.3035e-01, -2.2735e-02,  ...,  1.1114e-01,\n",
              "            3.8724e-01, -8.9785e-01],\n",
              "          [-4.4217e-02, -1.5322e-01, -3.5200e-01,  ...,  9.4882e-02,\n",
              "            3.4085e-01, -2.3409e-01],\n",
              "          [ 8.1159e-01,  2.0764e-01,  5.0055e-02,  ..., -9.5391e-02,\n",
              "           -1.1332e-01,  8.8934e-02]],\n",
              "\n",
              "         [[ 2.8172e-01, -1.4177e-01, -3.3247e-02,  ..., -4.2518e-03,\n",
              "           -6.2682e-01, -1.6292e-01],\n",
              "          [-2.8536e-01,  3.4164e-01,  5.3720e-01,  ...,  9.4941e-02,\n",
              "            9.9449e-01,  2.8292e-01],\n",
              "          [ 4.3838e-01, -2.9630e-01, -2.5576e-01,  ..., -5.9919e-01,\n",
              "           -3.7913e-01, -2.4922e-01],\n",
              "          [ 3.3467e-01, -1.0743e-01, -8.1995e-02,  ..., -1.9623e-01,\n",
              "           -3.3464e-02, -3.5116e-01],\n",
              "          [ 3.7951e-01, -4.8360e-02, -4.9684e-01,  ...,  6.8499e-02,\n",
              "            5.6471e-02, -5.8396e-01],\n",
              "          [ 1.0059e-01,  5.4547e-02,  1.1273e-01,  ...,  3.2158e-01,\n",
              "            1.5917e-01, -1.0496e-01]],\n",
              "\n",
              "         [[ 7.5605e-02, -1.9960e-01,  2.2040e-01,  ..., -6.0181e-01,\n",
              "            2.5395e-01,  3.7749e-02],\n",
              "          [ 2.9911e-01,  1.7572e-01,  6.8884e-02,  ..., -6.8541e-01,\n",
              "           -1.1695e-01,  2.3146e-01],\n",
              "          [ 5.1705e-01,  1.8396e-01,  3.1354e-01,  ..., -6.7236e-01,\n",
              "           -2.3644e-01,  2.5096e-01],\n",
              "          [ 4.0935e-01,  3.3078e-01, -3.5073e-01,  ..., -7.0570e-01,\n",
              "            1.7417e-01, -2.9588e-01],\n",
              "          [ 5.0201e-01,  7.2858e-02,  2.1424e-02,  ..., -7.2208e-01,\n",
              "           -3.8192e-02,  1.8288e-01],\n",
              "          [ 7.7221e-01,  2.4236e-01,  5.7417e-01,  ..., -5.0820e-01,\n",
              "           -2.3929e-01, -1.2645e-01]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[ 3.7756e-01,  7.3150e-01, -1.1882e-01,  ...,  9.4002e-02,\n",
              "           -1.0454e+00, -1.3381e-01],\n",
              "          [ 1.4518e-02,  3.8433e-02,  2.3876e-01,  ...,  1.5453e-01,\n",
              "           -7.3277e-01, -3.5147e-01],\n",
              "          [ 1.8699e-01, -1.2291e-01, -1.0304e-01,  ...,  1.4176e-01,\n",
              "           -6.2718e-01, -1.0263e-01],\n",
              "          [ 3.6050e-01, -5.0544e-01, -2.7210e-02,  ...,  4.4159e-02,\n",
              "           -8.7005e-01,  4.9466e-01],\n",
              "          [ 2.2878e-01, -3.5444e-01,  8.8599e-02,  ..., -2.2618e-01,\n",
              "           -4.4383e-01, -6.4354e-01],\n",
              "          [-3.2019e-01, -8.6222e-01,  4.4417e-01,  ...,  8.7666e-02,\n",
              "           -6.9810e-01,  8.6263e-02]],\n",
              "\n",
              "         [[ 3.3763e-01, -2.0805e-01, -3.4444e-01,  ...,  2.3931e-01,\n",
              "           -3.6649e+00,  2.3964e-02],\n",
              "          [ 1.0362e-01, -3.1600e-01,  2.7278e-01,  ...,  2.4204e-01,\n",
              "            9.2895e-02, -3.9845e-01],\n",
              "          [-1.0172e-02, -2.4284e-01, -2.4068e-01,  ...,  2.2366e-01,\n",
              "           -3.5977e-02,  1.0953e-01],\n",
              "          [-7.2490e-02,  1.9810e-03, -4.1424e-01,  ...,  2.0000e-01,\n",
              "           -3.6160e-01, -3.9004e-02],\n",
              "          [ 1.8403e-01, -7.3142e-02,  2.6546e-01,  ...,  2.2672e-01,\n",
              "           -2.5091e-01, -2.4397e-01],\n",
              "          [ 8.6073e-02,  1.0812e-01, -2.0256e-02,  ...,  1.7817e-01,\n",
              "           -9.8526e-02,  3.2952e-01]],\n",
              "\n",
              "         [[ 3.9806e-02, -1.4273e-01, -3.9806e-02,  ..., -1.9534e-01,\n",
              "            2.2148e-01, -8.1456e-02],\n",
              "          [ 4.7874e-02,  1.3381e-01, -3.5316e-01,  ..., -1.8026e-01,\n",
              "            3.5475e-01,  2.2342e-01],\n",
              "          [ 1.1435e-01,  8.2104e-04, -2.0935e-01,  ...,  8.7846e-02,\n",
              "            2.5217e-01, -4.6417e-02],\n",
              "          [-1.6734e-01,  2.0600e-01,  2.0624e-01,  ..., -4.3484e-02,\n",
              "           -7.6015e-03,  3.8539e-02],\n",
              "          [-5.1328e-01,  4.0679e-01,  2.8819e-01,  ..., -2.9411e-01,\n",
              "            5.2092e-02,  1.9975e-01],\n",
              "          [ 1.7818e-01, -2.6880e-01,  1.3732e-01,  ..., -7.4366e-02,\n",
              "            4.7067e-03,  4.5031e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-1.3064e-01, -1.0979e+00,  3.2432e-01,  ..., -6.3491e-01,\n",
              "           -1.5534e-01, -3.2244e-02],\n",
              "          [ 1.3725e-02, -2.0183e+00, -9.8645e-01,  ..., -1.1660e-01,\n",
              "            3.4448e-01, -6.6186e-01],\n",
              "          [ 9.4490e-02, -3.2758e+00,  6.1941e-01,  ...,  4.5878e-01,\n",
              "           -4.0015e-01, -2.0075e+00],\n",
              "          [ 2.3207e-02, -3.3768e+00,  7.4078e-01,  ..., -5.0500e-01,\n",
              "           -1.5626e-01, -6.0365e-01],\n",
              "          [ 1.0785e+00, -3.8969e+00, -1.2355e-01,  ..., -4.0394e-01,\n",
              "           -8.8049e-02, -5.8806e-01],\n",
              "          [-1.2229e-01, -2.7038e+00, -1.3312e-01,  ...,  1.2056e+00,\n",
              "            2.9294e-01, -5.9798e-01]],\n",
              "\n",
              "         [[-5.1276e-01,  2.7387e-01, -4.6331e-01,  ...,  1.2137e+00,\n",
              "           -5.0740e-01, -4.4837e-01],\n",
              "          [-1.8329e+00,  1.2899e-01, -6.6760e-01,  ...,  2.3605e-01,\n",
              "            7.1616e-01,  9.9932e-01],\n",
              "          [-1.2218e+00, -1.6648e+00, -1.4763e+00,  ...,  1.3128e+00,\n",
              "            5.2047e-01, -2.5405e-01],\n",
              "          [-2.2176e+00, -4.8461e-01, -2.6711e+00,  ...,  5.9749e-01,\n",
              "            1.7317e-02, -4.0113e-02],\n",
              "          [-1.3992e+00,  5.9290e-02, -2.1331e+00,  ..., -9.2057e-02,\n",
              "            1.4466e-01, -6.4651e-01],\n",
              "          [-1.5682e+00, -1.4826e+00, -1.6059e+00,  ...,  1.1557e+00,\n",
              "            1.0286e+00, -8.7543e-01]],\n",
              "\n",
              "         [[ 1.2301e+00,  3.0167e+00,  3.7457e+00,  ...,  6.4829e-01,\n",
              "            1.6559e+00, -7.5835e-01],\n",
              "          [-3.5377e+00,  1.1902e+00, -2.4225e+00,  ..., -2.9495e+00,\n",
              "            3.9361e+00, -1.2360e-01],\n",
              "          [-3.0972e+00,  1.4176e+00, -2.7547e+00,  ..., -2.9195e+00,\n",
              "            2.4607e+00, -3.0572e-01],\n",
              "          [-3.5733e+00,  1.2221e+00, -4.2962e+00,  ..., -3.7012e+00,\n",
              "            3.6193e+00,  7.1025e-02],\n",
              "          [-2.6796e+00,  5.2268e-01, -3.7430e+00,  ..., -4.3213e+00,\n",
              "            3.2149e+00,  4.7728e-01],\n",
              "          [-2.7994e+00, -7.4025e-01, -4.3877e+00,  ..., -3.7174e+00,\n",
              "            3.8463e+00,  3.0511e-01]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[ 1.3653e+00, -2.7223e+00, -2.6976e+00,  ...,  9.1960e-01,\n",
              "            4.5927e-01,  2.6761e+00],\n",
              "          [-2.4631e+00,  1.1095e+00, -2.1970e-01,  ...,  2.8535e-01,\n",
              "           -2.1046e+00,  5.3661e-02],\n",
              "          [-1.9357e+00,  1.4632e+00,  1.6262e+00,  ..., -5.9558e-01,\n",
              "           -1.7803e+00, -1.5960e-01],\n",
              "          [-2.7177e+00,  2.3906e+00,  5.6437e-01,  ..., -1.2695e+00,\n",
              "           -2.0717e+00, -1.3307e+00],\n",
              "          [-3.2488e+00,  2.5646e+00,  1.6430e+00,  ..., -1.1798e+00,\n",
              "           -2.7122e+00, -1.7752e+00],\n",
              "          [-2.9525e+00,  3.3346e+00,  1.1839e+00,  ..., -4.4591e-01,\n",
              "           -1.8934e+00, -1.0008e+00]],\n",
              "\n",
              "         [[ 1.7415e+00,  4.4209e-01,  9.3136e-01,  ..., -2.7852e-03,\n",
              "           -9.8478e-01, -3.0201e-01],\n",
              "          [ 1.6643e+00,  9.6782e-01,  9.4258e-01,  ...,  1.8557e-01,\n",
              "           -1.9957e+00, -1.6265e+00],\n",
              "          [ 2.4709e+00,  5.6700e-01,  1.1531e+00,  ..., -5.1622e-01,\n",
              "           -1.9563e+00, -1.4089e+00],\n",
              "          [ 2.4939e+00,  4.4018e-01,  1.4379e+00,  ..., -6.3855e-02,\n",
              "           -1.4510e+00, -1.6071e+00],\n",
              "          [ 2.4986e+00,  4.1564e-01,  1.4304e+00,  ...,  5.3095e-01,\n",
              "           -1.4236e+00, -1.3154e+00],\n",
              "          [ 2.0589e+00,  5.3403e-01,  1.0690e+00,  ...,  4.6139e-02,\n",
              "           -1.7872e+00, -1.1648e+00]],\n",
              "\n",
              "         [[-2.6354e-01,  1.7693e-01, -5.4590e-01,  ...,  2.5397e-01,\n",
              "            2.7925e-01,  1.6319e-01],\n",
              "          [-2.9182e-01,  9.7907e-01, -7.5173e-03,  ..., -1.2013e-01,\n",
              "            1.0109e-01,  1.5215e-01],\n",
              "          [-8.7742e-01,  7.6747e-01, -2.4190e-01,  ..., -1.4323e-01,\n",
              "            4.6133e-01, -2.6916e-01],\n",
              "          [-3.7645e-01,  5.4746e-01, -1.4316e-01,  ..., -1.1311e-01,\n",
              "            2.6597e-01, -3.7619e-01],\n",
              "          [-6.4325e-01,  1.1332e+00, -1.0315e-01,  ..., -2.3044e-02,\n",
              "            3.0609e-01, -2.5633e-01],\n",
              "          [-4.9360e-01, -3.6729e-01, -5.1951e-01,  ...,  7.0880e-01,\n",
              "            5.5634e-01,  6.3853e-01]]]], grad_fn=<PermuteBackward0>), tensor([[[[-5.0386e-04,  4.8796e-03, -1.4501e-01,  ..., -7.6289e-03,\n",
              "           -2.8119e-02, -5.4427e-01],\n",
              "          [ 6.3085e-01,  6.1058e-01, -6.1285e-01,  ..., -4.3984e-01,\n",
              "           -8.9345e-02,  9.5278e-01],\n",
              "          [-9.2913e-01,  3.0817e-01, -2.9516e-01,  ...,  7.7802e-01,\n",
              "            2.6756e-02,  9.5083e-01],\n",
              "          [ 2.7943e-01, -6.4588e-01, -2.3441e-01,  ..., -7.0097e-01,\n",
              "           -4.5664e-01,  4.8783e-01],\n",
              "          [ 1.8783e-01, -6.7494e-01,  5.9295e-01,  ..., -4.5358e-01,\n",
              "           -2.1184e-01,  4.8028e-01],\n",
              "          [-5.2241e-02,  6.8633e-01, -1.4014e+00,  ...,  4.3354e-01,\n",
              "           -2.8227e-02, -6.6725e-01]],\n",
              "\n",
              "         [[ 3.9716e-02,  1.0030e-02,  3.6718e-02,  ..., -4.8265e-02,\n",
              "           -3.5170e-03,  1.6775e-02],\n",
              "          [-3.5756e-01, -1.3532e-01, -6.9402e-02,  ..., -4.3818e-01,\n",
              "           -2.9149e-01,  4.3879e-02],\n",
              "          [-9.1146e-01,  5.2173e-01, -4.8033e-02,  ..., -4.0299e-01,\n",
              "           -7.1093e-01, -8.7185e-01],\n",
              "          [-4.4527e-01, -1.5029e-01,  2.5486e-01,  ..., -6.4516e-01,\n",
              "           -1.1133e+00, -2.9016e-01],\n",
              "          [ 6.1183e-01,  1.0872e-01,  2.2389e-01,  ..., -2.7703e-01,\n",
              "           -9.0769e-02, -6.8453e-01],\n",
              "          [ 1.7736e-01, -3.1208e-01,  7.4906e-01,  ..., -7.9562e-02,\n",
              "            1.1088e+00, -3.7057e-01]],\n",
              "\n",
              "         [[ 3.3389e-02, -7.5867e-01, -7.8789e-02,  ...,  5.0789e-02,\n",
              "            6.6967e-03, -5.6933e-02],\n",
              "          [ 5.7211e-02, -1.0848e+00,  2.4741e-01,  ...,  1.1169e-01,\n",
              "           -1.7808e-01,  3.4327e-01],\n",
              "          [-5.3668e-01, -1.4336e+00,  1.5879e-01,  ...,  2.2978e-02,\n",
              "           -3.9232e-01,  2.2563e-01],\n",
              "          [ 3.6796e-01, -1.5618e+00, -2.4339e-01,  ...,  4.5267e-01,\n",
              "           -5.6538e-02,  4.3667e-01],\n",
              "          [-2.6711e-01, -1.7929e+00, -2.4416e-01,  ..., -3.1658e-01,\n",
              "            2.3689e-01, -2.3922e-01],\n",
              "          [ 4.0544e-01, -1.2539e+00,  2.9625e-02,  ..., -9.3193e-02,\n",
              "            1.3547e-01, -1.2183e-01]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[ 3.6745e-02, -7.3889e-02,  1.3169e+00,  ..., -4.2897e-02,\n",
              "            1.9233e-01, -2.8341e-02],\n",
              "          [ 3.8117e-01,  2.6241e-01,  2.4748e+00,  ..., -8.5376e-02,\n",
              "            2.5405e-01,  2.1769e-01],\n",
              "          [-2.7466e-01, -6.6800e-01,  1.4769e+00,  ..., -5.6994e-01,\n",
              "           -9.5238e-02, -8.7423e-01],\n",
              "          [-2.0663e-02,  1.2560e-01,  1.1409e+00,  ...,  2.6658e-01,\n",
              "            2.4689e-01, -3.4657e-01],\n",
              "          [-1.2420e-01, -3.3136e-01,  1.2537e+00,  ..., -4.9859e-01,\n",
              "            2.7275e-03, -1.5627e-01],\n",
              "          [ 1.6148e-01, -9.2721e-01,  2.3590e+00,  ..., -1.4192e-01,\n",
              "            1.0982e-01,  3.7039e-01]],\n",
              "\n",
              "         [[ 3.3704e-03, -1.1483e-01, -1.7243e-01,  ...,  1.5501e-01,\n",
              "            1.0659e-01,  1.5490e-01],\n",
              "          [ 4.0766e-01, -1.4636e-01, -4.3799e-01,  ...,  2.4124e-01,\n",
              "           -6.0877e-01, -6.8683e-01],\n",
              "          [ 9.0057e-01, -1.0316e-01, -7.8812e-01,  ...,  2.2784e-01,\n",
              "           -3.7292e-01, -3.8934e-01],\n",
              "          [-1.5576e-01, -6.2389e-01,  4.8749e-01,  ...,  3.3859e-01,\n",
              "            1.6339e-01, -3.8100e-01],\n",
              "          [-4.0277e-02, -6.4219e-01, -2.0192e-01,  ...,  6.1986e-02,\n",
              "           -1.7428e-01, -5.9515e-01],\n",
              "          [ 1.1847e+00, -1.4801e-01,  4.0542e-02,  ..., -3.4410e-01,\n",
              "           -3.4345e-01,  1.8182e-01]],\n",
              "\n",
              "         [[ 1.4329e-02,  2.1596e-02,  2.4439e-02,  ..., -6.4790e-02,\n",
              "            2.3542e-01,  4.6180e-03],\n",
              "          [-1.8990e-01, -5.5295e-01,  1.8532e-01,  ..., -6.7860e-01,\n",
              "           -2.1723e+00,  7.9766e-03],\n",
              "          [ 4.2582e-02,  2.0382e-01, -5.9157e-01,  ...,  1.4240e-02,\n",
              "           -2.2094e+00,  3.7906e-01],\n",
              "          [ 3.0221e-02, -6.7867e-01, -1.3742e-01,  ...,  2.3376e-01,\n",
              "           -2.0616e+00,  3.0841e-01],\n",
              "          [ 1.2550e-01,  1.7969e-01, -6.3862e-01,  ..., -1.0343e-01,\n",
              "           -2.1398e+00,  4.9142e-01],\n",
              "          [ 2.2519e-01, -2.1449e-01,  4.3800e-01,  ..., -8.9251e-02,\n",
              "           -1.5421e+00, -3.9618e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[ 0.0278, -0.2285,  0.1759,  ..., -0.8720,  0.7339, -1.2083],\n",
              "          [ 0.0478, -0.1441, -0.9091,  ..., -0.4182, -1.3772,  1.1339],\n",
              "          [ 0.7627,  1.1873, -0.3928,  ...,  0.7769,  0.6196,  0.4921],\n",
              "          [-0.8145, -0.6908,  1.2485,  ...,  1.8469, -0.2819,  0.9744],\n",
              "          [ 0.9700,  0.9533, -0.1659,  ...,  0.6139, -0.8078, -0.4211],\n",
              "          [-0.5831, -0.4528,  0.8838,  ...,  1.0087,  0.0908, -0.4216]],\n",
              "\n",
              "         [[ 0.7876,  0.2094,  0.0446,  ..., -0.1404, -1.0861, -0.1898],\n",
              "          [ 1.1112, -1.8956,  1.0507,  ...,  1.9821,  5.5479,  1.0125],\n",
              "          [ 1.5453, -1.4164,  0.8450,  ...,  0.9391,  4.9938,  1.9131],\n",
              "          [ 1.4107, -1.3791, -2.2004,  ...,  0.8510,  5.4865,  2.4558],\n",
              "          [ 0.7021, -1.5814, -1.5249,  ..., -0.2607,  5.5710,  1.3889],\n",
              "          [-0.5512, -0.1419,  0.0736,  ..., -0.1554,  4.3222,  1.4929]],\n",
              "\n",
              "         [[ 0.3675, -0.3501, -0.3418,  ...,  0.3457,  1.4536,  0.2457],\n",
              "          [ 1.0300, -5.5901, -1.2632,  ..., -2.2341, -2.1683, -6.8352],\n",
              "          [ 0.3330, -6.2456, -0.9554,  ..., -2.2585, -3.5653, -5.3488],\n",
              "          [-1.0442, -6.6399, -0.8635,  ..., -4.4144, -3.1527, -6.4589],\n",
              "          [-1.8199, -6.3233, -1.8660,  ..., -5.0831, -3.7197, -6.0896],\n",
              "          [-2.1795, -6.5746, -1.7934,  ..., -3.6354, -2.0247, -5.4399]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[ 0.2211,  1.7470,  0.5168,  ...,  0.2433,  0.4647, -1.6733],\n",
              "          [-0.6683, -5.4727,  0.0866,  ..., -2.7056, -0.5983,  6.1132],\n",
              "          [-0.6446, -4.9307, -0.9359,  ..., -1.0986, -0.6394,  7.9648],\n",
              "          [ 2.0076, -4.3691,  2.2582,  ..., -1.4670, -2.2404,  8.4920],\n",
              "          [-0.0360, -3.3960,  1.4475,  ..., -3.2220, -2.8991,  8.1838],\n",
              "          [ 0.9417, -6.6598,  1.0966,  ..., -2.1420, -1.2194,  5.4850]],\n",
              "\n",
              "         [[ 0.0437, -0.0386,  0.1539,  ..., -0.1031, -0.0938, -0.1531],\n",
              "          [-0.1070, -1.9387, -0.0520,  ..., -0.5811, -0.6224, -1.2919],\n",
              "          [ 0.8936, -0.9874, -0.0857,  ..., -0.4946,  0.9981,  0.7153],\n",
              "          [ 0.9501, -0.2418, -1.2259,  ..., -1.1724, -0.2831, -0.9340],\n",
              "          [ 0.3350, -0.4703, -1.9612,  ..., -0.6313, -0.3998, -0.1783],\n",
              "          [ 0.0232, -0.2069, -0.3620,  ..., -0.7298, -0.3270, -0.1436]],\n",
              "\n",
              "         [[ 0.4139, -0.0507,  1.8796,  ..., -0.2293, -0.1941, -1.0033],\n",
              "          [ 3.4526,  1.4639, -2.2115,  ...,  0.9500,  2.4456,  2.7788],\n",
              "          [ 2.3418,  1.4140, -3.3231,  ...,  2.1847,  0.1398,  5.4528],\n",
              "          [ 1.6773,  0.4959, -2.1514,  ...,  1.4845,  0.4154,  5.6465],\n",
              "          [ 2.2856, -0.1520, -1.4455,  ...,  2.5380, -0.8606,  5.1368],\n",
              "          [ 3.3152,  1.9767, -3.4954,  ...,  0.8229,  1.2146,  4.3472]]]],\n",
              "       grad_fn=<PermuteBackward0>), tensor([[[[ 0.0393,  0.0640, -0.0040,  ...,  0.0148,  0.1003,  0.0364],\n",
              "          [ 0.1582, -1.1694, -0.0248,  ...,  0.2137, -0.5703, -0.5709],\n",
              "          [ 0.1754, -1.4146,  0.4659,  ...,  0.2240, -0.9869, -1.0116],\n",
              "          [ 0.3632, -0.6254, -0.1036,  ...,  0.4642, -1.3710, -1.3989],\n",
              "          [ 0.5230, -0.9575, -0.1848,  ..., -0.4282, -0.7455,  0.4404],\n",
              "          [ 0.2804, -0.6895,  0.4342,  ..., -0.0971, -0.8839,  0.0383]],\n",
              "\n",
              "         [[-0.0396,  0.0046,  0.0802,  ..., -0.0323, -0.0323, -0.0479],\n",
              "          [ 0.1648, -0.2043,  0.5372,  ...,  0.1427,  0.0635,  0.3155],\n",
              "          [ 0.4556, -0.1101,  0.1871,  ..., -0.9036,  0.4574, -0.0609],\n",
              "          [ 0.3413,  0.5869, -0.0881,  ...,  0.2356,  0.7448,  0.1079],\n",
              "          [ 0.0714, -0.1719,  0.0053,  ...,  0.2374, -0.4384,  0.0394],\n",
              "          [-0.0462,  0.0259, -0.1829,  ...,  0.1554,  0.2546, -0.2574]],\n",
              "\n",
              "         [[ 0.0398, -0.1029, -0.0551,  ..., -0.0279,  0.0944, -0.1543],\n",
              "          [-0.8137,  0.4013,  0.9105,  ..., -0.1257, -0.3255,  0.5048],\n",
              "          [-0.1281,  0.3516, -0.0126,  ...,  0.2341, -0.8913,  0.3460],\n",
              "          [ 0.2095, -0.1264,  0.1191,  ..., -0.1372, -0.4316, -0.3342],\n",
              "          [ 0.4889,  0.3173, -0.2661,  ..., -0.0685, -0.1831, -0.0832],\n",
              "          [ 0.4398,  0.4534, -0.5030,  ..., -0.0138, -0.1987,  0.0262]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[-0.0199,  0.1196,  0.0043,  ..., -0.0280,  0.0703, -0.0397],\n",
              "          [ 0.3715, -0.5477, -0.0329,  ..., -0.0597,  0.0434, -0.3601],\n",
              "          [ 0.1812, -0.3746, -0.4362,  ...,  0.1464,  0.0978,  0.7410],\n",
              "          [-0.0613, -0.3056,  0.0510,  ...,  0.4735,  0.5111, -0.5793],\n",
              "          [ 0.3057, -0.4663, -0.5908,  ...,  0.2281,  0.1876, -0.2980],\n",
              "          [ 0.7175, -0.1776,  0.2786,  ...,  0.5381, -0.0496, -0.4127]],\n",
              "\n",
              "         [[-0.1650, -0.1273, -0.0911,  ..., -0.2380, -0.0235, -0.0439],\n",
              "          [ 0.9110, -0.0087, -0.0261,  ...,  0.5544,  0.1531,  0.1284],\n",
              "          [ 1.2670, -0.7294,  0.4092,  ...,  0.0656,  0.7652,  0.3958],\n",
              "          [ 0.3569, -0.5794,  1.1983,  ..., -0.1765,  0.8304, -0.1362],\n",
              "          [ 0.2610, -0.4843,  0.7386,  ..., -0.1415,  0.7355,  0.1748],\n",
              "          [ 0.5808,  0.1995, -0.2671,  ..., -0.3098,  0.5678, -0.2362]],\n",
              "\n",
              "         [[ 0.1198, -0.0662, -0.0280,  ..., -0.0150, -0.0899, -0.1058],\n",
              "          [ 0.5997,  0.7461, -1.3864,  ..., -0.0461, -0.2745,  0.4968],\n",
              "          [-0.5353,  1.0639, -0.2839,  ...,  0.1107,  0.3240, -0.1728],\n",
              "          [-0.2476, -0.3575,  0.9907,  ...,  0.4448, -0.5969, -0.6610],\n",
              "          [-0.1843, -0.0406,  0.1821,  ..., -0.2379, -0.3760, -0.2287],\n",
              "          [ 0.6519,  0.1185, -0.1150,  ..., -0.4358,  0.0272,  0.6218]]]],\n",
              "       grad_fn=<PermuteBackward0>)), (tensor([[[[-8.7030e-01, -1.3107e-01,  3.3352e-01,  ..., -9.5781e-01,\n",
              "            2.2612e-02, -2.9464e+00],\n",
              "          [ 1.7508e+00,  1.7543e+00, -2.6911e+00,  ..., -2.8611e+00,\n",
              "           -1.9383e+00,  7.8015e+00],\n",
              "          [ 1.6322e+00,  2.8836e+00, -1.6611e+00,  ..., -2.2794e+00,\n",
              "           -1.7668e+00,  8.7002e+00],\n",
              "          [ 3.2166e+00,  1.8584e+00, -3.8383e+00,  ..., -2.3957e+00,\n",
              "           -5.1764e-01,  8.1900e+00],\n",
              "          [ 1.8284e+00,  1.1155e+00, -2.1240e+00,  ..., -1.3951e+00,\n",
              "           -1.6652e+00,  8.8558e+00],\n",
              "          [ 1.5516e+00, -8.1064e-01, -1.9661e+00,  ..., -2.3194e+00,\n",
              "           -2.0382e+00,  8.3906e+00]],\n",
              "\n",
              "         [[ 3.9207e-01, -8.7198e-02,  4.5695e-01,  ..., -1.4772e-01,\n",
              "           -5.9227e-02, -2.2404e+00],\n",
              "          [-1.1583e+00, -6.0464e-01,  4.2464e+00,  ..., -1.9573e+00,\n",
              "            7.2344e-02,  5.8803e+00],\n",
              "          [-9.9543e-01, -3.3555e-01,  4.2528e+00,  ..., -3.0640e+00,\n",
              "           -5.9468e-01,  7.2892e+00],\n",
              "          [-2.8287e-01, -4.4087e-01,  3.6727e+00,  ..., -2.6997e+00,\n",
              "           -5.9051e-02,  6.0405e+00],\n",
              "          [-4.9784e-01,  1.6855e+00,  2.6839e+00,  ..., -3.6506e-01,\n",
              "           -2.8762e-01,  6.5555e+00],\n",
              "          [-4.3405e-02,  5.8385e-01,  4.5669e+00,  ..., -5.1437e-01,\n",
              "           -1.6460e-03,  6.2617e+00]],\n",
              "\n",
              "         [[ 1.3705e-01, -6.6146e-01, -2.0491e-01,  ...,  1.4911e-01,\n",
              "            2.5535e-01, -1.7442e-01],\n",
              "          [-3.7873e-01,  1.7907e+00,  1.3241e+00,  ...,  6.8441e-01,\n",
              "            7.1830e-02,  7.4523e-01],\n",
              "          [ 5.3523e-01,  3.0362e+00, -8.1811e-01,  ..., -2.8611e-01,\n",
              "            1.8527e-02,  7.9273e-01],\n",
              "          [ 1.8743e+00,  2.5305e+00,  4.8280e-01,  ..., -6.6031e-01,\n",
              "           -5.2698e-01, -1.0195e+00],\n",
              "          [ 1.3534e+00,  1.8063e+00,  7.0721e-01,  ..., -1.1865e+00,\n",
              "            9.8181e-01,  1.0169e-01],\n",
              "          [ 4.9880e-01,  2.8054e+00, -6.0689e-02,  ..., -1.0687e+00,\n",
              "           -2.0174e-02, -4.3129e-02]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[-3.7748e-01,  2.7054e-02, -1.0139e-02,  ...,  1.2542e+00,\n",
              "            6.6676e-02,  1.7786e+00],\n",
              "          [ 2.0922e+00, -1.8368e+00,  8.8316e-01,  ..., -2.6662e+00,\n",
              "           -5.0644e-01, -2.3432e+00],\n",
              "          [ 4.5503e-01, -2.3442e+00,  1.7964e-01,  ..., -1.8412e+00,\n",
              "           -8.8821e-01, -1.8638e+00],\n",
              "          [ 1.5239e-01, -1.3705e+00, -1.1869e+00,  ..., -2.4681e+00,\n",
              "           -1.0691e+00, -1.4761e-01],\n",
              "          [ 4.0168e-01, -5.1137e-01, -1.2740e-02,  ..., -3.1662e+00,\n",
              "           -2.5876e-01, -4.5227e-01],\n",
              "          [ 4.5449e-01, -1.0575e+00, -1.5642e+00,  ..., -3.3170e+00,\n",
              "           -4.0152e-01, -5.7120e-02]],\n",
              "\n",
              "         [[-3.3143e-01, -1.4473e-01,  2.1405e-01,  ...,  2.5461e-01,\n",
              "           -2.4395e-02,  9.5474e-03],\n",
              "          [ 6.6630e-01, -3.1565e-01,  7.3383e-01,  ...,  1.0558e+00,\n",
              "           -8.0450e-02,  4.5648e-01],\n",
              "          [ 9.0545e-01,  4.1605e-01, -1.2812e+00,  ...,  1.1237e+00,\n",
              "           -3.2837e-01,  9.0404e-01],\n",
              "          [-1.0038e+00, -8.8085e-01, -3.4357e-01,  ...,  1.3417e-01,\n",
              "            8.5924e-01, -4.9534e-01],\n",
              "          [-4.2319e-01, -3.7186e-01,  1.1485e-01,  ..., -8.9013e-01,\n",
              "           -2.8913e-01, -2.8248e-01],\n",
              "          [-5.7276e-01, -2.5535e-01,  1.8486e+00,  ...,  8.8906e-01,\n",
              "           -9.9078e-02,  7.4036e-01]],\n",
              "\n",
              "         [[ 3.4086e+00,  2.1957e+00, -2.0634e+00,  ..., -2.8445e+00,\n",
              "           -3.8668e+00, -1.2118e+00],\n",
              "          [-4.2556e+00,  2.0300e+00,  6.3396e+00,  ..., -4.0227e+00,\n",
              "            1.3018e+01,  8.2304e-01],\n",
              "          [-6.2067e+00, -1.1700e-01,  7.0761e+00,  ..., -2.4648e+00,\n",
              "            1.0669e+01,  1.3504e+00],\n",
              "          [-3.1606e+00,  4.1534e-01,  7.9025e+00,  ..., -8.9768e-01,\n",
              "            9.0784e+00, -2.1386e+00],\n",
              "          [-2.5403e+00, -2.2678e+00,  1.0921e+01,  ..., -9.5943e-01,\n",
              "            7.2750e+00, -2.7901e+00],\n",
              "          [-5.3488e+00, -3.2197e+00,  1.1355e+01,  ..., -4.9830e-01,\n",
              "            1.0649e+01, -2.7508e+00]]]], grad_fn=<PermuteBackward0>), tensor([[[[-8.1242e-03, -4.6875e-02,  2.8140e-02,  ...,  6.3446e-02,\n",
              "            3.7524e-02,  6.2208e-02],\n",
              "          [-4.0258e-01,  2.0352e-01, -1.7190e-01,  ..., -6.8618e-03,\n",
              "           -8.6012e-02,  8.1696e-02],\n",
              "          [-1.0193e+00, -1.1423e-01, -1.6346e-02,  ..., -5.5846e-02,\n",
              "            5.8015e-01, -5.1616e-01],\n",
              "          [-6.5421e-01,  1.8192e-01,  4.0396e-01,  ..., -9.4589e-02,\n",
              "           -3.4431e-01, -2.3813e-03],\n",
              "          [-7.5336e-01, -6.5432e-02, -1.1092e-01,  ..., -1.2421e-01,\n",
              "            4.9972e-01, -4.0450e-01],\n",
              "          [-3.5663e-01,  4.3362e-02,  1.4280e-01,  ..., -1.8175e-01,\n",
              "           -2.4874e-01, -1.8776e-01]],\n",
              "\n",
              "         [[-7.9340e-02, -1.9839e-02, -1.3748e-01,  ..., -4.5362e-02,\n",
              "            4.7141e-02, -2.1514e-02],\n",
              "          [-3.6559e-01,  5.7533e-01,  1.9134e-01,  ...,  3.0759e-01,\n",
              "           -3.3098e-01,  7.0523e-01],\n",
              "          [-2.8882e-01,  7.2680e-01, -3.7786e-01,  ...,  9.2409e-01,\n",
              "           -3.2663e-01,  1.9276e-01],\n",
              "          [-9.1745e-02,  5.9616e-01, -5.2608e-01,  ...,  1.3736e+00,\n",
              "            1.0730e+00,  4.0497e-01],\n",
              "          [ 8.6104e-02,  2.5348e-01,  5.7327e-01,  ...,  6.3453e-01,\n",
              "            3.1625e-01, -1.9989e-01],\n",
              "          [-3.7981e-01,  2.5935e-01,  6.0768e-03,  ...,  4.6034e-02,\n",
              "           -7.8008e-02,  9.7197e-02]],\n",
              "\n",
              "         [[ 6.4877e-02,  1.0147e-01,  9.0731e-02,  ...,  2.4853e-02,\n",
              "           -7.0408e-02,  2.1704e-03],\n",
              "          [-1.6649e-01,  7.3457e-01, -6.5314e-01,  ..., -4.6060e-01,\n",
              "            1.8070e-01, -9.1629e-01],\n",
              "          [-4.0540e-01,  8.4780e-01,  6.8936e-02,  ...,  7.4165e-01,\n",
              "           -9.9329e-02, -1.4099e+00],\n",
              "          [-7.4623e-01, -5.2256e-02, -7.5383e-01,  ..., -6.0665e-01,\n",
              "            8.7008e-01, -1.1510e-01],\n",
              "          [-4.9174e-01,  2.1036e-01, -7.5342e-02,  ..., -2.0890e-01,\n",
              "            9.6522e-01,  1.4085e+00],\n",
              "          [ 3.7004e-03,  3.7339e-01, -5.4795e-01,  ...,  2.4594e-01,\n",
              "           -4.9088e-01,  6.3562e-01]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[ 6.9637e-04,  8.0578e-02, -8.2013e-02,  ...,  4.4803e-02,\n",
              "            3.8873e-02, -1.2982e-01],\n",
              "          [ 7.5419e-01,  5.5860e-01,  3.7413e-01,  ...,  5.4127e-01,\n",
              "           -4.2004e-01, -1.9566e-01],\n",
              "          [ 9.5900e-01, -9.3648e-01, -1.3508e-01,  ...,  1.0731e+00,\n",
              "            2.8844e-01,  1.2426e+00],\n",
              "          [-9.9729e-01,  7.5944e-02,  1.6969e-02,  ..., -5.7996e-01,\n",
              "           -8.3307e-02,  1.4138e+00],\n",
              "          [-4.6124e-01,  5.4417e-01,  6.3979e-01,  ...,  2.2375e-01,\n",
              "            7.5013e-01,  5.5173e-01],\n",
              "          [ 3.4234e-01,  5.5962e-01, -4.9762e-01,  ..., -1.2503e-01,\n",
              "           -2.2732e-01, -5.4930e-01]],\n",
              "\n",
              "         [[-1.2790e-01, -5.0632e-02,  1.0964e-01,  ..., -7.0735e-02,\n",
              "            5.3396e-02, -2.0372e-02],\n",
              "          [ 3.8034e-01, -8.5625e-01, -5.8978e-01,  ..., -5.0662e-02,\n",
              "           -1.1192e-01,  7.9341e-01],\n",
              "          [ 3.9383e-01,  1.5811e+00, -1.4667e+00,  ..., -1.0406e-01,\n",
              "           -1.2417e+00,  1.1340e+00],\n",
              "          [ 8.5347e-01,  7.9993e-02, -2.0703e-01,  ...,  1.1377e+00,\n",
              "           -2.2900e-01, -2.6040e-01],\n",
              "          [ 8.8632e-01, -9.1077e-02,  2.5467e-01,  ...,  7.0645e-01,\n",
              "           -7.1997e-01, -4.2250e-01],\n",
              "          [ 5.1949e-01, -7.1596e-01,  2.0953e-01,  ...,  6.7525e-02,\n",
              "           -3.8477e-01, -5.5342e-01]],\n",
              "\n",
              "         [[-1.7268e-02, -1.3456e-02, -2.0527e-02,  ..., -2.6838e-02,\n",
              "            9.7050e-03, -1.7938e-02],\n",
              "          [-4.5637e-01, -1.8936e-01, -1.2677e-01,  ..., -1.7269e-01,\n",
              "            6.1539e-01, -4.1034e-01],\n",
              "          [-1.0600e-01, -8.4440e-02, -1.1388e-01,  ..., -7.7690e-01,\n",
              "            3.2156e-01, -4.0246e-01],\n",
              "          [-3.7841e-01, -5.7820e-01, -4.3206e-01,  ...,  2.5440e-01,\n",
              "            3.8117e-01,  1.9341e-01],\n",
              "          [-4.1153e-01, -5.8443e-01, -1.6665e-01,  ...,  1.4526e-01,\n",
              "            3.4736e-02,  3.0570e-01],\n",
              "          [ 3.6316e-01, -1.0582e-02,  6.6335e-02,  ..., -1.2839e-01,\n",
              "            2.4856e-01, -2.0173e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[ 2.4765e-02, -2.8574e-01,  2.1615e-01,  ...,  1.6883e+00,\n",
              "           -2.1490e-01, -7.0574e-02],\n",
              "          [ 5.1595e-01,  1.3149e+00,  5.7712e-01,  ..., -3.9323e+00,\n",
              "            9.6912e-01, -1.2394e+00],\n",
              "          [-2.9693e-02,  1.1793e+00, -6.5967e-01,  ..., -4.2372e+00,\n",
              "            5.7690e-01, -1.6926e+00],\n",
              "          [-1.0371e+00,  2.7154e+00,  1.7781e-02,  ..., -4.3839e+00,\n",
              "            2.4234e+00, -2.6274e+00],\n",
              "          [-1.4991e+00, -7.6971e-01, -1.1588e+00,  ..., -4.3928e+00,\n",
              "            1.3730e-01, -1.6429e+00],\n",
              "          [-1.4735e+00,  3.1871e-01, -1.0389e+00,  ..., -4.9317e+00,\n",
              "           -3.9254e-01, -1.7967e+00]],\n",
              "\n",
              "         [[ 1.8058e-01,  9.8178e-01, -1.4240e+00,  ..., -1.2947e-01,\n",
              "            2.6640e-01,  9.1793e-01],\n",
              "          [-8.5669e-01, -5.4721e+00,  1.0800e+00,  ..., -1.6963e+00,\n",
              "            1.9019e+00, -1.0892e+00],\n",
              "          [-1.1117e+00, -4.9173e+00,  7.8284e-01,  ..., -1.4940e+00,\n",
              "            2.4796e-02, -4.9983e-01],\n",
              "          [ 4.8947e-02, -4.7938e+00,  9.8131e-01,  ..., -1.8192e+00,\n",
              "            2.6485e-01, -1.0620e+00],\n",
              "          [-1.6141e+00, -5.0709e+00,  2.0999e+00,  ..., -1.1873e+00,\n",
              "            7.5435e-02, -3.0764e+00],\n",
              "          [-7.8986e-01, -4.4016e+00,  3.6907e+00,  ...,  2.6976e-01,\n",
              "            9.4088e-01, -1.7091e+00]],\n",
              "\n",
              "         [[-6.6871e-01,  2.4777e-01, -4.1925e-02,  ...,  1.8080e-01,\n",
              "            4.8689e-02, -2.9203e-01],\n",
              "          [ 2.3664e+00,  1.5596e-01, -1.5351e-01,  ..., -1.3722e+00,\n",
              "            2.1714e-02, -3.2923e-01],\n",
              "          [ 3.0142e+00, -8.3327e-01, -1.5481e+00,  ..., -9.4648e-01,\n",
              "           -1.2877e+00, -2.2055e-01],\n",
              "          [ 2.4744e+00, -1.6211e+00, -1.2062e+00,  ..., -5.7362e-01,\n",
              "           -1.1508e+00,  2.0608e-01],\n",
              "          [ 1.8560e+00, -8.5713e-01, -4.7251e-01,  ..., -5.3552e-02,\n",
              "            6.4264e-01,  5.7514e-01],\n",
              "          [ 1.8156e+00, -4.1867e-01, -1.1279e+00,  ..., -7.1365e-01,\n",
              "           -7.6609e-01,  7.6921e-01]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[-4.0074e-02,  1.1457e-01,  1.4191e-01,  ..., -9.7227e-02,\n",
              "            2.0769e-02,  1.5848e-01],\n",
              "          [-2.3402e-01,  4.9729e-01,  3.6498e-01,  ...,  1.6001e+00,\n",
              "           -4.0757e-01,  5.1833e-01],\n",
              "          [-5.5897e-01,  1.3897e+00, -3.8576e-01,  ...,  8.3981e-01,\n",
              "           -1.2595e+00,  2.2492e+00],\n",
              "          [ 1.4373e+00,  6.6219e-01,  4.0389e-01,  ...,  1.9539e-01,\n",
              "            7.2377e-01,  6.8051e-01],\n",
              "          [ 1.3718e+00, -2.8739e-02,  8.1679e-01,  ...,  6.2033e-01,\n",
              "            4.3560e-01,  9.2334e-01],\n",
              "          [ 1.1913e+00, -9.2510e-02, -1.0526e+00,  ...,  3.5844e-01,\n",
              "           -9.0007e-01,  6.9788e-01]],\n",
              "\n",
              "         [[-3.0022e+00,  3.9174e-01, -1.1223e-02,  ..., -4.7967e-01,\n",
              "           -3.3196e-01,  1.2360e+00],\n",
              "          [ 5.4858e+00, -4.9417e-01, -9.7123e-01,  ..., -1.0712e+00,\n",
              "            8.3288e-01, -1.9873e-01],\n",
              "          [ 6.1512e+00,  5.3546e-01, -4.4482e-01,  ..., -6.0983e-01,\n",
              "            2.1007e+00, -5.2643e-01],\n",
              "          [ 5.2942e+00, -1.0248e+00,  5.2528e-01,  ..., -5.5483e-01,\n",
              "            9.7236e-02,  2.2984e-01],\n",
              "          [ 4.3117e+00,  4.9781e-01, -1.3096e-01,  ..., -1.6206e+00,\n",
              "            4.4512e-01,  7.8787e-01],\n",
              "          [ 4.4591e+00, -7.0723e-01, -1.7599e+00,  ..., -1.6576e+00,\n",
              "            2.9140e-01, -1.0702e+00]],\n",
              "\n",
              "         [[-7.1124e-04, -2.4458e-01,  2.4069e-03,  ..., -1.7966e-01,\n",
              "            3.2623e-01,  7.5991e-02],\n",
              "          [ 1.2148e+00, -1.7890e+00,  6.7829e-01,  ..., -9.1060e-01,\n",
              "            9.8833e-01, -2.1769e+00],\n",
              "          [ 1.1859e+00, -1.2763e+00, -9.6002e-01,  ..., -1.0801e+00,\n",
              "            1.7483e+00, -1.4504e+00],\n",
              "          [-5.3131e-01, -2.2688e-01,  2.3142e-01,  ...,  4.4283e-02,\n",
              "           -3.9822e-01,  4.4697e-02],\n",
              "          [-2.4831e-01, -8.3690e-01,  1.3477e-01,  ...,  4.1984e-01,\n",
              "           -5.1679e-01,  9.5962e-01],\n",
              "          [ 5.9277e-01, -2.4693e+00, -3.6349e-01,  ...,  6.1139e-01,\n",
              "           -6.1808e-02, -9.8944e-02]]]], grad_fn=<PermuteBackward0>), tensor([[[[-1.6154e-02, -2.4324e-02,  4.0457e-03,  ..., -5.0684e-03,\n",
              "           -4.3276e-02,  3.5572e-01],\n",
              "          [ 2.6818e+00,  1.7537e-01,  9.3541e-01,  ..., -1.3259e+00,\n",
              "            2.1633e-01, -6.9552e-01],\n",
              "          [ 1.1938e-01,  5.1399e-01,  1.3852e+00,  ..., -5.6911e-01,\n",
              "            3.1499e-01, -1.1596e+00],\n",
              "          [ 7.0013e-01,  4.4819e-01,  4.6311e-01,  ...,  9.3848e-01,\n",
              "            3.2797e-01, -1.0059e+00],\n",
              "          [-2.8077e-01, -8.3947e-01, -1.2542e-02,  ...,  9.7510e-02,\n",
              "            5.8868e-01, -9.3605e-01],\n",
              "          [ 1.0464e-01,  8.9705e-01, -3.9863e-01,  ...,  7.1537e-01,\n",
              "            4.6298e-01, -1.2573e+00]],\n",
              "\n",
              "         [[ 2.8516e-03, -2.1652e-02,  2.0662e-02,  ..., -1.2961e-02,\n",
              "            2.0125e-02, -4.7159e-03],\n",
              "          [ 1.1268e+00,  3.0521e-01,  1.0834e-01,  ..., -2.1078e-01,\n",
              "            2.3302e+00, -2.1748e-01],\n",
              "          [ 2.7536e-01, -3.8793e-01,  8.1447e-01,  ..., -9.7594e-01,\n",
              "            2.7138e+00, -1.7310e+00],\n",
              "          [ 6.0071e-01, -1.4351e-01,  6.5462e-01,  ..., -8.7777e-01,\n",
              "            1.5363e+00, -9.1005e-02],\n",
              "          [-3.3877e-02, -7.7362e-01, -4.3469e-01,  ..., -1.5966e+00,\n",
              "            1.5244e+00,  7.6661e-01],\n",
              "          [-7.4897e-02, -1.5574e-01,  6.8695e-01,  ..., -1.7175e+00,\n",
              "            1.3172e+00, -1.1867e+00]],\n",
              "\n",
              "         [[-5.5537e-02,  4.8931e-03, -4.0918e-02,  ..., -3.8937e-02,\n",
              "            3.6256e-04, -8.0482e-02],\n",
              "          [-4.4262e-01, -8.4464e-01, -4.2454e-01,  ..., -5.0922e-01,\n",
              "            1.6153e-01, -1.2626e+00],\n",
              "          [-2.9793e-01, -2.4387e-01,  2.4497e-03,  ..., -4.4222e-01,\n",
              "            4.2989e-01, -6.3693e-01],\n",
              "          [ 6.0853e-01, -8.0155e-01, -8.8478e-02,  ..., -3.7859e-01,\n",
              "           -2.6182e-01, -8.8943e-01],\n",
              "          [-2.3984e-01,  7.0767e-02,  9.5194e-01,  ..., -5.9044e-01,\n",
              "           -3.1295e-01, -3.2710e-01],\n",
              "          [-1.0847e+00,  2.2831e-01,  1.5102e+00,  ..., -5.4047e-01,\n",
              "            2.6387e-01,  1.0495e-01]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[-3.2976e-01, -2.0310e-01, -7.0779e-02,  ..., -4.7672e-01,\n",
              "            2.2137e-01,  9.6389e-02],\n",
              "          [ 5.7016e-01, -1.0929e+00, -4.9122e-01,  ...,  3.6517e+00,\n",
              "            1.7164e-01,  9.8449e-01],\n",
              "          [ 1.5448e+00, -1.8355e+00,  1.6478e-01,  ...,  3.4966e+00,\n",
              "           -3.1032e-01,  7.3325e-01],\n",
              "          [ 1.4630e+00, -2.2654e+00,  4.9638e-01,  ...,  2.6043e+00,\n",
              "           -4.1595e-01, -1.8811e-02],\n",
              "          [ 1.4644e+00, -1.6342e+00,  1.4693e-01,  ...,  6.7189e-01,\n",
              "           -4.5184e-01, -8.7228e-01],\n",
              "          [ 1.8949e+00, -1.1935e+00, -1.7366e-01,  ...,  2.0890e+00,\n",
              "            3.4335e-01, -6.6367e-01]],\n",
              "\n",
              "         [[-8.1667e-02, -1.3310e-01, -4.5393e-02,  ..., -1.8094e-01,\n",
              "           -1.4460e-01,  1.1990e-01],\n",
              "          [ 3.9043e-01,  1.7310e-01,  8.6875e-02,  ...,  1.2848e-01,\n",
              "           -5.3353e-01,  6.6141e-01],\n",
              "          [ 7.7109e-01,  8.2094e-01, -1.0062e-01,  ...,  2.6518e-01,\n",
              "            2.1024e-01, -1.1626e-01],\n",
              "          [ 1.6811e-01,  3.2902e-01, -2.4722e-01,  ...,  3.5839e-01,\n",
              "            3.0618e-01,  4.5357e-01],\n",
              "          [ 6.5340e-02,  1.4469e-01, -9.8414e-01,  ...,  6.6134e-02,\n",
              "            6.6428e-01, -3.0888e-01],\n",
              "          [-2.2953e-01, -3.7004e-01,  5.0883e-03,  ...,  3.9493e-01,\n",
              "           -1.8374e-01, -4.1303e-01]],\n",
              "\n",
              "         [[-3.1941e-02, -3.7102e-02,  8.7104e-02,  ...,  8.3825e-02,\n",
              "           -3.5994e-02,  2.2552e-02],\n",
              "          [-5.0377e-01,  3.8704e-02, -1.5550e+00,  ..., -3.8794e-01,\n",
              "           -5.9796e-02,  1.3067e+00],\n",
              "          [-5.6683e-01,  3.0900e-01, -1.2810e+00,  ...,  1.9473e+00,\n",
              "           -4.0892e-01,  1.0743e+00],\n",
              "          [-3.3321e-01,  2.1906e+00, -1.8495e+00,  ...,  7.5185e-01,\n",
              "            2.8149e-01,  1.3113e-01],\n",
              "          [ 1.4400e+00,  5.9131e-01, -1.0731e+00,  ..., -6.8547e-01,\n",
              "            9.1401e-01,  1.1784e-02],\n",
              "          [-3.2880e-01, -5.6723e-01, -1.6043e+00,  ...,  8.7968e-01,\n",
              "           -8.0441e-01,  4.6702e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-3.3482e-01,  8.5828e-01, -1.5853e-01,  ...,  1.1293e+00,\n",
              "           -1.6286e-01,  1.3779e-01],\n",
              "          [-1.2525e+00, -4.4456e+00,  6.4178e-01,  ..., -3.5176e+00,\n",
              "            7.5164e-01,  1.2281e+00],\n",
              "          [ 4.8553e-02, -3.7374e+00, -1.0323e+00,  ..., -4.0431e+00,\n",
              "            1.0465e-01,  1.9350e+00],\n",
              "          [ 3.6695e-01, -4.0209e+00,  3.4245e-03,  ..., -3.7186e+00,\n",
              "            8.3725e-01,  1.8465e+00],\n",
              "          [ 4.3565e-01, -4.1859e+00, -5.1394e-01,  ..., -3.9392e+00,\n",
              "            3.4864e-01,  2.7611e+00],\n",
              "          [-2.0830e-01, -4.2439e+00,  2.1758e-01,  ..., -4.0875e+00,\n",
              "            2.8291e-01,  9.3790e-01]],\n",
              "\n",
              "         [[ 4.7401e-02,  8.7476e-01, -6.4117e-01,  ..., -2.1939e-02,\n",
              "            2.8363e-01,  1.4145e-02],\n",
              "          [-1.1709e-01,  2.6250e+00,  2.0271e+00,  ...,  1.9873e+00,\n",
              "           -1.5678e+00, -3.5490e-01],\n",
              "          [ 2.8169e-01,  3.8268e+00,  1.6449e-01,  ...,  7.6413e-01,\n",
              "           -3.6457e-01,  4.6030e-01],\n",
              "          [-3.2218e-02,  1.5030e+00,  1.0374e+00,  ...,  2.2536e-01,\n",
              "            2.3796e-01,  1.3107e-01],\n",
              "          [-1.4555e+00,  6.1394e-02,  2.0230e-01,  ...,  5.7679e-01,\n",
              "           -3.4119e-01, -3.6901e-02],\n",
              "          [ 1.0269e-01,  1.2891e+00,  1.4301e+00,  ...,  2.1459e+00,\n",
              "            1.2098e+00, -1.6379e-01]],\n",
              "\n",
              "         [[-3.1181e-01,  1.4279e-01, -9.9188e-01,  ..., -3.5612e-01,\n",
              "           -4.8890e-02, -1.3194e-01],\n",
              "          [-1.2945e+00, -1.1560e-01,  3.1124e+00,  ..., -3.0171e-01,\n",
              "           -1.0849e+00,  1.7216e+00],\n",
              "          [-1.8600e-01,  3.3889e-01,  4.2001e+00,  ...,  4.7704e-01,\n",
              "           -8.2659e-01,  4.6822e-01],\n",
              "          [-4.4651e-01,  2.4318e-01,  3.6639e+00,  ...,  1.3950e-02,\n",
              "            3.2386e-01,  1.0890e+00],\n",
              "          [-7.9998e-01,  8.1838e-02,  2.6315e+00,  ...,  2.2924e-01,\n",
              "           -5.7973e-01,  2.8440e-01],\n",
              "          [-3.7387e-01,  1.1863e+00,  2.4528e+00,  ..., -3.6433e-01,\n",
              "           -1.1857e-01,  7.4698e-01]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[ 3.8220e-01,  7.4177e-02, -5.5519e-02,  ..., -4.9372e-02,\n",
              "            2.2708e-01,  1.6501e-02],\n",
              "          [ 1.0162e-01, -2.1764e-01,  1.2341e+00,  ...,  3.4856e-01,\n",
              "            2.8324e+00, -1.0510e+00],\n",
              "          [-1.5150e+00,  5.8457e-01,  7.7998e-02,  ..., -6.6996e-01,\n",
              "            2.7045e+00, -1.4888e+00],\n",
              "          [-6.2087e-01, -4.8617e-01, -4.7685e-01,  ...,  1.1970e+00,\n",
              "            1.6407e+00, -1.2734e+00],\n",
              "          [ 9.5686e-01, -1.9545e+00,  6.6601e-01,  ..., -2.2669e-01,\n",
              "           -3.0115e+00, -9.5266e-02],\n",
              "          [-3.0050e-01,  4.7663e-01,  3.4278e-01,  ...,  3.7904e-01,\n",
              "           -7.6638e-01,  3.7996e-01]],\n",
              "\n",
              "         [[ 2.0809e-01,  6.1432e-02,  3.1973e-01,  ...,  4.1752e-01,\n",
              "            9.5741e-03,  2.3014e-01],\n",
              "          [ 1.2577e+00, -2.8466e-01,  5.4628e-01,  ..., -1.8791e+00,\n",
              "            5.7396e-01, -5.5332e-01],\n",
              "          [ 1.1116e+00, -7.5737e-01,  6.4844e-01,  ..., -1.5645e+00,\n",
              "           -1.5809e+00,  4.1169e-01],\n",
              "          [ 8.3905e-01, -7.5067e-01,  1.3814e+00,  ..., -1.8298e+00,\n",
              "           -8.3398e-01,  3.1912e-01],\n",
              "          [-4.9134e-02,  6.5244e-01,  2.0986e+00,  ..., -1.7663e+00,\n",
              "           -8.1021e-01,  1.2176e+00],\n",
              "          [ 2.2110e+00, -1.1678e+00,  1.1681e+00,  ..., -1.1528e+00,\n",
              "           -8.9433e-01,  1.5714e+00]],\n",
              "\n",
              "         [[-3.0148e+00,  5.4584e-01,  5.5762e-01,  ..., -9.4472e-01,\n",
              "            3.3175e-01,  1.9976e-01],\n",
              "          [ 8.4171e+00, -9.6737e-01, -3.2526e+00,  ...,  1.6170e+00,\n",
              "           -1.0802e+00,  1.2372e+00],\n",
              "          [ 9.6897e+00, -1.1331e+00, -1.7853e+00,  ...,  2.3783e+00,\n",
              "           -1.4423e+00,  1.4789e-01],\n",
              "          [ 6.9400e+00,  3.7749e-01, -2.5889e+00,  ...,  1.6472e+00,\n",
              "           -1.2179e+00, -3.9979e-01],\n",
              "          [ 7.2968e+00, -3.2558e-01, -1.7686e+00,  ...,  1.4138e+00,\n",
              "           -8.2296e-01, -4.4660e-01],\n",
              "          [ 8.8654e+00, -8.1741e-01, -1.0308e+00,  ...,  2.6950e+00,\n",
              "           -4.0656e-01, -6.9206e-01]]]], grad_fn=<PermuteBackward0>), tensor([[[[ 3.6623e-02, -5.2531e-02,  1.9742e-02,  ..., -7.7511e-02,\n",
              "            7.2724e-04, -8.8710e-02],\n",
              "          [-1.1080e-01, -1.9711e-01, -3.8658e-01,  ...,  1.5363e-01,\n",
              "           -2.0910e-01, -1.9224e-01],\n",
              "          [-5.1661e-01, -9.1689e-01, -8.5505e-01,  ...,  1.4782e-01,\n",
              "            3.2896e-01, -4.5628e-01],\n",
              "          [ 1.0344e-01, -2.4564e-01,  8.3946e-01,  ..., -9.9901e-02,\n",
              "           -1.1686e+00, -5.5571e-01],\n",
              "          [ 2.5556e-02, -9.2472e-01,  3.6053e-01,  ..., -1.8009e-01,\n",
              "            7.7921e-02, -1.7845e-01],\n",
              "          [-3.5530e-01, -2.3453e-01,  8.8834e-01,  ...,  3.4967e-01,\n",
              "            6.0329e-01, -2.0708e-01]],\n",
              "\n",
              "         [[ 6.4450e-02,  2.1530e-02, -2.1496e-02,  ..., -2.4472e-02,\n",
              "            1.2992e-02, -4.4305e-03],\n",
              "          [-3.7108e-01,  7.0646e-01, -1.8276e-01,  ...,  7.4364e-01,\n",
              "           -3.2046e-01, -3.6888e-02],\n",
              "          [ 3.0719e-01,  6.4914e-01,  2.9484e-01,  ..., -7.5102e-01,\n",
              "            5.8702e-01, -4.8220e-01],\n",
              "          [-6.7293e-02, -5.7148e-01, -1.5933e+00,  ...,  1.6733e+00,\n",
              "           -1.8511e-01,  1.1920e+00],\n",
              "          [ 1.9644e-01, -5.6456e-01,  1.9579e-01,  ...,  2.2131e-01,\n",
              "            3.5443e-01, -1.2213e-01],\n",
              "          [ 1.3444e+00,  1.0929e+00,  9.4127e-03,  ...,  3.8020e-01,\n",
              "            1.3439e-01,  8.0993e-01]],\n",
              "\n",
              "         [[ 7.8684e-02,  1.9472e-02,  6.3902e-03,  ...,  1.7769e-02,\n",
              "           -6.6467e-02, -6.4331e-02],\n",
              "          [-5.3634e-01, -1.6938e+00, -3.6196e-01,  ..., -6.0521e-01,\n",
              "            1.4639e-01,  1.1799e+00],\n",
              "          [-1.1545e+00,  3.0059e-02, -4.4696e-01,  ..., -1.3903e+00,\n",
              "            5.3299e-01,  8.2690e-01],\n",
              "          [ 4.6064e-01, -2.1372e-01, -4.0844e-01,  ...,  2.5939e-01,\n",
              "            6.1937e-01,  3.0829e-01],\n",
              "          [-8.8537e-02, -5.4331e-01, -5.6786e-02,  ..., -2.9024e-01,\n",
              "            4.7331e-01, -1.0161e+00],\n",
              "          [ 2.2295e-01, -6.5099e-01, -2.2750e-01,  ..., -2.1274e-01,\n",
              "            1.0690e-01, -5.1326e-01]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[ 5.2003e-03,  2.2984e-02,  2.6918e-02,  ..., -8.5574e-02,\n",
              "           -2.0333e-02,  2.4534e-02],\n",
              "          [-1.1725e+00,  5.2927e-01, -1.9076e-02,  ...,  2.2893e+00,\n",
              "           -2.0483e+00, -9.2695e-01],\n",
              "          [-5.1094e-01, -3.8951e-02, -1.6645e-01,  ...,  8.1070e-01,\n",
              "            6.3388e-02, -5.0653e-01],\n",
              "          [-7.8262e-01, -1.8614e+00,  9.9201e-01,  ...,  1.1582e+00,\n",
              "           -9.7156e-01, -1.5937e+00],\n",
              "          [-1.0454e+00, -7.7303e-01, -3.5567e-01,  ...,  2.2553e+00,\n",
              "            7.6001e-01,  8.3173e-01],\n",
              "          [-1.7469e-01, -1.2151e+00,  3.7551e-01,  ...,  6.4112e-01,\n",
              "           -3.1412e-01,  2.1092e-01]],\n",
              "\n",
              "         [[ 3.6519e-02, -1.5498e-02,  2.4342e-02,  ...,  3.0825e-02,\n",
              "           -7.8301e-03, -2.5399e-03],\n",
              "          [ 2.8326e+00,  4.7027e-01, -7.8561e-01,  ...,  4.3802e-01,\n",
              "           -1.0660e+00,  7.5608e-01],\n",
              "          [ 7.8388e-01,  2.0792e+00, -8.3427e-02,  ..., -7.0464e-01,\n",
              "           -9.3525e-01,  2.5855e-01],\n",
              "          [-4.5254e-02,  1.1874e+00,  4.2804e-03,  ..., -3.2944e-01,\n",
              "            4.4935e-01, -5.8823e-01],\n",
              "          [-8.1041e-01,  1.1329e+00, -6.9417e-01,  ..., -8.7374e-02,\n",
              "            1.4765e+00, -8.4150e-01],\n",
              "          [ 1.9055e-01,  1.3285e+00, -9.7200e-01,  ..., -4.0545e-01,\n",
              "            2.0765e-01, -7.5219e-01]],\n",
              "\n",
              "         [[ 7.5280e-02, -1.9492e-01, -7.6429e-02,  ..., -1.8599e-02,\n",
              "            1.9642e-01, -4.4872e-02],\n",
              "          [-6.9956e-01,  4.3262e-01, -8.7427e-02,  ...,  7.6113e-01,\n",
              "           -6.8946e-01,  1.3790e+00],\n",
              "          [-4.3296e-01,  1.7071e-01,  1.2394e+00,  ..., -6.6777e-01,\n",
              "           -1.6473e-01,  8.7541e-01],\n",
              "          [ 3.1399e-01,  2.5564e-01,  7.4330e-01,  ..., -2.0756e-01,\n",
              "           -6.9116e-01,  6.8337e-01],\n",
              "          [ 1.7068e-01,  2.9172e-01,  6.7051e-02,  ..., -2.6251e-01,\n",
              "            1.0682e+00,  9.0947e-01],\n",
              "          [-5.2537e-01, -1.1229e+00, -7.3799e-03,  ..., -4.7540e-01,\n",
              "            4.1600e-02,  1.3140e+00]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[ 1.0622, -0.2432, -0.1308,  ...,  0.6292,  0.7119, -0.2988],\n",
              "          [-3.5992, -1.1062,  0.0161,  ...,  0.9225, -4.8857,  0.6919],\n",
              "          [-4.7897, -0.0594,  0.1695,  ...,  0.3728, -4.7533,  0.9813],\n",
              "          [-3.9776, -0.7992,  0.6504,  ..., -0.9531, -3.8319,  1.6347],\n",
              "          [-2.7718, -1.2607,  0.6090,  ..., -1.5250, -4.7130,  1.2392],\n",
              "          [-4.1433, -1.2856, -0.5402,  ..., -0.8239, -3.8812,  1.3331]],\n",
              "\n",
              "         [[-0.1469, -0.0684,  0.1629,  ..., -0.0474, -0.8726, -0.2045],\n",
              "          [ 0.1264, -0.4389, -0.8180,  ...,  0.1548,  0.4406,  0.3716],\n",
              "          [ 0.5156, -1.1103, -0.2882,  ..., -0.4894,  0.1624, -0.0589],\n",
              "          [-1.6527, -1.0563, -0.6072,  ..., -1.1017,  0.0535, -0.0354],\n",
              "          [-3.1063, -0.0724, -1.0717,  ..., -1.0462, -0.1414,  0.4964],\n",
              "          [-2.5628, -0.5012,  0.5066,  ..., -1.5240,  0.0604, -0.1344]],\n",
              "\n",
              "         [[ 0.1925,  0.2996,  1.1201,  ..., -0.4592,  0.4389, -0.5034],\n",
              "          [-0.4540, -2.2230, -3.8891,  ..., -1.9251, -2.1660,  1.1903],\n",
              "          [-1.5588, -1.6678, -1.8163,  ..., -1.6715, -1.9626,  1.9236],\n",
              "          [-1.0955, -0.5170, -1.2186,  ..., -0.8014, -1.7353,  1.9815],\n",
              "          [ 0.5364,  0.5999, -2.7679,  ..., -0.6558, -1.7384,  1.8979],\n",
              "          [ 0.3696, -0.5783, -2.5453,  ..., -1.1821, -0.2278,  1.6621]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[-0.1420,  0.0799, -0.2224,  ...,  0.0068,  0.1565,  0.0186],\n",
              "          [-0.7659, -0.5092, -1.6443,  ...,  2.0583, -0.1097, -1.7782],\n",
              "          [-1.1256, -1.2050, -0.9941,  ...,  0.9366,  0.0403, -1.2565],\n",
              "          [-1.0389, -0.8849, -0.9914,  ...,  0.5020, -0.4605, -0.6806],\n",
              "          [-0.6823, -1.4069,  0.4629,  ...,  1.0612,  0.3512, -0.8425],\n",
              "          [-0.7486,  0.9669, -0.7371,  ...,  0.9725, -0.8985, -1.8500]],\n",
              "\n",
              "         [[-0.3561, -2.1814,  0.1236,  ..., -0.0884, -0.0492,  0.9104],\n",
              "          [-0.5912,  4.1895,  0.1894,  ..., -0.6465, -0.9461, -1.5394],\n",
              "          [-0.3283,  2.6835,  0.2440,  ..., -0.0904, -0.9839, -0.9499],\n",
              "          [-0.0228,  2.5660,  0.6615,  ...,  0.9821, -0.9375,  0.6673],\n",
              "          [ 0.8376,  4.1883,  0.0836,  ...,  1.7137, -0.5141, -0.3587],\n",
              "          [-0.5995,  2.3097, -1.2796,  ...,  2.1329, -1.3652,  0.1674]],\n",
              "\n",
              "         [[ 0.3734,  0.0663, -0.1479,  ...,  0.6412,  0.1363,  0.2640],\n",
              "          [-0.7882, -0.2995, -1.8492,  ..., -0.8530,  0.7999, -2.4095],\n",
              "          [-1.3832, -1.0268,  0.0482,  ..., -0.0870,  2.3460, -0.5641],\n",
              "          [ 0.1036,  0.4912,  0.1360,  ..., -0.0875,  1.6843,  0.6221],\n",
              "          [-1.7499, -0.3015, -0.0989,  ...,  0.8762,  1.8987,  0.5572],\n",
              "          [-1.0835,  1.0173,  0.0188,  ...,  0.3827,  0.9723,  0.4069]]]],\n",
              "       grad_fn=<PermuteBackward0>), tensor([[[[-3.4548e-02,  5.3397e-02, -6.4730e-02,  ..., -2.3077e-02,\n",
              "            5.7564e-05,  2.0528e-02],\n",
              "          [ 1.0016e+00, -4.0137e-01,  1.2147e+00,  ...,  1.4849e-01,\n",
              "           -6.8773e-01, -1.4645e+00],\n",
              "          [ 5.4379e-01,  6.3111e-01,  9.8342e-01,  ...,  4.3895e-01,\n",
              "           -4.2241e-01, -5.7790e-01],\n",
              "          [ 3.5831e-02,  8.4904e-02, -1.3572e-01,  ...,  3.4934e-01,\n",
              "           -3.7194e-01,  2.3089e-01],\n",
              "          [ 4.4857e-01,  3.8879e-01, -2.6029e-02,  ...,  4.3995e-01,\n",
              "           -2.1893e-02, -9.0961e-03],\n",
              "          [ 5.1104e-03,  6.2186e-01, -3.7488e-01,  ..., -3.3123e-01,\n",
              "            1.9742e-01, -5.3203e-01]],\n",
              "\n",
              "         [[ 1.1421e-02, -2.3286e-02,  2.4031e-02,  ...,  1.9458e-02,\n",
              "           -5.3157e-02,  1.4022e-02],\n",
              "          [-4.0418e-01, -1.2997e-01, -7.2872e-01,  ...,  1.8599e-01,\n",
              "           -4.5109e-01, -1.0680e+00],\n",
              "          [-6.4167e-01,  8.6063e-01, -6.2534e-01,  ..., -1.0698e+00,\n",
              "           -3.2913e-01, -1.0043e+00],\n",
              "          [-3.9532e-02, -8.7186e-01,  8.1230e-01,  ..., -7.6409e-01,\n",
              "            1.0357e+00,  2.3721e-01],\n",
              "          [ 7.9328e-02,  3.1447e-02,  5.6908e-01,  ...,  1.0151e+00,\n",
              "            6.6934e-01, -8.3340e-01],\n",
              "          [ 7.6557e-01,  2.8478e-01, -9.8829e-01,  ...,  4.2863e-01,\n",
              "            4.6228e-01, -1.6146e+00]],\n",
              "\n",
              "         [[ 4.1737e-02, -2.4639e-02,  5.5960e-02,  ...,  2.9685e-02,\n",
              "           -5.1147e-03, -2.1416e-03],\n",
              "          [-9.6392e-01, -1.4922e+00,  8.6303e-01,  ...,  1.4004e+00,\n",
              "            8.2056e-01,  1.8219e+00],\n",
              "          [-6.0176e-01, -1.4259e+00,  2.6957e-01,  ...,  8.5750e-01,\n",
              "           -4.4388e-02, -2.8364e-02],\n",
              "          [-1.1069e+00,  9.1786e-02, -7.8640e-01,  ..., -9.5629e-01,\n",
              "           -8.3392e-01, -1.2265e+00],\n",
              "          [ 2.8198e-01,  2.7305e-02,  2.5204e-02,  ...,  5.3052e-01,\n",
              "            1.7246e+00, -6.3740e-01],\n",
              "          [ 8.8451e-01,  3.3576e-01,  1.1889e+00,  ..., -4.6685e-01,\n",
              "            1.5539e-01,  8.0855e-02]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[-1.8734e-01,  7.9658e-02,  5.4667e-02,  ...,  4.3881e-02,\n",
              "            4.9607e-02, -1.2179e-01],\n",
              "          [-8.3049e-01,  8.2956e-01, -1.9838e+00,  ..., -2.4472e-01,\n",
              "            1.1366e-01,  3.4451e-01],\n",
              "          [-4.3028e-01,  6.7652e-01, -1.0306e+00,  ...,  1.5450e-01,\n",
              "           -4.8128e-01, -1.3193e-01],\n",
              "          [-4.9271e-01, -2.5497e-01, -8.6658e-01,  ...,  8.5101e-01,\n",
              "            9.0670e-01,  6.6104e-01],\n",
              "          [-1.8823e-01, -9.4259e-01, -2.0459e+00,  ...,  1.0838e+00,\n",
              "            4.8472e-01, -2.5469e-02],\n",
              "          [ 2.9852e-01,  5.7317e-01, -2.5249e-01,  ...,  6.7642e-01,\n",
              "            6.1211e-01, -2.9425e-01]],\n",
              "\n",
              "         [[-5.9185e-01, -1.7446e-05,  4.2870e-02,  ..., -1.6821e-02,\n",
              "            1.4827e-02, -6.8937e-03],\n",
              "          [-1.3500e+00,  4.2742e-01, -1.3228e+00,  ...,  5.6385e-01,\n",
              "            1.3627e-01, -1.1372e-01],\n",
              "          [-3.3173e-01, -9.5323e-02, -5.2836e-01,  ...,  6.6250e-01,\n",
              "            5.8353e-01, -9.4523e-01],\n",
              "          [-1.8403e+00,  4.8461e-02, -1.0567e+00,  ...,  8.1026e-01,\n",
              "           -2.9524e-01, -2.2251e-01],\n",
              "          [-1.1981e+00, -7.9309e-01, -1.9822e-02,  ...,  7.7858e-01,\n",
              "           -3.7621e-01, -4.4632e-01],\n",
              "          [-1.4880e+00,  2.3929e-01, -5.9485e-01,  ..., -7.8657e-01,\n",
              "            4.3988e-01, -1.1763e-02]],\n",
              "\n",
              "         [[-2.5289e-03,  8.2326e-02, -4.4994e-02,  ...,  5.7166e-02,\n",
              "            3.3988e-02, -4.4381e-02],\n",
              "          [-1.1708e+00,  5.0415e-01,  1.5769e-01,  ..., -5.0099e-01,\n",
              "           -2.8502e-01, -1.6045e+00],\n",
              "          [-3.2575e-01,  4.2468e-01,  9.2236e-01,  ...,  4.3526e-01,\n",
              "           -1.2245e+00, -1.0126e+00],\n",
              "          [ 4.6151e-01,  8.7212e-03, -3.7324e-01,  ..., -1.1099e+00,\n",
              "           -1.7222e+00, -2.3369e-01],\n",
              "          [ 5.1246e-01, -6.0969e-01, -4.3360e-01,  ..., -1.4428e+00,\n",
              "           -1.7156e+00, -1.1080e+00],\n",
              "          [-3.3381e-01, -2.3586e-01, -7.1717e-02,  ..., -9.0389e-01,\n",
              "            4.7265e-01,  5.7925e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-3.7207e-02, -2.3316e+00,  1.6703e-01,  ..., -2.2565e-01,\n",
              "           -1.9570e-01,  7.1027e-02],\n",
              "          [-5.9362e-01,  4.3922e+00,  1.2814e+00,  ...,  2.0506e-01,\n",
              "            1.3838e+00,  3.7458e-01],\n",
              "          [ 5.5426e-01,  4.9177e+00,  1.3381e+00,  ..., -3.2952e-01,\n",
              "           -4.1370e-01,  3.2551e-01],\n",
              "          [ 1.2551e-01,  4.8533e+00,  2.1883e-01,  ...,  1.2863e+00,\n",
              "            6.2305e-01, -9.9431e-01],\n",
              "          [-9.0767e-01,  4.8285e+00,  1.4908e+00,  ...,  7.6037e-02,\n",
              "           -3.9099e-01,  4.7480e-01],\n",
              "          [-1.6611e+00,  4.0597e+00,  4.8230e-01,  ..., -3.8383e-01,\n",
              "           -1.0657e-01, -2.2896e-01]],\n",
              "\n",
              "         [[-7.9816e-01,  2.2714e-01,  4.6665e-01,  ..., -5.2178e-01,\n",
              "            1.0731e+00,  1.1169e+00],\n",
              "          [ 4.6041e-01,  1.3515e+00,  6.0871e-01,  ...,  1.4881e+00,\n",
              "            1.3897e-01, -1.8870e+00],\n",
              "          [-5.6738e-01,  1.4699e+00,  3.1994e-01,  ..., -6.9045e-01,\n",
              "           -1.0569e-01, -1.1483e+00],\n",
              "          [ 2.7215e-01,  9.5285e-01, -2.3676e-01,  ...,  1.4574e+00,\n",
              "            1.3281e+00, -6.8950e-01],\n",
              "          [ 4.1020e-01,  7.4241e-01, -2.5598e-01,  ...,  2.2400e+00,\n",
              "            1.7098e+00, -4.2011e-01],\n",
              "          [ 3.9844e-01,  1.1133e+00, -8.4200e-02,  ...,  3.0032e-02,\n",
              "            2.7038e+00, -1.3606e+00]],\n",
              "\n",
              "         [[-8.5172e-01,  4.6461e-01,  2.0545e-02,  ...,  4.9692e-01,\n",
              "           -2.3368e-01,  1.1502e+00],\n",
              "          [ 7.0471e-01, -1.8002e+00,  1.5480e-01,  ...,  7.2190e-01,\n",
              "            3.4133e-01,  5.3574e-01],\n",
              "          [ 1.2685e+00, -1.0912e+00,  1.0906e-01,  ...,  2.7605e-01,\n",
              "            1.6596e-01, -5.7380e-01],\n",
              "          [ 1.3617e+00, -6.1328e-01,  2.2902e-01,  ...,  3.6071e-01,\n",
              "            1.0656e-03, -1.5399e-01],\n",
              "          [ 7.9895e-01, -3.7729e-01,  6.6074e-01,  ...,  6.5469e-01,\n",
              "            3.3417e-02, -6.8874e-01],\n",
              "          [ 4.6927e-01, -7.5745e-01,  1.1760e+00,  ...,  7.8610e-01,\n",
              "            6.8137e-02, -3.8556e-01]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[-3.0616e-01, -1.4787e-01,  1.3669e-01,  ...,  1.8873e-01,\n",
              "            1.7284e+00, -2.8581e+00],\n",
              "          [ 1.6069e+00, -2.3576e+00, -7.0319e-01,  ..., -8.7629e-01,\n",
              "           -5.1247e+00,  6.1593e+00],\n",
              "          [ 2.0990e+00, -1.7908e+00, -4.1003e-01,  ..., -7.3284e-01,\n",
              "           -5.3661e+00,  5.1166e+00],\n",
              "          [ 9.4110e-01, -1.2163e+00, -3.5492e-01,  ..., -1.2988e+00,\n",
              "           -5.7477e+00,  4.1709e+00],\n",
              "          [ 3.3125e-01, -1.6730e-01, -3.4695e-01,  ..., -3.7573e-01,\n",
              "           -4.4614e+00,  4.2270e+00],\n",
              "          [ 1.2056e+00, -4.2655e-01, -4.4296e-01,  ..., -1.5052e+00,\n",
              "           -5.7380e+00,  5.0548e+00]],\n",
              "\n",
              "         [[ 1.8310e-01,  3.6814e-01,  2.2611e-01,  ..., -2.1254e-01,\n",
              "            2.7709e-02, -1.4778e-01],\n",
              "          [-1.4965e+00, -9.3200e-01,  6.4269e-02,  ...,  5.1434e-01,\n",
              "            1.9869e-01, -3.8550e-01],\n",
              "          [-1.5542e+00, -1.2209e+00, -9.3105e-01,  ...,  4.0812e-01,\n",
              "           -2.3918e-01, -8.5009e-01],\n",
              "          [-1.0915e+00, -1.1933e+00, -9.3470e-01,  ...,  1.4855e+00,\n",
              "           -4.2301e-02,  6.9526e-01],\n",
              "          [-5.3673e-01, -1.3222e+00, -5.9678e-01,  ...,  9.9332e-01,\n",
              "            4.5712e-02, -1.4001e-01],\n",
              "          [-6.8870e-01,  1.3693e-01, -8.6034e-01,  ...,  8.6072e-01,\n",
              "           -9.0343e-01,  1.3857e-01]],\n",
              "\n",
              "         [[ 3.6597e-01,  1.1065e-01,  6.1236e-01,  ...,  5.2692e-01,\n",
              "            5.7217e-01, -3.3553e-01],\n",
              "          [-1.2398e+00, -1.5088e+00, -1.7914e+00,  ..., -1.5708e+00,\n",
              "           -4.3450e+00,  1.1038e-01],\n",
              "          [-2.4276e+00, -1.8199e+00, -1.1861e+00,  ..., -1.1671e+00,\n",
              "           -4.6124e+00,  3.0262e-01],\n",
              "          [ 4.0639e-01, -1.4590e+00, -5.7597e-01,  ..., -5.5747e-01,\n",
              "           -4.1715e+00,  3.5338e-01],\n",
              "          [-5.5726e-03, -1.3057e+00,  3.2239e-01,  ..., -1.6857e-01,\n",
              "           -3.3845e+00, -6.8563e-02],\n",
              "          [-8.2617e-01, -1.2604e+00,  3.9021e-02,  ..., -1.3793e+00,\n",
              "           -4.4951e+00,  3.7345e-01]]]], grad_fn=<PermuteBackward0>), tensor([[[[ 5.4477e-02, -1.0079e-02, -7.1997e-03,  ...,  1.2584e-01,\n",
              "           -6.7115e-02, -3.4897e-02],\n",
              "          [-2.3350e-02, -7.7292e-01, -6.8354e-01,  ...,  8.3756e-01,\n",
              "            9.9730e-01, -8.1692e-02],\n",
              "          [-6.5309e-01,  7.3907e-01, -2.6762e-01,  ...,  1.3291e+00,\n",
              "            8.0594e-01, -7.2559e-01],\n",
              "          [-2.9077e-01, -3.9681e-01,  1.3560e+00,  ..., -7.3916e-01,\n",
              "           -3.2538e-01,  5.5844e-01],\n",
              "          [-2.2303e+00, -6.5809e-01,  6.0276e-01,  ..., -2.0536e-01,\n",
              "            8.3765e-01,  7.9675e-02],\n",
              "          [-9.8033e-01,  4.7385e-01, -7.7479e-01,  ...,  2.8721e-01,\n",
              "           -2.9462e-02, -9.2476e-01]],\n",
              "\n",
              "         [[-6.2822e-04,  3.5647e-02,  4.8210e-02,  ..., -7.1053e-03,\n",
              "           -7.6824e-03,  1.6443e-03],\n",
              "          [-2.0298e+00,  6.5653e-01,  1.9411e+00,  ...,  2.0333e+00,\n",
              "            3.1527e-01,  1.0739e+00],\n",
              "          [-8.9249e-01, -9.5878e-01,  1.2723e+00,  ...,  9.9460e-01,\n",
              "           -7.0347e-01,  8.4456e-01],\n",
              "          [-7.7376e-01, -8.3531e-01,  1.2222e+00,  ...,  1.0078e-01,\n",
              "           -9.2584e-01,  3.2086e-01],\n",
              "          [-4.8355e-01, -9.4924e-02,  3.7075e-01,  ...,  2.0194e-01,\n",
              "            3.3439e-01, -1.7295e-01],\n",
              "          [-7.8586e-01, -2.0758e-01,  2.3349e-01,  ..., -1.1099e-01,\n",
              "           -8.8055e-01, -4.8879e-01]],\n",
              "\n",
              "         [[ 5.2694e-02, -4.0038e-02,  6.1282e-02,  ...,  4.4516e-02,\n",
              "           -6.2781e-02, -6.8190e-02],\n",
              "          [-2.0529e-03, -9.0418e-01, -9.4592e-01,  ..., -8.4291e-01,\n",
              "            1.0987e+00, -6.0153e-02],\n",
              "          [-7.8738e-01,  7.9043e-01,  4.4491e-01,  ...,  4.7417e-01,\n",
              "            7.6887e-01, -1.3695e-01],\n",
              "          [-7.0412e-01,  5.1678e-01,  3.6020e-01,  ..., -3.5438e-01,\n",
              "           -3.3918e-01,  8.7149e-02],\n",
              "          [-8.0383e-01, -8.3688e-01,  5.7470e-01,  ..., -8.5378e-01,\n",
              "           -7.9301e-01,  3.1049e-01],\n",
              "          [-1.0165e+00,  4.7358e-01, -9.7129e-02,  ..., -1.0274e+00,\n",
              "            9.4269e-02,  1.1214e-01]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[-9.5199e-02, -4.4339e-02,  4.9220e-02,  ..., -8.3539e-02,\n",
              "            5.0388e-02,  7.1123e-03],\n",
              "          [-5.9128e-01, -8.3941e-01,  1.0536e+00,  ..., -1.1120e+00,\n",
              "           -3.0707e-01,  8.0751e-01],\n",
              "          [-1.3601e-01,  3.9578e-01, -2.7580e-01,  ..., -8.1312e-01,\n",
              "            5.3467e-01,  9.6678e-01],\n",
              "          [-7.8487e-01,  1.2114e+00, -1.1233e+00,  ..., -6.1899e-01,\n",
              "            5.1284e-02,  1.6614e+00],\n",
              "          [-6.1110e-01, -1.5127e-03, -1.7263e+00,  ...,  6.9541e-02,\n",
              "           -3.2281e-02,  1.7019e+00],\n",
              "          [ 1.1126e-01, -1.7696e-01,  6.9595e-01,  ..., -4.2558e-01,\n",
              "            9.2986e-01, -1.3433e-02]],\n",
              "\n",
              "         [[ 1.3284e-01, -7.5167e-02,  1.2040e-01,  ...,  5.7788e-02,\n",
              "            2.8678e-02, -1.3395e-01],\n",
              "          [-2.3350e+00,  1.1374e+00, -1.6088e+00,  ...,  1.8349e-01,\n",
              "           -4.4839e-01,  6.5219e-01],\n",
              "          [-1.2199e+00, -5.3009e-01, -2.2838e+00,  ...,  1.4053e+00,\n",
              "            8.3301e-01,  8.5951e-01],\n",
              "          [-2.1905e+00, -1.0763e+00, -1.1167e+00,  ...,  2.8992e+00,\n",
              "           -3.2272e+00,  2.0099e+00],\n",
              "          [-1.4494e+00, -4.8295e-01, -1.0523e+00,  ...,  6.4848e-01,\n",
              "           -1.2064e+00, -6.7560e-01],\n",
              "          [-1.7448e+00, -1.0998e-01, -5.8667e-01,  ...,  9.8461e-01,\n",
              "           -4.7924e-01, -1.9353e-01]],\n",
              "\n",
              "         [[ 2.1119e-01, -4.4821e-02, -5.3789e-02,  ...,  4.3649e-02,\n",
              "            5.0222e-02,  2.0442e-02],\n",
              "          [-1.5172e-01, -6.4075e-01,  5.3211e-01,  ..., -3.2948e-02,\n",
              "           -4.8430e-01, -3.1675e-01],\n",
              "          [ 4.5864e-01,  1.0365e-01,  6.0283e-01,  ..., -9.9612e-01,\n",
              "            7.8603e-01, -5.6929e-01],\n",
              "          [ 4.8841e-02,  2.0888e+00,  2.9520e-01,  ..., -1.5227e+00,\n",
              "            2.8560e-01, -3.3142e-01],\n",
              "          [ 6.8440e-03, -6.4114e-01,  8.4954e-02,  ...,  7.0213e-01,\n",
              "            1.4509e+00, -1.1652e+00],\n",
              "          [-8.2612e-01,  1.4824e+00,  1.1647e+00,  ...,  1.2872e+00,\n",
              "           -1.6486e-01, -4.7960e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[ 0.0200, -0.2452, -0.4425,  ...,  0.3212,  0.3173,  0.3776],\n",
              "          [-0.0679,  0.2633,  0.3316,  ...,  0.1958, -1.2484, -0.8579],\n",
              "          [ 0.0929,  0.9737,  0.4271,  ...,  0.3681, -0.8846,  0.2366],\n",
              "          [ 0.8373,  0.2666, -0.5879,  ...,  0.1759, -0.1402,  0.8047],\n",
              "          [ 0.7131, -0.6639, -1.4788,  ..., -0.1000,  0.6749,  2.0664],\n",
              "          [ 0.5725,  0.3369, -0.8202,  ...,  1.1855, -0.8625,  1.9822]],\n",
              "\n",
              "         [[-0.2836,  0.1654,  0.1153,  ...,  0.0489, -1.1488, -0.1509],\n",
              "          [ 0.3927, -0.6577, -0.5794,  ...,  1.7782,  0.5212,  0.5387],\n",
              "          [-0.2971,  0.3516, -0.7071,  ..., -0.4335, -0.1234, -0.0613],\n",
              "          [ 0.5634,  0.3129,  1.0815,  ..., -0.2831, -0.0934,  1.1478],\n",
              "          [-0.0990,  1.1519,  0.7738,  ...,  0.6360, -0.8015,  0.5829],\n",
              "          [-1.0153,  1.5480, -0.4128,  ..., -0.3065,  1.6150,  0.7077]],\n",
              "\n",
              "         [[-1.2462, -0.1084,  0.5545,  ..., -0.6596,  0.4746, -0.2796],\n",
              "          [ 1.9915, -0.0545, -0.5929,  ...,  0.5586, -0.3925,  1.3713],\n",
              "          [ 3.7305,  0.9538, -0.3449,  ...,  1.3642, -0.1608,  1.1240],\n",
              "          [ 1.1766,  1.2215, -0.7511,  ...,  1.4840, -0.0075,  0.0502],\n",
              "          [ 0.8893,  1.2568, -0.5829,  ...,  0.0602,  0.1813,  1.1269],\n",
              "          [ 1.9826,  1.6607, -0.6548,  ...,  0.1812, -0.8497, -0.3110]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[ 0.7893, -0.9110, -0.3952,  ..., -1.0467, -0.4146,  0.4948],\n",
              "          [ 0.2853,  0.2080, -1.5923,  ...,  0.5082,  0.1523, -0.5372],\n",
              "          [-0.0294,  1.4696, -2.9787,  ...,  0.9933, -0.3180, -0.2894],\n",
              "          [-0.5058,  0.8156, -1.6326,  ...,  0.8870, -1.6961, -2.1074],\n",
              "          [ 0.3247,  0.0082, -0.8735,  ..., -0.9679, -0.9444, -0.8488],\n",
              "          [ 0.6044,  0.5360, -1.3501,  ...,  0.5249,  0.3610, -0.5921]],\n",
              "\n",
              "         [[-0.9110,  2.5492,  0.3216,  ...,  0.3547,  1.9495, -0.5243],\n",
              "          [ 1.7240, -2.9237,  1.2609,  ..., -0.0811, -4.9036,  1.2008],\n",
              "          [ 2.8472, -1.6889,  1.7736,  ..., -0.1800, -5.5071,  0.9130],\n",
              "          [ 0.4954, -2.8508,  0.6091,  ...,  1.2232, -4.1723,  0.9641],\n",
              "          [-0.3964, -3.0195,  0.1010,  ...,  0.5583, -3.8583,  1.1830],\n",
              "          [ 1.3521, -3.3449,  0.1219,  ..., -0.0282, -3.6390,  1.0923]],\n",
              "\n",
              "         [[-2.0122, -0.3675, -1.1128,  ..., -0.3889,  0.0583,  0.2524],\n",
              "          [ 3.0838,  0.3365,  0.2964,  ...,  0.5554, -0.4763,  1.2107],\n",
              "          [ 2.0000,  0.1467,  1.2487,  ...,  0.1594, -1.4486,  0.2864],\n",
              "          [ 2.6381, -0.2924,  0.1479,  ..., -0.4627, -0.6980,  0.1370],\n",
              "          [ 2.6070, -0.2033,  0.0909,  ..., -1.2359, -0.3538, -0.6430],\n",
              "          [ 1.7504,  0.6858,  2.0758,  ..., -0.3202, -1.0301,  0.0471]]]],\n",
              "       grad_fn=<PermuteBackward0>), tensor([[[[-5.5811e-02, -8.1160e-02,  3.5665e-02,  ...,  1.0902e-01,\n",
              "           -2.7879e-02,  2.5281e-02],\n",
              "          [-8.8006e-03,  4.5670e-01,  3.7311e-01,  ..., -5.2422e-01,\n",
              "           -3.2976e-01,  3.2919e-01],\n",
              "          [ 2.3648e-02,  4.9421e-01,  1.0085e-01,  ..., -7.6901e-01,\n",
              "            3.6245e-01,  9.8110e-01],\n",
              "          [-3.0649e-01, -5.2405e-01, -5.0697e-02,  ...,  2.6667e-01,\n",
              "           -2.7957e-01,  2.3545e-01],\n",
              "          [-7.7527e-01, -8.8126e-01,  1.6188e-01,  ...,  7.0002e-01,\n",
              "           -5.3137e-01, -3.8244e-01],\n",
              "          [ 4.3921e-01, -8.4610e-02,  7.7424e-01,  ...,  1.0834e+00,\n",
              "           -1.0135e+00,  7.3192e-01]],\n",
              "\n",
              "         [[ 1.6063e-02,  7.9707e-03, -3.7657e-02,  ...,  2.3601e-02,\n",
              "            1.4850e-02,  4.8284e-02],\n",
              "          [-1.9371e-01,  5.1645e-01,  2.3493e+00,  ...,  9.8825e-01,\n",
              "            1.7443e-02, -6.8609e-01],\n",
              "          [ 3.1969e-01,  1.2238e+00, -1.1045e+00,  ..., -1.1206e+00,\n",
              "            4.0168e-01,  7.4625e-01],\n",
              "          [-8.4220e-01,  9.5875e-03, -1.1536e+00,  ...,  3.1307e-01,\n",
              "           -9.2076e-01,  1.6886e-02],\n",
              "          [ 6.7363e-01,  2.1620e+00,  7.5189e-01,  ..., -1.5060e-01,\n",
              "           -8.0957e-01,  9.9159e-01],\n",
              "          [ 7.2450e-01,  1.5846e+00, -4.9820e-01,  ..., -6.3952e-01,\n",
              "            4.3777e-01, -6.8856e-01]],\n",
              "\n",
              "         [[ 4.5020e-02,  4.0703e-02, -8.5913e-02,  ..., -2.0362e-03,\n",
              "           -1.8462e-02,  1.6407e-04],\n",
              "          [ 1.1996e+00,  8.5361e-02, -1.1544e-01,  ...,  3.4113e-01,\n",
              "           -5.6559e-01, -9.2612e-02],\n",
              "          [ 2.4485e+00, -8.0361e-01, -7.4377e-03,  ...,  6.6814e-01,\n",
              "           -1.5826e-01,  1.5288e+00],\n",
              "          [ 8.1555e-01, -9.3594e-01,  2.3426e-01,  ...,  1.3530e+00,\n",
              "           -1.1756e-01, -3.6261e-01],\n",
              "          [ 6.5079e-02, -1.0036e+00, -6.7711e-01,  ...,  9.3121e-01,\n",
              "           -1.0274e+00, -8.4414e-01],\n",
              "          [ 9.3166e-01, -9.4614e-01, -4.0232e-01,  ...,  3.1039e-01,\n",
              "            2.3024e-01, -6.9760e-01]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[ 4.0051e-03,  3.0031e-02, -6.7098e-03,  ..., -2.2071e-02,\n",
              "           -2.5694e-02,  3.6145e-02],\n",
              "          [-1.7836e+00,  1.3953e+00,  2.4627e-01,  ..., -2.3286e-02,\n",
              "            5.3286e-01,  1.8789e+00],\n",
              "          [ 9.4935e-03, -2.7717e-01,  2.9411e-02,  ..., -6.0428e-02,\n",
              "           -1.0414e+00,  1.3411e+00],\n",
              "          [-5.2332e-01, -8.1683e-01, -6.8611e-01,  ..., -6.0822e-01,\n",
              "           -2.1057e-01, -1.2381e+00],\n",
              "          [-3.2304e+00, -1.1221e+00, -7.7213e-02,  ..., -6.9244e-01,\n",
              "           -2.2690e-01, -5.4137e-01],\n",
              "          [ 1.1949e-01, -7.3931e-01,  8.8638e-01,  ...,  5.8849e-03,\n",
              "            8.8094e-01,  1.1650e+00]],\n",
              "\n",
              "         [[-6.5377e-02, -4.5243e-02,  2.9972e-02,  ...,  1.7289e-02,\n",
              "           -4.0501e-02, -1.0150e-01],\n",
              "          [ 1.8560e+00, -1.4579e+00,  1.9172e-02,  ...,  8.0446e-01,\n",
              "            1.4534e+00,  3.9945e-02],\n",
              "          [ 1.4365e+00, -1.0610e+00, -2.1904e-01,  ...,  3.7558e-01,\n",
              "            6.0494e-01,  1.1609e+00],\n",
              "          [ 1.7717e-01,  2.3413e-01,  2.7044e-01,  ...,  2.1738e-01,\n",
              "           -6.2691e-01,  8.3158e-01],\n",
              "          [ 5.9438e-01,  3.7680e-02,  3.7383e-01,  ...,  1.4423e-01,\n",
              "            7.2523e-01,  1.0609e+00],\n",
              "          [-5.7178e-01,  7.1508e-01,  2.9552e-01,  ...,  1.6616e-01,\n",
              "            7.6969e-01,  7.2526e-01]],\n",
              "\n",
              "         [[-1.1829e-02,  3.3011e-02, -4.7851e-02,  ..., -7.7909e-03,\n",
              "            1.3503e-02,  1.1945e-02],\n",
              "          [ 8.7942e-01,  1.1863e+00,  1.7838e-01,  ..., -2.3405e-01,\n",
              "            3.6903e-01, -1.2711e+00],\n",
              "          [ 4.8135e-01,  8.3537e-01,  1.0789e-01,  ...,  5.2185e-01,\n",
              "            1.8551e+00, -1.1395e+00],\n",
              "          [ 2.8532e-01,  4.1229e-01, -8.0330e-01,  ...,  9.2710e-01,\n",
              "            6.3253e-01,  1.1979e+00],\n",
              "          [-1.5214e+00, -2.4572e-01, -1.1389e+00,  ...,  1.8103e+00,\n",
              "            6.7831e-01,  2.3402e-01],\n",
              "          [-5.1205e-01,  4.8017e-01,  3.8364e-01,  ..., -1.1088e-01,\n",
              "            6.0421e-01, -5.0688e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-0.5105,  0.4888, -0.8554,  ..., -1.0267, -1.3161,  0.2115],\n",
              "          [ 0.9001, -0.1155,  0.2457,  ...,  2.1422,  0.4717, -1.1785],\n",
              "          [-0.7167, -0.7210, -0.2316,  ...,  2.2173, -0.3908, -0.6265],\n",
              "          [-0.7979,  0.5811, -1.4272,  ...,  0.4868,  0.2466,  0.6593],\n",
              "          [-0.4129,  0.3261, -2.4919,  ...,  0.9294,  0.8224, -0.4097],\n",
              "          [ 0.3331, -0.3971, -0.7753,  ...,  1.9517,  0.7287,  0.0114]],\n",
              "\n",
              "         [[ 0.8641, -2.0632,  0.1507,  ...,  0.2358, -2.4628, -0.4365],\n",
              "          [ 0.4368,  1.4969, -0.7631,  ..., -0.2008,  1.6103,  0.2228],\n",
              "          [ 0.1610,  0.3513, -1.0852,  ...,  0.4297, -1.6737, -0.6511],\n",
              "          [ 0.3653,  1.0067,  0.8664,  ...,  2.0231, -0.0303,  0.1486],\n",
              "          [ 1.5761,  0.7534,  0.1137,  ...,  0.0530,  1.5888, -1.0447],\n",
              "          [ 1.6097,  1.4888, -0.1884,  ...,  0.1873,  1.4904, -0.3658]],\n",
              "\n",
              "         [[ 1.0047,  0.3677, -0.1759,  ..., -0.8085, -1.3894, -0.3705],\n",
              "          [-0.1295, -0.2966, -0.5535,  ...,  0.9405,  0.0970, -0.6187],\n",
              "          [-0.7310,  0.0733, -0.4941,  ...,  1.3184, -0.1770, -0.6431],\n",
              "          [-1.3938, -0.0180,  0.1662,  ...,  2.6852, -0.1904,  0.7468],\n",
              "          [-0.7081, -1.5590,  1.2445,  ...,  2.0137, -0.6200,  0.5064],\n",
              "          [ 0.3280, -0.4720,  0.0911,  ...,  0.6272,  0.4549, -0.6737]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[ 0.2270, -0.5682,  0.4831,  ..., -0.6640,  1.0591,  0.2766],\n",
              "          [ 0.1098,  1.5119, -0.6879,  ..., -2.9334,  0.6231, -0.1252],\n",
              "          [-0.6351,  2.1879, -1.8850,  ..., -2.2055,  0.3981, -0.2443],\n",
              "          [-0.3514,  2.2413, -0.5218,  ..., -1.9378, -1.9885,  1.0978],\n",
              "          [-0.9031,  2.7600, -1.9149,  ..., -1.4593, -2.2993,  1.5111],\n",
              "          [ 0.2469,  0.6472, -1.7598,  ..., -3.9361, -0.8897,  1.6776]],\n",
              "\n",
              "         [[ 0.2557,  0.5486,  0.5427,  ...,  0.7337,  0.0735,  0.8409],\n",
              "          [-0.3881,  3.1865, -0.3928,  ..., -0.0125, -0.5083, -0.0521],\n",
              "          [-0.4876,  3.1043, -0.4298,  ...,  1.2308,  0.3444, -0.2725],\n",
              "          [-0.7570,  2.2686,  0.3038,  ...,  1.1798, -0.7424,  1.7813],\n",
              "          [-0.2227,  0.9588,  0.7954,  ...,  0.2862, -0.9289,  1.5603],\n",
              "          [ 1.1639,  1.7966,  0.6397,  ..., -0.6471, -0.2594,  0.6789]],\n",
              "\n",
              "         [[-0.7236,  0.3119, -1.5837,  ..., -0.3841,  0.2253, -1.3224],\n",
              "          [-0.7607,  0.9895, -0.0810,  ..., -1.6074,  1.7233, -0.6085],\n",
              "          [-0.7018,  1.2038, -1.0300,  ..., -1.3744, -0.1333, -0.2139],\n",
              "          [-1.4948,  0.2443,  0.0477,  ...,  0.3903,  0.1469,  1.6998],\n",
              "          [-0.5284, -0.2279,  0.5391,  ..., -0.7578, -0.4719,  2.6165],\n",
              "          [-0.4818,  0.3410,  1.8578,  ...,  1.4991,  1.0476, -0.8652]]]],\n",
              "       grad_fn=<PermuteBackward0>), tensor([[[[ 1.8845e-02,  4.6500e-02, -6.7226e-02,  ...,  6.0936e-02,\n",
              "           -2.3287e-02, -9.3406e-02],\n",
              "          [-9.8696e-01, -1.5839e-01, -1.2078e+00,  ...,  1.0043e+00,\n",
              "            5.8086e-01, -1.9480e+00],\n",
              "          [-3.4429e-01, -1.5093e-01, -8.7627e-02,  ...,  7.3128e-01,\n",
              "           -1.9684e-01, -2.1145e-01],\n",
              "          [ 1.2915e+00, -3.7071e-01,  1.3614e+00,  ...,  5.9877e-01,\n",
              "            4.3612e-01, -5.9876e-01],\n",
              "          [-9.2125e-01,  1.4785e+00,  1.3037e+00,  ..., -8.5009e-01,\n",
              "            1.7724e-01, -4.5268e-01],\n",
              "          [ 4.6964e-01,  4.9357e-01,  1.0088e+00,  ..., -3.9590e-01,\n",
              "            2.7451e-01, -9.2512e-01]],\n",
              "\n",
              "         [[ 5.9533e-02,  7.3235e-04,  3.6023e-02,  ..., -3.3076e-02,\n",
              "           -2.6139e-02, -2.8997e-03],\n",
              "          [ 5.6298e-02,  8.4551e-01,  1.3106e+00,  ..., -3.8217e-01,\n",
              "           -7.9231e-01,  2.7626e-01],\n",
              "          [ 3.4951e-01,  6.4176e-01,  4.9065e-01,  ..., -1.4230e+00,\n",
              "           -1.6634e+00, -2.4345e-01],\n",
              "          [ 5.9170e-01, -1.4747e+00, -8.4589e-01,  ..., -1.8840e-01,\n",
              "           -9.7905e-01,  6.7782e-01],\n",
              "          [ 1.3133e+00, -6.8098e-01, -1.0696e+00,  ...,  3.5261e-01,\n",
              "           -3.0254e-01,  8.4739e-01],\n",
              "          [-2.3442e-02,  2.1214e-02, -8.6372e-01,  ..., -3.8451e-01,\n",
              "           -5.5677e-01,  1.7497e+00]],\n",
              "\n",
              "         [[-1.0822e-02,  1.1039e-02, -2.9605e-02,  ...,  1.4061e-02,\n",
              "            3.5705e-02,  5.5029e-02],\n",
              "          [ 1.2765e+00,  2.8045e-01,  1.6887e+00,  ...,  3.3468e-01,\n",
              "           -3.9073e-01,  1.1352e+00],\n",
              "          [ 6.8233e-01,  1.3307e+00,  2.3899e+00,  ...,  1.1392e+00,\n",
              "            2.5695e-01, -5.7332e-01],\n",
              "          [-6.0023e-01,  4.1833e-01,  6.4010e-01,  ..., -4.5268e-01,\n",
              "            6.7436e-01, -4.1339e-01],\n",
              "          [ 8.5439e-01,  1.1185e+00, -6.7016e-02,  ...,  6.5744e-01,\n",
              "           -5.1848e-01,  1.9500e-01],\n",
              "          [ 3.8282e-01,  3.3950e-01,  1.0357e+00,  ...,  9.0854e-02,\n",
              "           -2.1069e-01, -2.8403e-01]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[-9.8938e-02,  3.7375e-02, -1.7196e-03,  ..., -4.7671e-02,\n",
              "           -2.1496e-02,  2.7531e-02],\n",
              "          [ 3.8287e-01,  2.7605e-01, -3.8119e-02,  ...,  1.5338e-03,\n",
              "           -1.0232e+00,  2.6402e-01],\n",
              "          [ 1.8463e-01,  6.5831e-01, -3.2374e-01,  ..., -5.2926e-01,\n",
              "           -1.5799e+00, -4.4431e-01],\n",
              "          [ 7.5410e-01,  7.9720e-01, -3.4485e-01,  ...,  1.3299e+00,\n",
              "            8.2549e-01,  2.8877e-01],\n",
              "          [-6.4810e-01, -8.4429e-02, -8.2208e-02,  ...,  2.4342e-01,\n",
              "           -8.0643e-01,  3.5728e-01],\n",
              "          [-6.0976e-01, -1.1543e+00,  8.9953e-01,  ...,  4.4889e-01,\n",
              "           -1.2343e+00, -1.3497e-01]],\n",
              "\n",
              "         [[ 8.4550e-02,  2.7683e-02,  6.4263e-02,  ...,  2.7156e-02,\n",
              "            4.6119e-02,  9.7695e-03],\n",
              "          [ 4.0524e-01,  3.2760e-01, -8.2816e-02,  ...,  2.6532e-02,\n",
              "            2.6010e+00, -1.3951e+00],\n",
              "          [-2.1445e-01,  7.0221e-02,  1.9204e-01,  ..., -3.7881e-01,\n",
              "            1.5635e+00, -8.6605e-01],\n",
              "          [ 2.1300e+00, -1.2072e+00,  8.9416e-01,  ...,  9.5254e-01,\n",
              "            1.1030e+00, -2.6183e-01],\n",
              "          [ 9.5106e-01, -4.5712e-01, -9.6595e-02,  ...,  9.7159e-01,\n",
              "            8.6205e-01,  1.1261e+00],\n",
              "          [-3.2111e-01,  7.1420e-01,  9.9663e-01,  ...,  1.0814e+00,\n",
              "            8.1234e-01, -5.8011e-01]],\n",
              "\n",
              "         [[-1.1082e-01,  2.3628e-02, -5.7385e-02,  ..., -9.5811e-02,\n",
              "            5.7013e-02, -1.9716e-02],\n",
              "          [ 5.9451e-02,  6.3312e-01, -9.8548e-02,  ..., -8.8554e-01,\n",
              "            8.5378e-01, -2.0224e-01],\n",
              "          [ 2.1196e-01,  4.2622e-01, -5.9598e-01,  ..., -7.7586e-01,\n",
              "            8.2437e-01,  8.7509e-02],\n",
              "          [-6.9414e-01, -1.1000e+00, -9.6296e-01,  ..., -1.6386e-01,\n",
              "            1.7602e+00,  5.7648e-01],\n",
              "          [ 6.1057e-02,  2.1197e-01,  7.2472e-01,  ...,  2.8164e-01,\n",
              "           -3.5391e-01,  9.6176e-01],\n",
              "          [ 1.0886e-01,  1.5887e-01, -1.5536e-01,  ..., -5.1391e-01,\n",
              "           -1.3542e+00, -1.9414e-02]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-1.7216, -0.3013, -0.2952,  ...,  0.1865,  0.3256, -0.5023],\n",
              "          [ 0.7548,  0.4109, -1.1135,  ...,  1.5156, -1.0666, -0.5053],\n",
              "          [ 1.3919, -0.0311, -0.8856,  ...,  2.0359, -0.4824, -0.2545],\n",
              "          [ 1.6270,  0.7024, -1.0340,  ...,  1.4227, -1.3207, -0.9279],\n",
              "          [ 0.1205,  0.0545, -0.7552,  ...,  0.8397, -1.0664, -0.1807],\n",
              "          [ 1.3693,  0.4868, -0.9318,  ...,  0.1488, -2.3419, -1.2043]],\n",
              "\n",
              "         [[ 0.1073, -0.0800,  2.2955,  ...,  0.2432,  0.0857, -0.1948],\n",
              "          [-0.1800, -1.4840, -0.8470,  ...,  1.4156, -0.5391, -0.7050],\n",
              "          [-0.0197, -1.5517,  0.3624,  ...,  1.2287, -0.4692, -1.6074],\n",
              "          [-0.3719, -1.2967, -0.5332,  ...,  1.6984, -0.0933, -0.4360],\n",
              "          [ 1.1688, -2.1240, -0.3268,  ...,  0.1326,  0.2101, -0.4148],\n",
              "          [ 0.5571, -0.8531, -1.0429,  ...,  0.0049,  0.3662,  0.0386]],\n",
              "\n",
              "         [[-0.1928,  1.0482,  0.4753,  ..., -0.5411,  0.3269, -0.1044],\n",
              "          [-1.0361,  0.0525, -0.0361,  ...,  1.8758,  0.9501, -0.5156],\n",
              "          [-1.5726,  0.7601,  0.6073,  ...,  1.9537,  1.3995, -0.4290],\n",
              "          [-0.9743,  0.3406, -0.1039,  ...,  0.9778, -0.1269, -0.2593],\n",
              "          [-0.5333,  1.0781,  1.3565,  ...,  0.0135,  1.8807,  0.4802],\n",
              "          [-0.6476,  0.5791, -0.2095,  ...,  1.0178,  1.3441, -0.1741]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[ 0.5510,  0.9702, -0.8676,  ..., -0.7088,  0.6934,  0.8508],\n",
              "          [-0.2662,  0.6483, -0.3250,  ..., -1.2908, -0.1877,  0.3347],\n",
              "          [ 0.5341,  0.8408,  0.0937,  ..., -1.0490,  0.3651,  1.6393],\n",
              "          [ 1.0939,  0.6440, -1.3783,  ..., -0.7654,  1.7352, -1.1333],\n",
              "          [ 0.1749,  1.7503, -1.2935,  ..., -0.0781,  1.9146, -0.0980],\n",
              "          [-0.5600, -0.2878, -0.4721,  ..., -0.3038, -0.6933,  0.0401]],\n",
              "\n",
              "         [[-0.4068,  0.3957,  0.3603,  ...,  0.6988,  0.0296, -0.0826],\n",
              "          [ 0.1856,  0.3884,  0.5564,  ...,  1.8274, -0.9805, -1.1542],\n",
              "          [-0.1918,  0.3419,  0.4547,  ...,  1.3674, -1.1578, -0.5406],\n",
              "          [-0.2366, -0.4058,  0.0320,  ...,  0.3145, -0.7438, -1.0064],\n",
              "          [-0.0128, -0.1874,  0.3651,  ...,  0.3483, -1.5189, -1.2959],\n",
              "          [-0.2434,  0.8263, -0.9643,  ..., -0.6078, -0.4157, -1.0236]],\n",
              "\n",
              "         [[-0.7434, -0.0046,  0.4467,  ..., -0.1008,  0.0175, -0.0680],\n",
              "          [ 0.1292, -0.4121,  0.5793,  ..., -0.3415, -0.8189,  0.5697],\n",
              "          [-0.3494, -1.6170,  0.9120,  ..., -1.0219, -1.0342, -0.1283],\n",
              "          [ 0.0764, -0.0059,  1.6281,  ..., -0.3185, -0.5387, -0.2978],\n",
              "          [ 0.3205,  0.4646,  0.9514,  ...,  0.2077, -1.5930, -0.5422],\n",
              "          [ 0.3030, -0.8921,  3.2489,  ...,  0.4727,  0.8145,  1.0939]]]],\n",
              "       grad_fn=<PermuteBackward0>), tensor([[[[ 0.1065, -0.1222, -0.1562,  ..., -0.2670,  0.2449, -0.1406],\n",
              "          [-0.1044,  0.2926, -0.7248,  ...,  3.2864, -1.3827,  0.7435],\n",
              "          [-1.8324, -0.7664, -0.3120,  ...,  1.7364, -0.8104,  1.0128],\n",
              "          [-0.2020, -0.2667,  0.4859,  ...,  0.7065, -0.3446, -0.3123],\n",
              "          [-1.8511,  0.2659,  0.9485,  ...,  1.3266, -0.3681,  0.3995],\n",
              "          [-2.3208,  0.6867, -0.1558,  ...,  1.0833, -2.7216,  1.5062]],\n",
              "\n",
              "         [[ 0.1237, -0.0431,  0.0194,  ..., -0.0261, -0.0917,  0.1659],\n",
              "          [ 0.0542, -0.3335, -0.0312,  ...,  0.0463,  0.5797, -0.2021],\n",
              "          [ 0.4608, -0.1426,  0.0870,  ...,  1.0891, -0.2560, -1.3860],\n",
              "          [-0.6706,  0.1789, -0.5151,  ..., -0.4707,  0.3074, -0.1003],\n",
              "          [-1.3197,  0.4932, -1.6485,  ..., -0.1951, -0.3593, -1.7597],\n",
              "          [ 0.1474,  0.8678,  0.3872,  ...,  0.4045,  0.3797,  0.6732]],\n",
              "\n",
              "         [[-0.0164,  0.0347, -0.0670,  ...,  0.0290,  0.0150,  0.0685],\n",
              "          [ 0.0639, -0.4717, -0.4944,  ..., -0.6009, -1.0867,  0.2280],\n",
              "          [ 0.1544, -0.3571,  0.4090,  ..., -0.7178, -0.6437,  0.5328],\n",
              "          [ 0.8741,  0.6525, -0.0964,  ..., -0.6263, -1.4515,  0.2534],\n",
              "          [ 0.7006, -1.1043,  0.2879,  ...,  0.5178, -1.0774,  0.1149],\n",
              "          [ 0.5804, -0.3549, -1.4500,  ..., -0.2374,  2.1193,  0.0549]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[-0.0089, -0.0111,  0.0988,  ...,  0.0838, -0.0212,  0.0478],\n",
              "          [ 0.0411,  0.5576, -0.1551,  ...,  0.2639,  1.2018, -0.3404],\n",
              "          [ 0.7508, -0.1606,  0.2762,  ..., -0.5388,  1.3543, -0.2186],\n",
              "          [-0.7855, -0.5104, -0.6055,  ...,  1.6914,  0.5878,  0.7912],\n",
              "          [-1.6008,  1.0621,  0.7534,  ...,  0.5354,  0.7387, -0.4241],\n",
              "          [-0.3044, -0.9291, -0.4625,  ..., -0.1507,  0.8173,  1.1524]],\n",
              "\n",
              "         [[-0.1965, -0.0647,  0.0663,  ..., -0.0456,  0.0493, -0.0815],\n",
              "          [ 0.4527,  0.3189, -0.4242,  ..., -0.9748,  1.0891, -0.4487],\n",
              "          [-0.1850,  1.5773, -0.3945,  ..., -0.6107,  0.8598, -1.3152],\n",
              "          [-1.6246, -0.7877,  0.1174,  ...,  0.5142,  0.2820,  0.4650],\n",
              "          [ 0.8826, -0.3152, -0.2988,  ..., -0.7754, -0.9379,  0.0393],\n",
              "          [-1.0028, -0.0130,  0.6231,  ..., -0.0344, -0.0956, -0.3314]],\n",
              "\n",
              "         [[ 0.1040, -0.1469,  0.1752,  ..., -0.1528,  0.0050, -0.1816],\n",
              "          [ 0.0442, -0.0878,  0.4190,  ..., -0.8916,  0.2352, -0.9527],\n",
              "          [ 0.4844, -0.3733,  0.8768,  ..., -1.2311,  0.5642, -0.7171],\n",
              "          [-0.7574, -0.9083,  1.0785,  ..., -0.7477,  1.6442,  1.1238],\n",
              "          [-0.4127, -2.1126, -1.0792,  ..., -0.7476,  0.3479,  0.5049],\n",
              "          [-0.7073,  1.3176, -0.4645,  ...,  1.1788,  0.4413,  0.8756]]]],\n",
              "       grad_fn=<PermuteBackward0>))), hidden_states=None, attentions=None, cross_attentions=None)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load GPT-2 model\n",
        "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
        "\n",
        "# tokenize text\n",
        "x = tokenizer('What a great input string!', return_tensors='pt')\n",
        "\n",
        "# forward pass\n",
        "out = model(input_ids=x['input_ids'], attention_mask=x['attention_mask'])\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tH6QGovCZ5I5"
      },
      "source": [
        "Try to ignore the wall of text that just appeared and just focus on the first line. For me it is:\n",
        "\n",
        "`CausalLMOutputWithCrossAttentions(loss=None, logits=tensor([[[ -37.2172,  -36.8864,  -40.3563,  ...`\n",
        "\n",
        "The model outputs some sort of weird `CausalLMOutputWithCrossAttentions`. IMO this is a bit confusing, but we roll with it. Let's look inside this data structure.\n",
        "\n",
        "First, we have `loss=None` No loss was calculated because we didn't pass it `labels` when it was called. We'll see more about that in a moment.\n",
        "\n",
        "Second, we have the raw `logits`, the next token probabilities for each token in `input_ids`. This is huge because for each input token in `input_ids`, each of the 50,000+ output tokens was given a score.\n",
        "\n",
        "Let's take a look at how we can get loss in out output. To do so, we need to pass `labels` in as well. As mentioned before, labels will be the same as `input_ids`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "EPP20bvUbQnF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(5.8369, grad_fn=<NllLossBackward0>)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tokenize text\n",
        "x = tokenizer('What a great input string!', return_tensors='pt')\n",
        "\n",
        "# forward pass\n",
        "out = model(\n",
        "    input_ids=x['input_ids'],\n",
        "    attention_mask=x['attention_mask'],\n",
        "    labels=x['input_ids']\n",
        ")\n",
        "out.loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3hWSkCnbgGu"
      },
      "source": [
        "Sweet! If we were training, we could now call `out.loss.backward()` and run backpropagation.\n",
        "\n",
        "The second method that is important is `model.generate()`. This allows us to generate text using our model. We can call it without any input, allowing it to ramble on its own!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "qfV8DWpRb3ii"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "c:\\Users\\Frank\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[50256,   198,   464,   717,   640,   314,  2497,   262,   649,  2196,\n",
              "           286,   262,   983,    11,   314,   373,   523,  6568,    13,   314]])"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# generate text\n",
        "model.generate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyLnJEaecBhJ"
      },
      "source": [
        "Right, it outputs a tensor with so we need to decode it using the tokenizer. There's a batch dim, so we should index in!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "pKwg1810cA-i"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'<|endoftext|>\\nThe first time I saw the new version of the game, I was so excited. I'"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# generate text\n",
        "out = model.generate()\n",
        "\n",
        "# decode output\n",
        "tokenizer.decode(out[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS7_A1CLcfMI"
      },
      "source": [
        "Nice! If we want to prompt it, we can encode text then pass the tensor into the model when generating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "FM0TFO2-cm3B"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'The UW is the best in the country in terms of its academic success. The UW is also the'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# encode prompt\n",
        "prompt = tokenizer('The UW is the best', return_tensors='pt')\n",
        "\n",
        "# generate text\n",
        "out = model.generate(**prompt)\n",
        "\n",
        "# decode output\n",
        "tokenizer.decode(out[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOKhpbbKc4hM"
      },
      "source": [
        "Perfect. As we can see, we've been getting warnings about not setting a `max_length` or `max_new_tokens`. We can control text generation via a variety of flags ([docs](https://huggingface.co/docs/transformers/main_classes/text_generation))! For this example, we can focus on how many new tokens to generate.\n",
        "\n",
        "To do so, we can use the `max_new_tokens` and `min_new_tokens` flags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pRU5hMAVdW5g"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'The UW is the best in the country in terms of its academic success. The UW is also the best in the country in terms of its academic success.\\n\\nThe UW is the best in the country in terms of its academic success. The UW is also the best in'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# encode prompt\n",
        "prompt = tokenizer('The UW is the best', return_tensors='pt')\n",
        "\n",
        "# generate text with 50 new tokens\n",
        "out = model.generate(**prompt, min_new_tokens=50, max_new_tokens=50)\n",
        "\n",
        "# decode output\n",
        "tokenizer.decode(out[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5wLqohsdptK"
      },
      "source": [
        "Now that we have a grasp on this, let's take a look at what it would take to fine-tune our own models!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aln1PWNOFaQ"
      },
      "source": [
        "# Training a model - loading a dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzZ7jALVeqPB"
      },
      "source": [
        "To train a model, we need data to train on. Fortunately HF has a bunch of datasets on their Hub. (I reread this and it sounded like an ad read. Sorry.) To download a dataset, we can use the `load_dataset` function from the `datasets` library. Lets do so for a dataset on [financial news](https://huggingface.co/datasets/zeroshot/twitter-financial-news-topic)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ccTwpg2Qfbxz"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset csv (C:/Users/Frank/.cache/huggingface/datasets/zeroshot___csv/zeroshot--twitter-financial-news-topic-7546d9df5195ffd4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "609f695f5c354a7fa3a69497fdf43416",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 16990\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 4117\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset('zeroshot/twitter-financial-news-topic')\n",
        "ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XywxzbDfveP"
      },
      "source": [
        "Let's train our model to generate tweets that are similar to the dataset. We won't need any of the `label`'s, so we can remove them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "HVKfyF6zgAfv"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 16990\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 4117\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rem_ds = ds.remove_columns('label')\n",
        "rem_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUFCg-rNgMVt"
      },
      "source": [
        "Now we need to tokenize the dataset. To do so, we can use `ds.map` to run a function over each example in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "dLZHtW3qgXwz"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at C:\\Users\\Frank\\.cache\\huggingface\\datasets\\zeroshot___csv\\zeroshot--twitter-financial-news-topic-7546d9df5195ffd4\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-bbdb88ea14f65658.arrow\n",
            "Loading cached processed dataset at C:\\Users\\Frank\\.cache\\huggingface\\datasets\\zeroshot___csv\\zeroshot--twitter-financial-news-topic-7546d9df5195ffd4\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-3f0df40a3e69cb6c.arrow\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 16990\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 4117\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tokenization function\n",
        "def token_func(example):\n",
        "    return tokenizer(example['text'])\n",
        "\n",
        "# run over entire dataset\n",
        "tokenized_ds = rem_ds.map(token_func)\n",
        "tokenized_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEci4frJgz_U"
      },
      "source": [
        "We no longer need the `text` column, so we can remove it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "4pfM_Bkng9Og"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask'],\n",
              "        num_rows: 16990\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['input_ids', 'attention_mask'],\n",
              "        num_rows: 4117\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rem_tokenized_ds = tokenized_ds.remove_columns('text')\n",
        "rem_tokenized_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7eU2PEGhUoM"
      },
      "source": [
        "Now we batch texts to be a consistant size (don't worry much about this part). This will reduce our texts down to a small amount of examples because each one was quite short"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "QZ2GMEZyhePW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at C:\\Users\\Frank\\.cache\\huggingface\\datasets\\zeroshot___csv\\zeroshot--twitter-financial-news-topic-7546d9df5195ffd4\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-6bcf639a17483fc7.arrow\n",
            "Loading cached processed dataset at C:\\Users\\Frank\\.cache\\huggingface\\datasets\\zeroshot___csv\\zeroshot--twitter-financial-news-topic-7546d9df5195ffd4\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-314c5a5fa24492b8.arrow\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask'],\n",
              "        num_rows: 5643\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['input_ids', 'attention_mask'],\n",
              "        num_rows: 1370\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from itertools import chain\n",
        "\n",
        "# group texts into blocks of block_size\n",
        "block_size = 128\n",
        "\n",
        "def group_texts(examples):\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the small remainder\n",
        "    if total_length >= block_size:\n",
        "        total_length = (total_length // block_size) * block_size\n",
        "    # Split by chunks of max_len.\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    return result\n",
        "\n",
        "batched_ds = rem_tokenized_ds.map(group_texts, batched=True)\n",
        "batched_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOVEK-RmjHdZ"
      },
      "source": [
        "We're going to want a loss, so we will copy the `input_ids` column to the `labels` column as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "V2olthqvjG56"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 5643\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['input_ids', 'attention_mask'],\n",
              "        num_rows: 1370\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batched_ds['train'] = batched_ds['train'].add_column('labels', batched_ds['train']['input_ids'])\n",
        "batched_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfMC1KFciZPk"
      },
      "source": [
        "Next, we can create a standard PyTorch dataloader from these datasets. I'll use HF's `default_data_collator`. We won't do a validation run so we only create `train_dl`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "4keDqpxliYru"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import default_data_collator\n",
        "\n",
        "train_dl = DataLoader(\n",
        "    batched_ds['train'],\n",
        "    shuffle=True,\n",
        "    batch_size=2, # small batch size bc i want to ensure it runs\n",
        "    collate_fn=default_data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSwRdWGgi-eV"
      },
      "source": [
        "Finally, we can create a standard training loop and train the model for 100 batches! Try making your own edits to this!\n",
        "\n",
        "Note: this will run much faster on a GPU. If you aren't using one, you can switch by pressing \"runtime\", then \"change runtime type\", then specify \"gpu\". Note that this will restart your current Colab session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "TrAdHBpFkBM5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "model.train()\n",
        "dl_iter = iter(train_dl)\n",
        "\n",
        "#for batch in train_dl: # uncomment to run full epoch\n",
        "for i in range(100):\n",
        "    batch = next(dl_iter)\n",
        "    # push all to device\n",
        "    batch = {k: batch[k].to(device) for k in batch.keys()}\n",
        "    # forward pass\n",
        "    out = model(**batch)\n",
        "    optimizer.zero_grad()\n",
        "    out.loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUmPAjtnoalt"
      },
      "source": [
        "Let's see what our model now produces!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "puwzYWXVoc_k"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"<|endoftext|>The world's largest oil company is now making a big move to the US shale gas, according to a report by the International Energy Agency. The company\""
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# generate text\n",
        "out = model.generate(min_new_tokens=30, max_new_tokens=30)\n",
        "tokenizer.decode(out[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITLtPS2lp7KQ"
      },
      "source": [
        "It needs more training, but you can see that it is starting to learn!\n",
        "\n",
        "To upload the model, we can use `model.push_to_hub()`. We first need to login to HF using the cli command `huggingface-cli login`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "JSFxCsPrtaPs"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79339db4595e44b78997700d4fd0620f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/510M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f72c8fc164494083b8ee601d74ad33f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/frangkli/hf-tutorial/commit/1ce158f6414eaee9c5b7cb41cb8f9449f77a523f', commit_message='Upload model', commit_description='', oid='1ce158f6414eaee9c5b7cb41cb8f9449f77a523f', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.push_to_hub('frangkli/hf-tutorial')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMnbP7dqttOL"
      },
      "source": [
        "Thank you for your time! Let me know if you have any feedback!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
