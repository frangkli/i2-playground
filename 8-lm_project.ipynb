{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XztXEJOzfjC"
      },
      "source": [
        "In this exercise, you'll train GPT-2 on a dataset of Shakespeare! You'll be using Hugging Face to download the dataset and pre-trained model - there's a tutorial located in the same folder as this in the GitHub repository. That tutorial will explain how to load models and datasets, pre-process them, and fine-tune the model!\n",
        "\n",
        "The starter code here is quite lightweight - you should refer back to the tutorial often. You will likely want to copy-paste some code from it, just make sure you know what it is doing!\n",
        "\n",
        "Once the model is trained, try generating some text! Hopefully it looks (somewhat) like Shakespeare!\n",
        "\n",
        "If you finish this and want to explore further, try training a different model or use a different dataset. This exercise is meant for you to be able to explore, so do what you find interesting!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftOXtNRCxVCg"
      },
      "source": [
        "Download required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3H25Eo5Xm-a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\frank\\anaconda3\\lib\\site-packages (4.26.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\frank\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in c:\\users\\frank\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: colorama in c:\\users\\frank\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: datasets in c:\\users\\frank\\anaconda3\\lib\\site-packages (2.10.1)\n",
            "Requirement already satisfied: responses<0.19 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (0.12.1)\n",
            "Requirement already satisfied: xxhash in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: pandas in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (1.4.4)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (2022.7.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (2.28.1)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (1.21.5)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (11.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.3.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\frank\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
            "Requirement already satisfied: colorama in c:\\users\\frank\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.5)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skalDwAkxd0P"
      },
      "source": [
        "Load the dataset you'll be using:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KfqFILiFtZx7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset tiny_shakespeare (C:/Users/Frank/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1962a0cd93194a2697bf629cc2b6c4cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 1\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 1\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 1\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset('tiny_shakespeare')\n",
        "ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDDu2xhUxiDX"
      },
      "source": [
        "Let's take a quick look at what's in the dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2sqvyaT6tw-J"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us\n"
          ]
        }
      ],
      "source": [
        "print(ds['train']['text'][0][:300])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TeSW4qnxNQx"
      },
      "source": [
        "\n",
        "And from here it's up to you! Make sure to look at the tutorial, it will hopefully be a good guide. If you have any questions be sure to ping someone in the Discord!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 1\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 1\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 1\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at C:\\Users\\Frank\\.cache\\huggingface\\datasets\\tiny_shakespeare\\default\\1.0.0\\b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e\\cache-babfbdcf4b3186c8.arrow\n",
            "Loading cached processed dataset at C:\\Users\\Frank\\.cache\\huggingface\\datasets\\tiny_shakespeare\\default\\1.0.0\\b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e\\cache-d9a53b4a56b68156.arrow\n",
            "Loading cached processed dataset at C:\\Users\\Frank\\.cache\\huggingface\\datasets\\tiny_shakespeare\\default\\1.0.0\\b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e\\cache-05b60ed0fdec609c.arrow\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 1\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 1\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 1\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def token_func(example):\n",
        "  return tokenizer(example['text'])\n",
        "\n",
        "tokenized_ds = ds.map(token_func, batched=True)\n",
        "tokenized_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask'],\n",
              "        num_rows: 1\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['input_ids', 'attention_mask'],\n",
              "        num_rows: 1\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask'],\n",
              "        num_rows: 1\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rem_tokenized_ds = tokenized_ds.remove_columns(['text'])\n",
        "rem_tokenized_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at C:\\Users\\Frank\\.cache\\huggingface\\datasets\\tiny_shakespeare\\default\\1.0.0\\b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e\\cache-6f89cb6b5a97da3a.arrow\n",
            "Loading cached processed dataset at C:\\Users\\Frank\\.cache\\huggingface\\datasets\\tiny_shakespeare\\default\\1.0.0\\b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e\\cache-0de79604e8c4b309.arrow\n",
            "Loading cached processed dataset at C:\\Users\\Frank\\.cache\\huggingface\\datasets\\tiny_shakespeare\\default\\1.0.0\\b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e\\cache-305a09f8c6a665ec.arrow\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 2359\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['input_ids', 'attention_mask'],\n",
              "        num_rows: 141\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask'],\n",
              "        num_rows: 140\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from itertools import chain\n",
        "\n",
        "# group texts into blocks of block_size\n",
        "block_size = 128\n",
        "\n",
        "def group_texts(examples):\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the small remainder\n",
        "    if total_length >= block_size:\n",
        "        total_length = (total_length // block_size) * block_size\n",
        "    # Split by chunks of max_len.\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    return result\n",
        "\n",
        "batched_ds = rem_tokenized_ds.map(group_texts, batched=True)\n",
        "\n",
        "batched_ds['train'] = batched_ds['train'].add_column('labels', batched_ds['train']['input_ids'])\n",
        "batched_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import default_data_collator\n",
        "\n",
        "train_dl = DataLoader(\n",
        "  batched_ds['train'],\n",
        "  shuffle=True,\n",
        "  batch_size=16,\n",
        "  collate_fn=default_data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 3.3795\n",
            "Loss: 3.6152\n",
            "Loss: 3.4189\n",
            "Loss: 3.6499\n",
            "Loss: 3.3276\n",
            "Loss: 3.6957\n",
            "Loss: 3.3236\n",
            "Loss: 3.4749\n",
            "Loss: 3.5228\n",
            "Loss: 3.3541\n",
            "Loss: 3.2301\n",
            "Loss: 3.4461\n",
            "Loss: 3.0970\n",
            "Loss: 3.2144\n",
            "Loss: 3.3786\n",
            "Loss: 3.4664\n",
            "Loss: 3.5698\n",
            "Loss: 3.4169\n",
            "Loss: 3.4750\n",
            "Loss: 3.3923\n",
            "Loss: 3.4117\n",
            "Loss: 3.6177\n",
            "Loss: 3.3912\n",
            "Loss: 3.4630\n",
            "Loss: 3.3955\n",
            "Loss: 3.5567\n",
            "Loss: 3.4228\n",
            "Loss: 3.6344\n",
            "Loss: 3.3220\n",
            "Loss: 3.4118\n",
            "Loss: 3.5458\n",
            "Loss: 3.5157\n",
            "Loss: 3.4853\n",
            "Loss: 3.3305\n",
            "Loss: 3.5574\n",
            "Loss: 3.4092\n",
            "Loss: 3.4761\n",
            "Loss: 3.4871\n",
            "Loss: 3.5109\n",
            "Loss: 3.2790\n",
            "Loss: 3.3625\n",
            "Loss: 2.9946\n",
            "Loss: 3.5450\n",
            "Loss: 3.5391\n",
            "Loss: 3.1538\n",
            "Loss: 3.2277\n",
            "Loss: 3.5986\n",
            "Loss: 3.4985\n",
            "Loss: 3.5277\n",
            "Loss: 3.6161\n",
            "Loss: 3.6001\n",
            "Loss: 3.2655\n",
            "Loss: 3.5254\n",
            "Loss: 3.4515\n",
            "Loss: 3.2421\n",
            "Loss: 3.5881\n",
            "Loss: 3.2949\n",
            "Loss: 3.4625\n",
            "Loss: 3.4916\n",
            "Loss: 3.5714\n",
            "Loss: 3.6535\n",
            "Loss: 3.3874\n",
            "Loss: 3.4360\n",
            "Loss: 3.6170\n",
            "Loss: 3.3610\n",
            "Loss: 3.4255\n",
            "Loss: 3.4381\n",
            "Loss: 3.5244\n",
            "Loss: 3.1968\n",
            "Loss: 3.4660\n",
            "Loss: 3.4384\n",
            "Loss: 3.5274\n",
            "Loss: 3.5693\n",
            "Loss: 3.4242\n",
            "Loss: 3.2391\n",
            "Loss: 3.2034\n",
            "Loss: 3.3489\n",
            "Loss: 3.3000\n",
            "Loss: 3.3342\n",
            "Loss: 3.5563\n",
            "Loss: 3.5245\n",
            "Loss: 3.5578\n",
            "Loss: 3.4849\n",
            "Loss: 3.4512\n",
            "Loss: 3.5820\n",
            "Loss: 3.4998\n",
            "Loss: 3.4369\n",
            "Loss: 3.3349\n",
            "Loss: 3.2489\n",
            "Loss: 3.5601\n",
            "Loss: 3.5768\n",
            "Loss: 3.2698\n",
            "Loss: 3.5704\n",
            "Loss: 3.4268\n",
            "Loss: 3.5283\n",
            "Loss: 3.3775\n",
            "Loss: 3.2936\n",
            "Loss: 3.3047\n",
            "Loss: 3.3307\n",
            "Loss: 3.3321\n",
            "Loss: 3.3256\n",
            "Loss: 3.5524\n",
            "Loss: 3.4019\n",
            "Loss: 3.4246\n",
            "Loss: 3.2729\n",
            "Loss: 3.4148\n",
            "Loss: 3.5860\n",
            "Loss: 3.4162\n",
            "Loss: 3.4474\n",
            "Loss: 3.4283\n",
            "Loss: 3.4339\n",
            "Loss: 3.4363\n",
            "Loss: 3.4755\n",
            "Loss: 3.2770\n",
            "Loss: 3.4074\n",
            "Loss: 3.3739\n",
            "Loss: 3.3292\n",
            "Loss: 3.4171\n",
            "Loss: 3.5132\n",
            "Loss: 3.3857\n",
            "Loss: 3.4484\n",
            "Loss: 3.5314\n",
            "Loss: 3.7128\n",
            "Loss: 3.4450\n",
            "Loss: 3.1338\n",
            "Loss: 3.3581\n",
            "Loss: 3.5979\n",
            "Loss: 3.4311\n",
            "Loss: 3.3897\n",
            "Loss: 3.5042\n",
            "Loss: 3.1918\n",
            "Loss: 3.4324\n",
            "Loss: 3.4462\n",
            "Loss: 3.3568\n",
            "Loss: 3.4130\n",
            "Loss: 3.1983\n",
            "Loss: 3.5905\n",
            "Loss: 3.4033\n",
            "Loss: 3.4054\n",
            "Loss: 3.5315\n",
            "Loss: 3.3818\n",
            "Loss: 3.3755\n",
            "Loss: 3.3641\n",
            "Loss: 3.3721\n",
            "Loss: 3.5450\n",
            "Loss: 3.6416\n",
            "Loss: 3.5009\n",
            "Loss: 3.3753\n",
            "Loss: 3.2845\n",
            "Loss: 3.2287\n",
            "Loss: 3.1962\n",
            "Loss: 3.0338\n",
            "Loss: 3.3344\n",
            "Loss: 3.0283\n",
            "Loss: 3.3429\n",
            "Loss: 3.0771\n",
            "Loss: 3.2848\n",
            "Loss: 3.2273\n",
            "Loss: 3.2649\n",
            "Loss: 3.0184\n",
            "Loss: 3.0918\n",
            "Loss: 3.0965\n",
            "Loss: 3.2655\n",
            "Loss: 3.1801\n",
            "Loss: 3.0093\n",
            "Loss: 2.9544\n",
            "Loss: 3.2358\n",
            "Loss: 3.3470\n",
            "Loss: 3.3235\n",
            "Loss: 3.4305\n",
            "Loss: 3.1484\n",
            "Loss: 3.2198\n",
            "Loss: 3.3415\n",
            "Loss: 3.3683\n",
            "Loss: 3.3043\n",
            "Loss: 3.0740\n",
            "Loss: 3.0891\n",
            "Loss: 3.2492\n",
            "Loss: 3.4202\n",
            "Loss: 3.0836\n",
            "Loss: 3.2176\n",
            "Loss: 3.1121\n",
            "Loss: 3.3072\n",
            "Loss: 3.1401\n",
            "Loss: 3.2456\n",
            "Loss: 3.1028\n",
            "Loss: 3.4277\n",
            "Loss: 3.1440\n",
            "Loss: 3.3131\n",
            "Loss: 3.2568\n",
            "Loss: 3.3689\n",
            "Loss: 3.0152\n",
            "Loss: 3.2539\n",
            "Loss: 3.1927\n",
            "Loss: 3.1300\n",
            "Loss: 3.3540\n",
            "Loss: 3.1262\n",
            "Loss: 3.1976\n",
            "Loss: 3.2627\n",
            "Loss: 3.1881\n",
            "Loss: 3.1979\n",
            "Loss: 3.3196\n",
            "Loss: 3.3727\n",
            "Loss: 3.1924\n",
            "Loss: 3.2568\n",
            "Loss: 3.3329\n",
            "Loss: 3.1259\n",
            "Loss: 3.3622\n",
            "Loss: 3.3087\n",
            "Loss: 3.3775\n",
            "Loss: 3.3783\n",
            "Loss: 3.2760\n",
            "Loss: 3.1515\n",
            "Loss: 3.1408\n",
            "Loss: 3.3210\n",
            "Loss: 3.3006\n",
            "Loss: 3.2192\n",
            "Loss: 2.9922\n",
            "Loss: 3.1604\n",
            "Loss: 3.2770\n",
            "Loss: 3.2867\n",
            "Loss: 3.0906\n",
            "Loss: 3.1279\n",
            "Loss: 2.9497\n",
            "Loss: 2.8779\n",
            "Loss: 3.2931\n",
            "Loss: 3.1508\n",
            "Loss: 3.3427\n",
            "Loss: 3.1986\n",
            "Loss: 3.1294\n",
            "Loss: 3.3230\n",
            "Loss: 3.4926\n",
            "Loss: 3.1787\n",
            "Loss: 3.2200\n",
            "Loss: 3.1774\n",
            "Loss: 3.0942\n",
            "Loss: 3.4424\n",
            "Loss: 3.3333\n",
            "Loss: 3.2399\n",
            "Loss: 3.2733\n",
            "Loss: 3.1302\n",
            "Loss: 3.5692\n",
            "Loss: 3.3164\n",
            "Loss: 3.2948\n",
            "Loss: 3.2633\n",
            "Loss: 2.9962\n",
            "Loss: 3.3306\n",
            "Loss: 3.2692\n",
            "Loss: 3.0913\n",
            "Loss: 3.0331\n",
            "Loss: 3.2116\n",
            "Loss: 3.2817\n",
            "Loss: 3.3800\n",
            "Loss: 3.2931\n",
            "Loss: 3.2732\n",
            "Loss: 3.2296\n",
            "Loss: 3.0525\n",
            "Loss: 3.2637\n",
            "Loss: 3.3403\n",
            "Loss: 3.4434\n",
            "Loss: 3.4434\n",
            "Loss: 3.0915\n",
            "Loss: 2.8905\n",
            "Loss: 3.2509\n",
            "Loss: 3.2300\n",
            "Loss: 3.0267\n",
            "Loss: 3.3634\n",
            "Loss: 3.3301\n",
            "Loss: 3.1150\n",
            "Loss: 3.3447\n",
            "Loss: 3.3724\n",
            "Loss: 3.3283\n",
            "Loss: 3.3290\n",
            "Loss: 3.2695\n",
            "Loss: 3.0051\n",
            "Loss: 3.2532\n",
            "Loss: 3.2859\n",
            "Loss: 3.3140\n",
            "Loss: 3.2347\n",
            "Loss: 3.3132\n",
            "Loss: 3.0569\n",
            "Loss: 3.2707\n",
            "Loss: 3.3158\n",
            "Loss: 3.5569\n",
            "Loss: 3.2438\n",
            "Loss: 3.3110\n",
            "Loss: 3.2901\n",
            "Loss: 3.2471\n",
            "Loss: 2.9856\n",
            "Loss: 3.1956\n",
            "Loss: 3.0294\n",
            "Loss: 2.9343\n",
            "Loss: 3.2760\n",
            "Loss: 3.1496\n",
            "Loss: 3.1802\n",
            "Loss: 3.4194\n",
            "Loss: 2.8865\n",
            "Loss: 2.7166\n",
            "Loss: 3.0067\n",
            "Loss: 2.7702\n",
            "Loss: 2.9776\n",
            "Loss: 3.0494\n",
            "Loss: 2.9559\n",
            "Loss: 2.8682\n",
            "Loss: 3.0981\n",
            "Loss: 3.2043\n",
            "Loss: 2.9785\n",
            "Loss: 3.1980\n",
            "Loss: 3.1292\n",
            "Loss: 3.1047\n",
            "Loss: 2.9388\n",
            "Loss: 3.0809\n",
            "Loss: 2.9916\n",
            "Loss: 3.0683\n",
            "Loss: 2.7677\n",
            "Loss: 2.9467\n",
            "Loss: 2.9585\n",
            "Loss: 3.1461\n",
            "Loss: 2.8693\n",
            "Loss: 2.9901\n",
            "Loss: 3.1500\n",
            "Loss: 3.2370\n",
            "Loss: 3.1306\n",
            "Loss: 2.9395\n",
            "Loss: 3.1294\n",
            "Loss: 3.0995\n",
            "Loss: 3.1496\n",
            "Loss: 3.0559\n",
            "Loss: 2.9702\n",
            "Loss: 2.8488\n",
            "Loss: 3.1556\n",
            "Loss: 3.1597\n",
            "Loss: 2.9968\n",
            "Loss: 3.1676\n",
            "Loss: 3.2074\n",
            "Loss: 3.1606\n",
            "Loss: 3.2043\n",
            "Loss: 2.8716\n",
            "Loss: 3.0522\n",
            "Loss: 3.0646\n",
            "Loss: 2.9113\n",
            "Loss: 3.0124\n",
            "Loss: 3.1160\n",
            "Loss: 3.2222\n",
            "Loss: 3.1164\n",
            "Loss: 2.8761\n",
            "Loss: 2.8650\n",
            "Loss: 3.0482\n",
            "Loss: 2.9981\n",
            "Loss: 3.0180\n",
            "Loss: 3.1683\n",
            "Loss: 3.0226\n",
            "Loss: 2.9906\n",
            "Loss: 3.0474\n",
            "Loss: 2.7462\n",
            "Loss: 3.0154\n",
            "Loss: 3.2491\n",
            "Loss: 2.8931\n",
            "Loss: 2.8910\n",
            "Loss: 3.1365\n",
            "Loss: 3.0835\n",
            "Loss: 2.9051\n",
            "Loss: 3.1229\n",
            "Loss: 3.1584\n",
            "Loss: 3.2555\n",
            "Loss: 3.1903\n",
            "Loss: 3.0285\n",
            "Loss: 2.8860\n",
            "Loss: 3.0724\n",
            "Loss: 2.9886\n",
            "Loss: 3.1817\n",
            "Loss: 3.2245\n",
            "Loss: 3.1132\n",
            "Loss: 3.0313\n",
            "Loss: 3.0216\n",
            "Loss: 3.1454\n",
            "Loss: 3.0609\n",
            "Loss: 3.0795\n",
            "Loss: 2.9736\n",
            "Loss: 2.9573\n",
            "Loss: 2.8461\n",
            "Loss: 3.0467\n",
            "Loss: 3.0196\n",
            "Loss: 3.1473\n",
            "Loss: 2.8533\n",
            "Loss: 3.1426\n",
            "Loss: 3.1510\n",
            "Loss: 3.1742\n",
            "Loss: 3.2050\n",
            "Loss: 3.0808\n",
            "Loss: 3.2050\n",
            "Loss: 3.1555\n",
            "Loss: 3.1001\n",
            "Loss: 3.2675\n",
            "Loss: 3.2157\n",
            "Loss: 3.1656\n",
            "Loss: 3.0875\n",
            "Loss: 3.2536\n",
            "Loss: 3.2118\n",
            "Loss: 3.1962\n",
            "Loss: 2.9995\n",
            "Loss: 3.1120\n",
            "Loss: 2.8007\n",
            "Loss: 3.0651\n",
            "Loss: 3.2028\n",
            "Loss: 3.0924\n",
            "Loss: 2.9712\n",
            "Loss: 3.1377\n",
            "Loss: 3.0664\n",
            "Loss: 2.8770\n",
            "Loss: 3.2958\n",
            "Loss: 3.0616\n",
            "Loss: 3.0968\n",
            "Loss: 2.9789\n",
            "Loss: 3.2552\n",
            "Loss: 3.1261\n",
            "Loss: 2.9194\n",
            "Loss: 3.3601\n",
            "Loss: 2.8461\n",
            "Loss: 2.9459\n",
            "Loss: 3.1177\n",
            "Loss: 3.2342\n",
            "Loss: 2.8822\n",
            "Loss: 3.0758\n",
            "Loss: 2.9887\n",
            "Loss: 3.1471\n",
            "Loss: 3.0916\n",
            "Loss: 3.0895\n",
            "Loss: 3.0544\n",
            "Loss: 3.0520\n",
            "Loss: 2.9629\n",
            "Loss: 3.1276\n",
            "Loss: 3.0680\n",
            "Loss: 3.1708\n",
            "Loss: 3.0655\n",
            "Loss: 3.1036\n",
            "Loss: 3.1491\n",
            "Loss: 2.9753\n",
            "Loss: 3.0637\n",
            "Loss: 3.1887\n",
            "Loss: 3.0539\n",
            "Loss: 3.0949\n",
            "Loss: 3.0813\n",
            "Loss: 3.1903\n",
            "Loss: 2.9795\n",
            "Loss: 2.9146\n",
            "Loss: 2.9441\n",
            "Loss: 2.6660\n",
            "Loss: 2.9187\n",
            "Loss: 2.9681\n",
            "Loss: 2.8767\n",
            "Loss: 2.8779\n",
            "Loss: 3.0624\n",
            "Loss: 2.8903\n",
            "Loss: 2.9984\n",
            "Loss: 2.6913\n",
            "Loss: 2.9559\n",
            "Loss: 2.6599\n",
            "Loss: 2.8096\n",
            "Loss: 2.8302\n",
            "Loss: 2.8094\n",
            "Loss: 2.8750\n",
            "Loss: 2.8333\n",
            "Loss: 2.8130\n",
            "Loss: 2.7515\n",
            "Loss: 2.8086\n",
            "Loss: 2.7192\n",
            "Loss: 2.8140\n",
            "Loss: 2.9214\n",
            "Loss: 2.9812\n",
            "Loss: 2.7628\n",
            "Loss: 2.7730\n",
            "Loss: 2.8256\n",
            "Loss: 2.7756\n",
            "Loss: 2.8850\n",
            "Loss: 2.8264\n",
            "Loss: 2.9063\n",
            "Loss: 2.9598\n",
            "Loss: 2.9441\n",
            "Loss: 3.0889\n",
            "Loss: 2.8264\n",
            "Loss: 3.1375\n",
            "Loss: 2.9325\n",
            "Loss: 2.8349\n",
            "Loss: 2.6886\n",
            "Loss: 3.0010\n",
            "Loss: 2.8527\n",
            "Loss: 2.8929\n",
            "Loss: 2.9667\n",
            "Loss: 2.9171\n",
            "Loss: 2.8096\n",
            "Loss: 2.9175\n",
            "Loss: 2.9005\n",
            "Loss: 2.8078\n",
            "Loss: 2.7928\n",
            "Loss: 3.0095\n",
            "Loss: 2.8166\n",
            "Loss: 2.8914\n",
            "Loss: 2.7889\n",
            "Loss: 3.0554\n",
            "Loss: 3.0306\n",
            "Loss: 2.9448\n",
            "Loss: 2.8129\n",
            "Loss: 2.9188\n",
            "Loss: 3.0337\n",
            "Loss: 2.8202\n",
            "Loss: 2.9141\n",
            "Loss: 3.0038\n",
            "Loss: 2.9226\n",
            "Loss: 2.9628\n",
            "Loss: 3.0506\n",
            "Loss: 2.9060\n",
            "Loss: 2.7960\n",
            "Loss: 2.7775\n",
            "Loss: 2.8477\n",
            "Loss: 2.8558\n",
            "Loss: 3.0424\n",
            "Loss: 3.0423\n",
            "Loss: 2.9364\n",
            "Loss: 2.9914\n",
            "Loss: 3.0258\n",
            "Loss: 2.9043\n",
            "Loss: 3.0883\n",
            "Loss: 2.9175\n",
            "Loss: 2.9300\n",
            "Loss: 2.9322\n",
            "Loss: 3.0421\n",
            "Loss: 2.8110\n",
            "Loss: 2.9235\n",
            "Loss: 2.8789\n",
            "Loss: 2.8251\n",
            "Loss: 2.9332\n",
            "Loss: 3.0347\n",
            "Loss: 2.8385\n",
            "Loss: 2.7477\n",
            "Loss: 3.1299\n",
            "Loss: 2.6016\n",
            "Loss: 2.8923\n",
            "Loss: 2.9258\n",
            "Loss: 2.7529\n",
            "Loss: 2.7994\n",
            "Loss: 3.0537\n",
            "Loss: 2.9704\n",
            "Loss: 3.0133\n",
            "Loss: 3.0298\n",
            "Loss: 3.1051\n",
            "Loss: 2.9598\n",
            "Loss: 3.0407\n",
            "Loss: 2.8662\n",
            "Loss: 2.8953\n",
            "Loss: 2.8303\n",
            "Loss: 2.8932\n",
            "Loss: 2.9580\n",
            "Loss: 2.9789\n",
            "Loss: 2.9732\n",
            "Loss: 2.9945\n",
            "Loss: 3.1067\n",
            "Loss: 3.0667\n",
            "Loss: 3.0361\n",
            "Loss: 2.9416\n",
            "Loss: 2.7342\n",
            "Loss: 2.6692\n",
            "Loss: 3.0306\n",
            "Loss: 2.9643\n",
            "Loss: 2.9101\n",
            "Loss: 3.0291\n",
            "Loss: 2.8008\n",
            "Loss: 3.0610\n",
            "Loss: 3.0279\n",
            "Loss: 2.9550\n",
            "Loss: 3.0096\n",
            "Loss: 2.8444\n",
            "Loss: 2.8841\n",
            "Loss: 2.8598\n",
            "Loss: 3.1144\n",
            "Loss: 2.9677\n",
            "Loss: 2.6935\n",
            "Loss: 3.0110\n",
            "Loss: 2.8211\n",
            "Loss: 2.9778\n",
            "Loss: 2.8928\n",
            "Loss: 2.8475\n",
            "Loss: 2.8891\n",
            "Loss: 2.7861\n",
            "Loss: 2.8298\n",
            "Loss: 3.0297\n",
            "Loss: 2.9766\n",
            "Loss: 2.7372\n",
            "Loss: 2.7410\n",
            "Loss: 2.9654\n",
            "Loss: 2.9007\n",
            "Loss: 2.8763\n",
            "Loss: 2.7691\n",
            "Loss: 2.9317\n",
            "Loss: 2.7430\n",
            "Loss: 2.7039\n",
            "Loss: 2.5539\n",
            "Loss: 2.8805\n",
            "Loss: 2.8067\n",
            "Loss: 2.5872\n",
            "Loss: 2.8390\n",
            "Loss: 2.6756\n",
            "Loss: 2.7354\n",
            "Loss: 2.4987\n",
            "Loss: 2.9363\n",
            "Loss: 2.6441\n",
            "Loss: 2.7967\n",
            "Loss: 2.7410\n",
            "Loss: 2.6866\n",
            "Loss: 2.7359\n",
            "Loss: 2.9546\n",
            "Loss: 2.7499\n",
            "Loss: 2.6811\n",
            "Loss: 2.5866\n",
            "Loss: 2.7268\n",
            "Loss: 2.6684\n",
            "Loss: 2.5445\n",
            "Loss: 2.8604\n",
            "Loss: 2.7966\n",
            "Loss: 2.8273\n",
            "Loss: 2.6365\n",
            "Loss: 2.7953\n",
            "Loss: 2.5942\n",
            "Loss: 2.5725\n",
            "Loss: 2.8407\n",
            "Loss: 2.5467\n",
            "Loss: 2.6820\n",
            "Loss: 2.7559\n",
            "Loss: 2.7176\n",
            "Loss: 2.6683\n",
            "Loss: 2.5964\n",
            "Loss: 2.8053\n",
            "Loss: 2.8357\n",
            "Loss: 2.7963\n",
            "Loss: 2.7596\n",
            "Loss: 2.9679\n",
            "Loss: 2.6191\n",
            "Loss: 2.7883\n",
            "Loss: 2.6740\n",
            "Loss: 2.6079\n",
            "Loss: 3.0252\n",
            "Loss: 2.5794\n",
            "Loss: 2.7375\n",
            "Loss: 2.7353\n",
            "Loss: 2.7824\n",
            "Loss: 2.7581\n",
            "Loss: 2.8144\n",
            "Loss: 2.8126\n",
            "Loss: 2.7506\n",
            "Loss: 2.8896\n",
            "Loss: 2.6942\n",
            "Loss: 2.7189\n",
            "Loss: 2.9189\n",
            "Loss: 2.5971\n",
            "Loss: 2.6940\n",
            "Loss: 2.7362\n",
            "Loss: 2.7215\n",
            "Loss: 2.9378\n",
            "Loss: 2.6961\n",
            "Loss: 2.9497\n",
            "Loss: 2.8491\n",
            "Loss: 2.8884\n",
            "Loss: 2.9106\n",
            "Loss: 2.7790\n",
            "Loss: 2.6818\n",
            "Loss: 2.5362\n",
            "Loss: 2.9358\n",
            "Loss: 2.6487\n",
            "Loss: 2.7806\n",
            "Loss: 2.6034\n",
            "Loss: 2.9319\n",
            "Loss: 2.7165\n",
            "Loss: 2.7720\n",
            "Loss: 2.7502\n",
            "Loss: 2.8295\n",
            "Loss: 2.7312\n",
            "Loss: 2.7529\n",
            "Loss: 2.6338\n",
            "Loss: 2.7303\n",
            "Loss: 2.8551\n",
            "Loss: 2.7800\n",
            "Loss: 2.7116\n",
            "Loss: 2.8041\n",
            "Loss: 2.6310\n",
            "Loss: 2.7802\n",
            "Loss: 2.7369\n",
            "Loss: 2.8672\n",
            "Loss: 2.7111\n",
            "Loss: 2.8038\n",
            "Loss: 2.7156\n",
            "Loss: 2.7410\n",
            "Loss: 2.5325\n",
            "Loss: 2.5949\n",
            "Loss: 2.7994\n",
            "Loss: 2.8104\n",
            "Loss: 2.9370\n",
            "Loss: 2.7178\n",
            "Loss: 2.7661\n",
            "Loss: 2.6421\n",
            "Loss: 2.8929\n",
            "Loss: 2.9282\n",
            "Loss: 2.6661\n",
            "Loss: 2.8115\n",
            "Loss: 2.8370\n",
            "Loss: 2.7073\n",
            "Loss: 2.7986\n",
            "Loss: 2.6422\n",
            "Loss: 2.6015\n",
            "Loss: 2.6764\n",
            "Loss: 2.6752\n",
            "Loss: 2.7275\n",
            "Loss: 2.7859\n",
            "Loss: 2.6415\n",
            "Loss: 2.8420\n",
            "Loss: 2.7092\n",
            "Loss: 2.8292\n",
            "Loss: 2.6962\n",
            "Loss: 2.7168\n",
            "Loss: 2.5457\n",
            "Loss: 2.7246\n",
            "Loss: 2.8895\n",
            "Loss: 2.7679\n",
            "Loss: 2.7323\n",
            "Loss: 2.8459\n",
            "Loss: 2.7687\n",
            "Loss: 2.6826\n",
            "Loss: 2.7199\n",
            "Loss: 2.8789\n",
            "Loss: 2.8896\n",
            "Loss: 2.8449\n",
            "Loss: 2.6948\n",
            "Loss: 2.7566\n",
            "Loss: 2.5653\n",
            "Loss: 2.8107\n",
            "Loss: 2.8609\n",
            "Loss: 2.8099\n",
            "Loss: 2.8616\n",
            "Loss: 2.9103\n",
            "Loss: 2.8291\n",
            "Loss: 2.8776\n",
            "Loss: 2.6561\n",
            "Loss: 2.5754\n",
            "Loss: 2.6134\n",
            "Loss: 2.4148\n",
            "Loss: 2.5403\n",
            "Loss: 2.6000\n",
            "Loss: 2.6339\n",
            "Loss: 2.4136\n",
            "Loss: 2.3916\n",
            "Loss: 2.5534\n",
            "Loss: 2.5849\n",
            "Loss: 2.5772\n",
            "Loss: 2.7376\n",
            "Loss: 2.5750\n",
            "Loss: 2.7463\n",
            "Loss: 2.5966\n",
            "Loss: 2.4425\n",
            "Loss: 2.6528\n",
            "Loss: 2.5100\n",
            "Loss: 2.6263\n",
            "Loss: 2.5608\n",
            "Loss: 2.5827\n",
            "Loss: 2.7077\n",
            "Loss: 2.6361\n",
            "Loss: 2.3762\n",
            "Loss: 2.5695\n",
            "Loss: 2.5527\n",
            "Loss: 2.5335\n",
            "Loss: 2.6264\n",
            "Loss: 2.6337\n",
            "Loss: 2.4518\n",
            "Loss: 2.6570\n",
            "Loss: 2.6245\n",
            "Loss: 2.4984\n",
            "Loss: 2.5021\n",
            "Loss: 2.6839\n",
            "Loss: 2.4460\n",
            "Loss: 2.5384\n",
            "Loss: 2.5485\n",
            "Loss: 2.7954\n",
            "Loss: 2.5853\n",
            "Loss: 2.3973\n",
            "Loss: 2.4241\n",
            "Loss: 2.5697\n",
            "Loss: 2.7602\n",
            "Loss: 2.6797\n",
            "Loss: 2.6541\n",
            "Loss: 2.6178\n",
            "Loss: 2.5445\n",
            "Loss: 2.4357\n",
            "Loss: 2.6389\n",
            "Loss: 2.6047\n",
            "Loss: 2.5776\n",
            "Loss: 2.5333\n",
            "Loss: 2.5489\n",
            "Loss: 2.7019\n",
            "Loss: 2.5860\n",
            "Loss: 2.5248\n",
            "Loss: 2.7395\n",
            "Loss: 2.6569\n",
            "Loss: 2.5724\n",
            "Loss: 2.7758\n",
            "Loss: 2.5834\n",
            "Loss: 2.6237\n",
            "Loss: 2.7176\n",
            "Loss: 2.5625\n",
            "Loss: 2.4745\n",
            "Loss: 2.7211\n",
            "Loss: 2.5344\n",
            "Loss: 2.6521\n",
            "Loss: 2.5714\n",
            "Loss: 2.6103\n",
            "Loss: 2.4724\n",
            "Loss: 2.6029\n",
            "Loss: 2.6367\n",
            "Loss: 2.6934\n",
            "Loss: 2.6950\n",
            "Loss: 2.5100\n",
            "Loss: 2.3144\n",
            "Loss: 2.5728\n",
            "Loss: 2.3896\n",
            "Loss: 2.7633\n",
            "Loss: 2.7506\n",
            "Loss: 2.5639\n",
            "Loss: 2.6842\n",
            "Loss: 2.5792\n",
            "Loss: 2.4794\n",
            "Loss: 2.6283\n",
            "Loss: 2.6773\n",
            "Loss: 2.5310\n",
            "Loss: 2.7363\n",
            "Loss: 2.6960\n",
            "Loss: 2.6248\n",
            "Loss: 2.4106\n",
            "Loss: 2.6566\n",
            "Loss: 2.6321\n",
            "Loss: 2.2877\n",
            "Loss: 2.6756\n",
            "Loss: 2.5040\n",
            "Loss: 2.5465\n",
            "Loss: 2.3905\n",
            "Loss: 2.7631\n",
            "Loss: 2.5017\n",
            "Loss: 2.5766\n",
            "Loss: 2.6301\n",
            "Loss: 2.5307\n",
            "Loss: 2.6593\n",
            "Loss: 2.4941\n",
            "Loss: 2.6711\n",
            "Loss: 2.7009\n",
            "Loss: 2.6475\n",
            "Loss: 2.8738\n",
            "Loss: 2.5963\n",
            "Loss: 2.6452\n",
            "Loss: 2.6915\n",
            "Loss: 2.3695\n",
            "Loss: 2.7480\n",
            "Loss: 2.4644\n",
            "Loss: 2.5335\n",
            "Loss: 2.4858\n",
            "Loss: 2.9155\n",
            "Loss: 2.6988\n",
            "Loss: 2.5131\n",
            "Loss: 2.5821\n",
            "Loss: 2.6337\n",
            "Loss: 2.7437\n",
            "Loss: 2.6852\n",
            "Loss: 2.5344\n",
            "Loss: 2.4789\n",
            "Loss: 2.7046\n",
            "Loss: 2.7683\n",
            "Loss: 2.7768\n",
            "Loss: 2.5889\n",
            "Loss: 2.4396\n",
            "Loss: 2.7467\n",
            "Loss: 2.5173\n",
            "Loss: 2.6860\n",
            "Loss: 2.6529\n",
            "Loss: 2.6878\n",
            "Loss: 2.6792\n",
            "Loss: 2.6211\n",
            "Loss: 2.7581\n",
            "Loss: 2.6904\n",
            "Loss: 2.6379\n",
            "Loss: 2.6875\n",
            "Loss: 2.5305\n",
            "Loss: 2.6100\n",
            "Loss: 2.5169\n",
            "Loss: 2.3448\n",
            "Loss: 2.3601\n",
            "Loss: 2.2754\n",
            "Loss: 2.5139\n",
            "Loss: 2.4465\n",
            "Loss: 2.2977\n",
            "Loss: 2.5559\n",
            "Loss: 2.4527\n",
            "Loss: 2.2493\n",
            "Loss: 2.4061\n",
            "Loss: 2.4332\n",
            "Loss: 2.5581\n",
            "Loss: 2.3810\n",
            "Loss: 2.3561\n",
            "Loss: 2.2057\n",
            "Loss: 2.2832\n",
            "Loss: 2.3823\n",
            "Loss: 2.3800\n",
            "Loss: 2.4662\n",
            "Loss: 2.4373\n",
            "Loss: 2.4516\n",
            "Loss: 2.4340\n",
            "Loss: 2.5035\n",
            "Loss: 2.2951\n",
            "Loss: 2.4639\n",
            "Loss: 2.5418\n",
            "Loss: 2.5229\n",
            "Loss: 2.3798\n",
            "Loss: 2.4976\n",
            "Loss: 2.5291\n",
            "Loss: 2.5442\n",
            "Loss: 2.3040\n",
            "Loss: 2.3165\n",
            "Loss: 2.1604\n",
            "Loss: 2.4483\n",
            "Loss: 2.4585\n",
            "Loss: 2.4372\n",
            "Loss: 2.2624\n",
            "Loss: 2.2964\n",
            "Loss: 2.5084\n",
            "Loss: 2.4283\n",
            "Loss: 2.4007\n",
            "Loss: 2.4995\n",
            "Loss: 2.5151\n",
            "Loss: 2.4246\n",
            "Loss: 2.3524\n",
            "Loss: 2.5169\n",
            "Loss: 2.4657\n",
            "Loss: 2.4024\n",
            "Loss: 2.4740\n",
            "Loss: 2.4492\n",
            "Loss: 2.5074\n",
            "Loss: 2.3249\n",
            "Loss: 2.2852\n",
            "Loss: 2.3039\n",
            "Loss: 2.4788\n",
            "Loss: 2.5296\n",
            "Loss: 2.6803\n",
            "Loss: 2.4434\n",
            "Loss: 2.5167\n",
            "Loss: 2.4656\n",
            "Loss: 2.6236\n",
            "Loss: 2.3603\n",
            "Loss: 2.3386\n",
            "Loss: 2.2554\n",
            "Loss: 2.5702\n",
            "Loss: 2.4143\n",
            "Loss: 2.2575\n",
            "Loss: 2.4486\n",
            "Loss: 2.4811\n",
            "Loss: 2.4468\n",
            "Loss: 2.5562\n",
            "Loss: 2.6533\n",
            "Loss: 2.5191\n",
            "Loss: 2.4077\n",
            "Loss: 2.4905\n",
            "Loss: 2.5569\n",
            "Loss: 2.5224\n",
            "Loss: 2.3977\n",
            "Loss: 2.5183\n",
            "Loss: 2.3973\n",
            "Loss: 2.3926\n",
            "Loss: 2.2630\n",
            "Loss: 2.4794\n",
            "Loss: 2.4106\n",
            "Loss: 2.5177\n",
            "Loss: 2.3096\n",
            "Loss: 2.4808\n",
            "Loss: 2.5413\n",
            "Loss: 2.6501\n",
            "Loss: 2.4567\n",
            "Loss: 2.3548\n",
            "Loss: 2.4155\n",
            "Loss: 2.3088\n",
            "Loss: 2.3058\n",
            "Loss: 2.1822\n",
            "Loss: 2.5228\n",
            "Loss: 2.5275\n",
            "Loss: 2.5167\n",
            "Loss: 2.4829\n",
            "Loss: 2.6695\n",
            "Loss: 2.3304\n",
            "Loss: 2.3247\n",
            "Loss: 2.3841\n",
            "Loss: 2.3458\n",
            "Loss: 2.4916\n",
            "Loss: 2.4392\n",
            "Loss: 2.4250\n",
            "Loss: 2.2654\n",
            "Loss: 2.4413\n",
            "Loss: 2.5616\n",
            "Loss: 2.6780\n",
            "Loss: 2.4853\n",
            "Loss: 2.6953\n",
            "Loss: 2.3787\n",
            "Loss: 2.4497\n",
            "Loss: 2.4416\n",
            "Loss: 2.3802\n",
            "Loss: 2.5730\n",
            "Loss: 2.5434\n",
            "Loss: 2.3987\n",
            "Loss: 2.3605\n",
            "Loss: 2.5742\n",
            "Loss: 2.5520\n",
            "Loss: 2.4047\n",
            "Loss: 2.5203\n",
            "Loss: 2.5900\n",
            "Loss: 2.3474\n",
            "Loss: 2.4231\n",
            "Loss: 2.4480\n",
            "Loss: 2.4041\n",
            "Loss: 2.5870\n",
            "Loss: 2.4811\n",
            "Loss: 2.5269\n",
            "Loss: 2.3891\n",
            "Loss: 2.5670\n",
            "Loss: 2.3228\n",
            "Loss: 2.3793\n",
            "Loss: 2.4991\n",
            "Loss: 2.6825\n",
            "Loss: 2.3911\n",
            "Loss: 2.6763\n",
            "Loss: 2.4727\n",
            "Loss: 2.4424\n",
            "Loss: 2.4606\n",
            "Loss: 2.4564\n",
            "Loss: 2.4263\n",
            "Loss: 2.4013\n",
            "Loss: 2.2330\n",
            "Loss: 2.2358\n",
            "Loss: 2.2735\n",
            "Loss: 2.2887\n",
            "Loss: 2.2431\n",
            "Loss: 2.0428\n",
            "Loss: 2.3462\n",
            "Loss: 2.2739\n",
            "Loss: 2.3126\n",
            "Loss: 2.1004\n",
            "Loss: 2.1103\n",
            "Loss: 2.1967\n",
            "Loss: 2.2034\n",
            "Loss: 2.0946\n",
            "Loss: 2.2379\n",
            "Loss: 2.2324\n",
            "Loss: 2.3291\n",
            "Loss: 2.4248\n",
            "Loss: 2.2106\n",
            "Loss: 2.3181\n",
            "Loss: 2.1462\n",
            "Loss: 2.2453\n",
            "Loss: 2.1286\n",
            "Loss: 2.3166\n",
            "Loss: 2.1069\n",
            "Loss: 2.1355\n",
            "Loss: 2.3086\n",
            "Loss: 2.2461\n",
            "Loss: 2.3833\n",
            "Loss: 2.1889\n",
            "Loss: 2.3891\n",
            "Loss: 2.3924\n",
            "Loss: 2.2822\n",
            "Loss: 2.3706\n",
            "Loss: 2.4181\n",
            "Loss: 2.2545\n",
            "Loss: 2.2756\n",
            "Loss: 2.3073\n",
            "Loss: 2.3504\n",
            "Loss: 2.3432\n",
            "Loss: 2.3515\n",
            "Loss: 2.3355\n",
            "Loss: 2.3081\n",
            "Loss: 2.2219\n",
            "Loss: 2.2843\n",
            "Loss: 2.3124\n",
            "Loss: 2.2768\n",
            "Loss: 2.3473\n",
            "Loss: 2.4440\n",
            "Loss: 2.3984\n",
            "Loss: 2.3304\n",
            "Loss: 2.2570\n",
            "Loss: 2.2951\n",
            "Loss: 2.1640\n",
            "Loss: 2.2629\n",
            "Loss: 2.3354\n",
            "Loss: 2.0002\n",
            "Loss: 2.3298\n",
            "Loss: 2.2789\n",
            "Loss: 2.3122\n",
            "Loss: 2.3028\n",
            "Loss: 2.2370\n",
            "Loss: 2.2542\n",
            "Loss: 2.3931\n",
            "Loss: 2.4017\n",
            "Loss: 2.3687\n",
            "Loss: 2.2425\n",
            "Loss: 2.2147\n",
            "Loss: 2.3520\n",
            "Loss: 2.2821\n",
            "Loss: 2.2901\n",
            "Loss: 2.3098\n",
            "Loss: 2.4067\n",
            "Loss: 2.2690\n",
            "Loss: 2.2380\n",
            "Loss: 2.2927\n",
            "Loss: 2.2167\n",
            "Loss: 2.2465\n",
            "Loss: 2.2188\n",
            "Loss: 2.3504\n",
            "Loss: 2.4084\n",
            "Loss: 2.2213\n",
            "Loss: 2.1696\n",
            "Loss: 2.3684\n",
            "Loss: 2.2926\n",
            "Loss: 2.3619\n",
            "Loss: 2.3249\n",
            "Loss: 2.1841\n",
            "Loss: 2.3828\n",
            "Loss: 2.3569\n",
            "Loss: 2.3828\n",
            "Loss: 2.4211\n",
            "Loss: 2.1791\n",
            "Loss: 2.0948\n",
            "Loss: 2.2834\n",
            "Loss: 2.2596\n",
            "Loss: 2.3397\n",
            "Loss: 2.2216\n",
            "Loss: 2.2294\n",
            "Loss: 2.4218\n",
            "Loss: 2.2692\n",
            "Loss: 2.2334\n",
            "Loss: 2.3803\n",
            "Loss: 2.3234\n",
            "Loss: 2.2417\n",
            "Loss: 2.2709\n",
            "Loss: 2.0732\n",
            "Loss: 2.3610\n",
            "Loss: 2.3088\n",
            "Loss: 2.2086\n",
            "Loss: 2.4930\n",
            "Loss: 2.2785\n",
            "Loss: 2.3308\n",
            "Loss: 2.2937\n",
            "Loss: 2.3956\n",
            "Loss: 2.3646\n",
            "Loss: 2.2187\n",
            "Loss: 2.1921\n",
            "Loss: 2.1938\n",
            "Loss: 2.2489\n",
            "Loss: 2.3841\n",
            "Loss: 2.3802\n",
            "Loss: 2.2626\n",
            "Loss: 2.2980\n",
            "Loss: 2.2080\n",
            "Loss: 2.3778\n",
            "Loss: 2.0819\n",
            "Loss: 2.3313\n",
            "Loss: 2.4690\n",
            "Loss: 2.3418\n",
            "Loss: 2.3116\n",
            "Loss: 2.1860\n",
            "Loss: 2.2332\n",
            "Loss: 2.2254\n",
            "Loss: 2.2916\n",
            "Loss: 2.4169\n",
            "Loss: 2.2363\n",
            "Loss: 2.2957\n",
            "Loss: 2.3296\n",
            "Loss: 2.1819\n",
            "Loss: 2.2584\n",
            "Loss: 2.6140\n",
            "Loss: 2.4373\n",
            "Loss: 2.2655\n",
            "Loss: 2.2867\n",
            "Loss: 2.1856\n",
            "Loss: 2.3890\n",
            "Loss: 2.2711\n",
            "Loss: 2.1870\n",
            "Loss: 2.0477\n",
            "Loss: 2.0233\n",
            "Loss: 2.0886\n",
            "Loss: 2.2093\n",
            "Loss: 2.0778\n",
            "Loss: 2.0361\n",
            "Loss: 2.0891\n",
            "Loss: 2.0473\n",
            "Loss: 2.1323\n",
            "Loss: 2.1384\n",
            "Loss: 2.1501\n",
            "Loss: 2.1233\n",
            "Loss: 1.9987\n",
            "Loss: 2.0660\n",
            "Loss: 1.9436\n",
            "Loss: 2.0347\n",
            "Loss: 2.2134\n",
            "Loss: 2.0230\n",
            "Loss: 2.0435\n",
            "Loss: 2.0205\n",
            "Loss: 2.0820\n",
            "Loss: 2.1473\n",
            "Loss: 2.0360\n",
            "Loss: 2.0680\n",
            "Loss: 2.2485\n",
            "Loss: 2.0819\n",
            "Loss: 1.9403\n",
            "Loss: 2.1250\n",
            "Loss: 2.0925\n",
            "Loss: 2.0026\n",
            "Loss: 2.0170\n",
            "Loss: 1.9348\n",
            "Loss: 2.0419\n",
            "Loss: 1.8730\n",
            "Loss: 2.1137\n",
            "Loss: 1.9893\n",
            "Loss: 2.1466\n",
            "Loss: 2.1678\n",
            "Loss: 2.1085\n",
            "Loss: 2.0404\n",
            "Loss: 2.1959\n",
            "Loss: 2.0975\n",
            "Loss: 2.0767\n",
            "Loss: 2.0436\n",
            "Loss: 2.1133\n",
            "Loss: 2.0432\n",
            "Loss: 2.1185\n",
            "Loss: 2.2507\n",
            "Loss: 2.0053\n",
            "Loss: 2.1066\n",
            "Loss: 2.1761\n",
            "Loss: 2.1922\n",
            "Loss: 2.2631\n",
            "Loss: 2.0050\n",
            "Loss: 2.0726\n",
            "Loss: 2.1789\n",
            "Loss: 2.2018\n",
            "Loss: 2.0525\n",
            "Loss: 1.9643\n",
            "Loss: 2.0610\n",
            "Loss: 2.0494\n",
            "Loss: 2.1271\n",
            "Loss: 2.2488\n",
            "Loss: 2.1153\n",
            "Loss: 2.0088\n",
            "Loss: 2.0767\n",
            "Loss: 2.1734\n",
            "Loss: 2.0197\n",
            "Loss: 2.2796\n",
            "Loss: 2.2938\n",
            "Loss: 2.0835\n",
            "Loss: 2.3194\n",
            "Loss: 2.2471\n",
            "Loss: 2.0643\n",
            "Loss: 2.2859\n",
            "Loss: 2.2258\n",
            "Loss: 2.0061\n",
            "Loss: 2.0047\n",
            "Loss: 2.0764\n",
            "Loss: 2.1904\n",
            "Loss: 2.2132\n",
            "Loss: 2.1662\n",
            "Loss: 2.2277\n",
            "Loss: 2.1077\n",
            "Loss: 2.1396\n",
            "Loss: 2.0365\n",
            "Loss: 2.1028\n",
            "Loss: 2.2104\n",
            "Loss: 2.1114\n",
            "Loss: 2.1682\n",
            "Loss: 2.0278\n",
            "Loss: 2.1165\n",
            "Loss: 2.1967\n",
            "Loss: 2.1688\n",
            "Loss: 2.0595\n",
            "Loss: 2.2612\n",
            "Loss: 2.2354\n",
            "Loss: 2.3297\n",
            "Loss: 2.2647\n",
            "Loss: 2.1784\n",
            "Loss: 2.1226\n",
            "Loss: 2.1278\n",
            "Loss: 2.3559\n",
            "Loss: 1.9223\n",
            "Loss: 2.2631\n",
            "Loss: 2.2082\n",
            "Loss: 2.2919\n",
            "Loss: 2.2155\n",
            "Loss: 2.0219\n",
            "Loss: 1.9526\n",
            "Loss: 2.1577\n",
            "Loss: 2.1390\n",
            "Loss: 2.1234\n",
            "Loss: 2.0907\n",
            "Loss: 2.0624\n",
            "Loss: 2.2496\n",
            "Loss: 2.3277\n",
            "Loss: 2.0424\n",
            "Loss: 2.2940\n",
            "Loss: 2.0583\n",
            "Loss: 2.2223\n",
            "Loss: 2.0719\n",
            "Loss: 2.1208\n",
            "Loss: 2.2221\n",
            "Loss: 2.2765\n",
            "Loss: 2.1252\n",
            "Loss: 2.1316\n",
            "Loss: 2.2046\n",
            "Loss: 2.1293\n",
            "Loss: 2.1932\n",
            "Loss: 2.2731\n",
            "Loss: 2.1158\n",
            "Loss: 2.1579\n",
            "Loss: 2.1568\n",
            "Loss: 2.0733\n",
            "Loss: 2.0111\n",
            "Loss: 2.1923\n",
            "Loss: 1.9611\n",
            "Loss: 2.1860\n",
            "Loss: 2.2504\n",
            "Loss: 2.1906\n",
            "Loss: 2.1094\n",
            "Loss: 2.1828\n",
            "Loss: 2.1117\n",
            "Loss: 2.2103\n",
            "Loss: 1.9934\n",
            "Loss: 2.4705\n",
            "Loss: 1.8656\n",
            "Loss: 1.9522\n",
            "Loss: 1.9958\n",
            "Loss: 1.9052\n",
            "Loss: 1.9056\n",
            "Loss: 1.9040\n",
            "Loss: 2.0202\n",
            "Loss: 1.7503\n",
            "Loss: 1.9125\n",
            "Loss: 1.8454\n",
            "Loss: 2.0174\n",
            "Loss: 2.0369\n",
            "Loss: 2.0093\n",
            "Loss: 1.9913\n",
            "Loss: 1.8892\n",
            "Loss: 1.8649\n",
            "Loss: 1.8682\n",
            "Loss: 1.8860\n",
            "Loss: 2.0181\n",
            "Loss: 1.9374\n",
            "Loss: 1.9493\n",
            "Loss: 1.7865\n",
            "Loss: 1.9013\n",
            "Loss: 1.8663\n",
            "Loss: 1.9853\n",
            "Loss: 1.8528\n",
            "Loss: 1.8161\n",
            "Loss: 1.9618\n",
            "Loss: 2.0249\n",
            "Loss: 2.0008\n",
            "Loss: 1.9297\n",
            "Loss: 1.8377\n",
            "Loss: 1.9825\n",
            "Loss: 2.0923\n",
            "Loss: 1.8445\n",
            "Loss: 1.8180\n",
            "Loss: 1.9793\n",
            "Loss: 2.0556\n",
            "Loss: 1.9681\n",
            "Loss: 1.9264\n",
            "Loss: 1.8877\n",
            "Loss: 1.8778\n",
            "Loss: 1.8764\n",
            "Loss: 1.9240\n",
            "Loss: 2.0378\n",
            "Loss: 1.8672\n",
            "Loss: 2.0262\n",
            "Loss: 2.0253\n",
            "Loss: 1.9634\n",
            "Loss: 1.9681\n",
            "Loss: 2.0740\n",
            "Loss: 1.8242\n",
            "Loss: 1.9545\n",
            "Loss: 2.1088\n",
            "Loss: 1.9603\n",
            "Loss: 2.1162\n",
            "Loss: 1.9244\n",
            "Loss: 1.9204\n",
            "Loss: 1.8818\n",
            "Loss: 2.0313\n",
            "Loss: 2.0002\n",
            "Loss: 1.9638\n",
            "Loss: 2.0286\n",
            "Loss: 1.9624\n",
            "Loss: 1.9647\n",
            "Loss: 1.8379\n",
            "Loss: 1.8705\n",
            "Loss: 1.8314\n",
            "Loss: 1.8999\n",
            "Loss: 1.9204\n",
            "Loss: 2.0224\n",
            "Loss: 1.9816\n",
            "Loss: 1.9309\n",
            "Loss: 1.9239\n",
            "Loss: 1.9316\n",
            "Loss: 1.9716\n",
            "Loss: 2.0108\n",
            "Loss: 2.0118\n",
            "Loss: 1.9923\n",
            "Loss: 1.8063\n",
            "Loss: 1.9193\n",
            "Loss: 2.0144\n",
            "Loss: 1.9124\n",
            "Loss: 1.9789\n",
            "Loss: 2.0471\n",
            "Loss: 2.0182\n",
            "Loss: 2.0717\n",
            "Loss: 1.9500\n",
            "Loss: 2.1305\n",
            "Loss: 1.9656\n",
            "Loss: 1.9613\n",
            "Loss: 2.0601\n",
            "Loss: 1.9111\n",
            "Loss: 1.9999\n",
            "Loss: 2.0745\n",
            "Loss: 2.0531\n",
            "Loss: 1.8210\n",
            "Loss: 2.0172\n",
            "Loss: 1.7967\n",
            "Loss: 1.9926\n",
            "Loss: 1.8392\n",
            "Loss: 2.0196\n",
            "Loss: 1.8925\n",
            "Loss: 2.0172\n",
            "Loss: 2.0905\n",
            "Loss: 1.9406\n",
            "Loss: 2.1408\n",
            "Loss: 1.9224\n",
            "Loss: 2.0494\n",
            "Loss: 2.0225\n",
            "Loss: 2.0539\n",
            "Loss: 2.0827\n",
            "Loss: 1.9535\n",
            "Loss: 1.8732\n",
            "Loss: 2.0120\n",
            "Loss: 1.8945\n",
            "Loss: 1.9848\n",
            "Loss: 1.8720\n",
            "Loss: 2.0513\n",
            "Loss: 1.9082\n",
            "Loss: 2.2411\n",
            "Loss: 2.0475\n",
            "Loss: 2.0583\n",
            "Loss: 1.9508\n",
            "Loss: 2.0397\n",
            "Loss: 2.0760\n",
            "Loss: 2.0836\n",
            "Loss: 2.0377\n",
            "Loss: 2.0218\n",
            "Loss: 2.1685\n",
            "Loss: 1.9912\n",
            "Loss: 1.8936\n",
            "Loss: 2.0526\n",
            "Loss: 2.0276\n",
            "Loss: 1.9025\n",
            "Loss: 1.8823\n",
            "Loss: 2.1304\n",
            "Loss: 2.0150\n",
            "Loss: 1.8769\n",
            "Loss: 2.0886\n",
            "Loss: 2.0284\n",
            "Loss: 2.1936\n",
            "Loss: 1.9202\n",
            "Loss: 2.0822\n",
            "Loss: 2.1552\n",
            "Loss: 1.9565\n",
            "Loss: 2.0678\n",
            "Loss: 2.0796\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "model.train()\n",
        "dl_iter = iter(train_dl)\n",
        "\n",
        "for i in range(10):\n",
        "  for batch in train_dl:\n",
        "    # push all to device\n",
        "    batch = {k: batch[k].to(device) for k in batch.keys()}\n",
        "\n",
        "    outputs = model(**batch)\n",
        "    optimizer.zero_grad()\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    print (f'Loss: {loss.item():.4f}')\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Washington:\n",
            "I will not be so long in saying.\n",
            "\n",
            "First Senator:\n",
            "You are a senator, and you must be so.\n",
            "You have been forsworn to the people's house:\n",
            "And you have not been so long to demand it.\n",
            "Your honours both,--\n",
            "\n",
            "SICINIUS:\n",
            "We have been so forswaken, and so long.\n",
            "I would be so brief to say the truth.\n",
            "The people are not so much in their love\n",
            "As they are in fearing to hear me tell it. They\n",
            "are not as much in fear\n"
          ]
        }
      ],
      "source": [
        "# generate text\n",
        "prompt = tokenizer('Washington:\\n', return_tensors='pt')\n",
        "prompt = prompt.input_ids.to(device)\n",
        "out = model.generate(\n",
        "  prompt,\n",
        "  min_new_tokens=30,\n",
        "  max_new_tokens=120,\n",
        "  no_repeat_ngram_size=3\n",
        ")\n",
        "print(tokenizer.decode(out[0]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
