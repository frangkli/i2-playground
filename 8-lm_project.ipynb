{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XztXEJOzfjC"
      },
      "source": [
        "In this exercise, you'll train GPT-2 on a dataset of Shakespeare! You'll be using Hugging Face to download the dataset and pre-trained model - there's a tutorial located in the same folder as this in the GitHub repository. That tutorial will explain how to load models and datasets, pre-process them, and fine-tune the model!\n",
        "\n",
        "The starter code here is quite lightweight - you should refer back to the tutorial often. You will likely want to copy-paste some code from it, just make sure you know what it is doing!\n",
        "\n",
        "Once the model is trained, try generating some text! Hopefully it looks (somewhat) like Shakespeare!\n",
        "\n",
        "If you finish this and want to explore further, try training a different model or use a different dataset. This exercise is meant for you to be able to explore, so do what you find interesting!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftOXtNRCxVCg"
      },
      "source": [
        "Download required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3H25Eo5Xm-a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\frank\\anaconda3\\lib\\site-packages (4.26.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\frank\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: requests in c:\\users\\frank\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: colorama in c:\\users\\frank\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
            "Requirement already satisfied: datasets in c:\\users\\frank\\anaconda3\\lib\\site-packages (2.10.1)\n",
            "Requirement already satisfied: packaging in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (2.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (1.21.5)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: xxhash in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (2022.7.1)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (0.12.1)\n",
            "Requirement already satisfied: responses<0.19 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (11.0.0)\n",
            "Requirement already satisfied: pandas in c:\\users\\frank\\anaconda3\\lib\\site-packages (from datasets) (1.4.4)\n",
            "Requirement already satisfied: filelock in c:\\users\\frank\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
            "Requirement already satisfied: colorama in c:\\users\\frank\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.5)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\frank\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skalDwAkxd0P"
      },
      "source": [
        "Load the dataset you'll be using:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KfqFILiFtZx7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset tiny_shakespeare (C:/Users/Frank/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0fad8e948c184a90ace0cf5bf3fb4720",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 1\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 1\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 1\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset('tiny_shakespeare')\n",
        "ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDDu2xhUxiDX"
      },
      "source": [
        "Let's take a quick look at what's in the dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2sqvyaT6tw-J"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us\n"
          ]
        }
      ],
      "source": [
        "print(ds['train']['text'][0][:300])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TeSW4qnxNQx"
      },
      "source": [
        "\n",
        "And from here it's up to you! Make sure to look at the tutorial, it will hopefully be a good guide. If you have any questions be sure to ping someone in the Discord!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 1\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 1\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 1\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at C:\\Users\\Frank\\.cache\\huggingface\\datasets\\tiny_shakespeare\\default\\1.0.0\\b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e\\cache-babfbdcf4b3186c8.arrow\n",
            "Loading cached processed dataset at C:\\Users\\Frank\\.cache\\huggingface\\datasets\\tiny_shakespeare\\default\\1.0.0\\b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e\\cache-d9a53b4a56b68156.arrow\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d2d2a78abd24d9382aa3497d4536366",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (17995 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 1\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 1\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 1\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def token_func(example):\n",
        "  return tokenizer(example['text'])\n",
        "\n",
        "tokenized_ds = ds.map(token_func, batched=True)\n",
        "tokenized_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask'],\n",
              "        num_rows: 1\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['input_ids', 'attention_mask'],\n",
              "        num_rows: 1\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask'],\n",
              "        num_rows: 1\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rem_tokenized_ds = tokenized_ds.remove_columns(['text'])\n",
        "rem_tokenized_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at C:\\Users\\Frank\\.cache\\huggingface\\datasets\\tiny_shakespeare\\default\\1.0.0\\b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e\\cache-6f89cb6b5a97da3a.arrow\n",
            "Loading cached processed dataset at C:\\Users\\Frank\\.cache\\huggingface\\datasets\\tiny_shakespeare\\default\\1.0.0\\b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e\\cache-0de79604e8c4b309.arrow\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c574f180731a4a2ab85e8deb344f5709",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 2359\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['input_ids', 'attention_mask'],\n",
              "        num_rows: 141\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask'],\n",
              "        num_rows: 140\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from itertools import chain\n",
        "\n",
        "# group texts into blocks of block_size\n",
        "block_size = 128\n",
        "\n",
        "def group_texts(examples):\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the small remainder\n",
        "    if total_length >= block_size:\n",
        "        total_length = (total_length // block_size) * block_size\n",
        "    # Split by chunks of max_len.\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    return result\n",
        "\n",
        "batched_ds = rem_tokenized_ds.map(group_texts, batched=True)\n",
        "batched_ds['train'] = batched_ds['train'].add_column('labels', batched_ds['train']['input_ids'])\n",
        "batched_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "128\n",
            "128\n"
          ]
        }
      ],
      "source": [
        "print(len(batched_ds['train'][0]['input_ids']))\n",
        "print(len(batched_ds['train'][0]['labels']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import default_data_collator\n",
        "\n",
        "train_dl = DataLoader(\n",
        "  batched_ds['train'],\n",
        "  shuffle=True,\n",
        "  batch_size=16,\n",
        "  collate_fn=default_data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step [1/16], Loss: 2.6431\n",
            "Step [2/16], Loss: 2.9191\n",
            "Step [3/16], Loss: 2.7397\n",
            "Step [4/16], Loss: 2.7720\n",
            "Step [5/16], Loss: 2.6491\n",
            "Step [6/16], Loss: 2.8077\n",
            "Step [7/16], Loss: 2.6606\n",
            "Step [8/16], Loss: 2.8092\n",
            "Step [9/16], Loss: 2.6795\n",
            "Step [10/16], Loss: 2.6024\n",
            "Step [11/16], Loss: 2.6891\n",
            "Step [12/16], Loss: 2.9350\n",
            "Step [13/16], Loss: 2.7233\n",
            "Step [14/16], Loss: 2.7324\n",
            "Step [15/16], Loss: 2.8585\n",
            "Step [16/16], Loss: 2.7384\n",
            "Step [1/16], Loss: 2.7024\n",
            "Step [2/16], Loss: 2.7754\n",
            "Step [3/16], Loss: 2.6712\n",
            "Step [4/16], Loss: 2.9204\n",
            "Step [5/16], Loss: 2.8153\n",
            "Step [6/16], Loss: 2.8420\n",
            "Step [7/16], Loss: 2.8336\n",
            "Step [8/16], Loss: 3.0135\n",
            "Step [9/16], Loss: 2.7976\n",
            "Step [10/16], Loss: 2.7232\n",
            "Step [11/16], Loss: 2.8511\n",
            "Step [12/16], Loss: 2.9437\n",
            "Step [13/16], Loss: 2.8820\n",
            "Step [14/16], Loss: 2.6424\n",
            "Step [15/16], Loss: 2.7303\n",
            "Step [16/16], Loss: 2.6960\n",
            "Step [1/16], Loss: 2.7519\n",
            "Step [2/16], Loss: 2.7385\n",
            "Step [3/16], Loss: 2.7152\n",
            "Step [4/16], Loss: 2.7813\n",
            "Step [5/16], Loss: 2.5900\n",
            "Step [6/16], Loss: 2.9124\n",
            "Step [7/16], Loss: 2.7390\n",
            "Step [8/16], Loss: 2.7126\n",
            "Step [9/16], Loss: 2.6192\n",
            "Step [10/16], Loss: 3.0092\n",
            "Step [11/16], Loss: 2.8484\n",
            "Step [12/16], Loss: 2.7397\n",
            "Step [13/16], Loss: 2.7222\n",
            "Step [14/16], Loss: 2.9022\n",
            "Step [15/16], Loss: 2.7045\n",
            "Step [16/16], Loss: 2.7719\n",
            "Step [1/16], Loss: 2.7517\n",
            "Step [2/16], Loss: 2.9472\n",
            "Step [3/16], Loss: 2.8003\n",
            "Step [4/16], Loss: 2.9069\n",
            "Step [5/16], Loss: 2.8616\n",
            "Step [6/16], Loss: 2.8331\n",
            "Step [7/16], Loss: 2.6345\n",
            "Step [8/16], Loss: 2.7114\n",
            "Step [9/16], Loss: 2.7300\n",
            "Step [10/16], Loss: 2.7684\n",
            "Step [11/16], Loss: 2.6776\n",
            "Step [12/16], Loss: 2.6782\n",
            "Step [13/16], Loss: 2.9009\n",
            "Step [14/16], Loss: 2.7410\n",
            "Step [15/16], Loss: 2.6988\n",
            "Step [16/16], Loss: 2.8782\n",
            "Step [1/16], Loss: 2.7210\n",
            "Step [2/16], Loss: 2.7565\n",
            "Step [3/16], Loss: 2.7037\n",
            "Step [4/16], Loss: 2.8635\n",
            "Step [5/16], Loss: 2.6454\n",
            "Step [6/16], Loss: 2.7879\n",
            "Step [7/16], Loss: 2.6595\n",
            "Step [8/16], Loss: 2.8733\n",
            "Step [9/16], Loss: 2.7934\n",
            "Step [10/16], Loss: 2.8728\n",
            "Step [11/16], Loss: 2.9014\n",
            "Step [12/16], Loss: 2.7572\n",
            "Step [13/16], Loss: 2.8911\n",
            "Step [14/16], Loss: 2.6726\n",
            "Step [15/16], Loss: 2.8071\n",
            "Step [16/16], Loss: 2.8574\n",
            "Step [1/16], Loss: 2.6540\n",
            "Step [2/16], Loss: 2.7384\n",
            "Step [3/16], Loss: 3.0059\n",
            "Step [4/16], Loss: 2.7367\n",
            "Step [5/16], Loss: 2.8152\n",
            "Step [6/16], Loss: 2.8287\n",
            "Step [7/16], Loss: 2.5958\n",
            "Step [8/16], Loss: 2.6976\n",
            "Step [9/16], Loss: 2.7839\n",
            "Step [10/16], Loss: 2.7500\n",
            "Step [11/16], Loss: 2.8941\n",
            "Step [12/16], Loss: 2.8073\n",
            "Step [13/16], Loss: 2.7656\n",
            "Step [14/16], Loss: 2.8218\n",
            "Step [15/16], Loss: 2.8303\n",
            "Step [16/16], Loss: 2.7556\n",
            "Step [1/16], Loss: 2.5782\n",
            "Step [2/16], Loss: 2.5930\n",
            "Step [3/16], Loss: 2.8210\n",
            "Step [4/16], Loss: 2.9316\n",
            "Step [5/16], Loss: 2.4435\n",
            "Step [6/16], Loss: 2.7428\n",
            "Step [7/16], Loss: 2.8528\n",
            "Step [8/16], Loss: 2.7619\n",
            "Step [9/16], Loss: 2.9623\n",
            "Step [10/16], Loss: 2.8379\n",
            "Step [11/16], Loss: 2.7475\n",
            "Step [12/16], Loss: 2.6446\n",
            "Step [13/16], Loss: 2.7977\n",
            "Step [14/16], Loss: 2.7394\n",
            "Step [15/16], Loss: 2.8132\n",
            "Step [16/16], Loss: 2.6649\n",
            "Step [1/16], Loss: 2.8150\n",
            "Step [2/16], Loss: 2.6663\n",
            "Step [3/16], Loss: 2.7203\n",
            "Step [4/16], Loss: 2.5655\n",
            "Step [5/16], Loss: 2.7211\n",
            "Step [6/16], Loss: 2.7856\n",
            "Step [7/16], Loss: 2.7235\n",
            "Step [8/16], Loss: 2.6591\n",
            "Step [9/16], Loss: 2.7350\n",
            "Step [10/16], Loss: 2.7510\n",
            "Step [11/16], Loss: 2.6765\n",
            "Step [12/16], Loss: 2.8646\n",
            "Step [13/16], Loss: 2.9255\n",
            "Step [14/16], Loss: 2.7789\n",
            "Step [15/16], Loss: 2.7999\n",
            "Step [16/16], Loss: 2.6977\n",
            "Step [1/16], Loss: 2.7722\n",
            "Step [2/16], Loss: 2.9219\n",
            "Step [3/16], Loss: 2.7041\n",
            "Step [4/16], Loss: 2.6319\n",
            "Step [5/16], Loss: 2.9412\n",
            "Step [6/16], Loss: 2.8659\n",
            "Step [7/16], Loss: 2.9269\n",
            "Step [8/16], Loss: 2.8286\n",
            "Step [9/16], Loss: 2.9253\n",
            "Step [10/16], Loss: 2.6992\n",
            "Step [11/16], Loss: 2.8782\n",
            "Step [12/16], Loss: 2.7597\n",
            "Step [13/16], Loss: 2.8332\n",
            "Step [14/16], Loss: 2.8261\n",
            "Step [15/16], Loss: 2.7702\n",
            "Step [16/16], Loss: 2.9027\n",
            "Step [1/16], Loss: 2.8384\n",
            "Step [2/16], Loss: 2.8820\n",
            "Step [3/16], Loss: 2.8167\n",
            "Step [4/16], Loss: 2.7441\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "model.train()\n",
        "dl_iter = iter(train_dl)\n",
        "\n",
        "for batch in train_dl:\n",
        "  batch_size = len(batch['input_ids'])\n",
        "  for i in range(batch_size):\n",
        "    try:\n",
        "      batch = next(dl_iter)\n",
        "    except StopIteration:\n",
        "      break\n",
        "\n",
        "    # push all to device\n",
        "    batch = {k: batch[k].to(device) for k in batch.keys()}\n",
        "\n",
        "    outputs = model(**batch)\n",
        "    optimizer.zero_grad()\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    print (f'Step [{i+1}/{batch_size}], Loss: {loss.item():.4f}')\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|endoftext|>I'll be brief, and not be long.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "I am not so brief, nor so long\n"
          ]
        }
      ],
      "source": [
        "# generate text\n",
        "out = model.generate(min_new_tokens=10, max_new_tokens=30)\n",
        "print(tokenizer.decode(out[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
